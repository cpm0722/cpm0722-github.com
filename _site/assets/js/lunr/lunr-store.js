var store = [{
        "title": "Attention Is All You Need",
        "excerpt":"Attention is All You Need title: Attention is All You Need subtitle: Transformer categories: Paper Review tags: NLP date: 2021-01-19 13:00:41 +0000 last_modified_at: 2021-01-19 13:00:41 +0000 — Archive Link: https://arxiv.org/abs/1706.03762 Created: Sep 21, 2020 3:16 PM Field: NLP Paper Link: https://arxiv.org/pdf/1706.03762.pdf Status: completed Submit Date: Jun 12, 2017 cleanUrl: /nlp/attention-is-all-you-need...","categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/Attention-is-All-You-Need/",
        "teaser": null
      },{
        "title": "Bert Pre Training Of Deep Bidirectional Transformers For Language Understanding",
        "excerpt":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding subtitle: BERT categories: Paper Review tags: NLP date: 2021-01-19 13:00:09 +0000 last_modified_at: 2021-01-19 13:00:09 +0000 — Archive Link: https://arxiv.org/abs/1810.04805 Created: Sep 21, 2020 3:17 PM Field: NLP Paper Link: https://arxiv.org/pdf/1810.04805.pdf Status:...","categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding/",
        "teaser": null
      },{
        "title": "Deep Contextualized Word Representations",
        "excerpt":"Deep contextualized word representations title: Deep contextualized word representations subtitle: ELMo categories: Paper Review tags: NLP date: 2021-01-19 13:01:33 +0000 last_modified_at: 2021-01-19 13:01:33 +0000 — Archive Link: https://arxiv.org/abs/1802.05365 Created: Jan 2, 2021 1:02 AM Field: NLP Paper Link: https://arxiv.org/pdf/1802.05365.pdf Status: completed Submit Date: Feb 15, 2018 Introduction word2vec이나 glove와 같은...","categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/Deep-contextualized-word-representations/",
        "teaser": null
      },{
        "title": "Efficient Estimation Of Word Representations In Vector Space",
        "excerpt":"Efficient Estimation of Word Representations in Vector Space title: Efficient Estimation of Word Representations in Vector Space subtitle: Word2Vec categories: Paper Review tags: NLP date: 2021-01-19 13:01:02 +0000 last_modified_at: 2021-01-19 13:01:02 +0000 — Archive Link: https://arxiv.org/abs/1301.3781 Created: Dec 12, 2020 7:48 PM Field: NLP Paper Link: https://arxiv.org/pdf/1301.3781.pdf Status: completed Submit...","categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/Efficient-Estimation-of-Word-Representations-in-Vector-Space/",
        "teaser": null
      },{
        "title": "Kr Bert A Small Scale Korean Specific Language Model",
        "excerpt":"KR-BERT: A Small-Scale Korean-Specific Language Model title: KR-BERT: A Small-Scale Korean-Specific Language Model subtitle: KR-BERT categories: Paper Review tags: NLP Korean date: 2021-01-19 13:01:42 +0000 last_modified_at: 2021-01-19 13:01:42 +0000 — Archive Link: https://arxiv.org/abs/2008.03979 Created: Nov 13, 2020 9:50 PM Field: NLP Paper Link: https://arxiv.org/pdf/2008.03979.pdf Status: completed Submit Date: Aug 10,...","categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/KR-BERT-A-Small-Scale-Korean-Specific-Language-Model/",
        "teaser": null
      },{
        "title": "Mass Masked Sequence To Sequence Pre Training For Language Generation",
        "excerpt":"MASS: Masked Sequence to Sequence Pre-training for Language Generation title: MASS: Masked Sequence to Sequence Pre-training for Language Generation subtitle: MASS categories: Paper Review tags: NLP date: 2021-01-19 12:59:35 +0000 last_modified_at: 2021-01-19 12:59:35 +0000 — Archive Link: https://arxiv.org/abs/1905.02450 Created: Sep 21, 2020 3:19 PM Field: NLP Paper Link: https://arxiv.org/pdf/1905.02450.pdf Status:...","categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/MASS-Masked-Sequence-to-Sequence-Pre-training-for-Language-Generation/",
        "teaser": null
      },{
        "title": "Roberta A Robustly Optimized Bert Pretraining Approach",
        "excerpt":"RoBERTa: A Robustly Optimized BERT Pretraining Approach title: RoBERTa: A Robustly Optimized BERT Pretraining Approach subtitle: RoBERTa categories: Paper Review tags: NLP date: 2021-01-19 13:00:33 +0000 last_modified_at: 2021-01-19 13:00:33 +0000 — Archive Link: https://arxiv.org/abs/1907.11692 Created: Oct 5, 2020 1:08 AM Field: NLP Paper Link: https://arxiv.org/pdf/1907.11692.pdf Status: completed Submit Date: Jul...","categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/RoBERTa-A-Robustly-Optimized-BERT-Pretraining-Approach/",
        "teaser": null
      },{
        "title": "Sequence To Sequence Learning With Neural Networks",
        "excerpt":"Sequence to Sequence Learning with Neural Networks title: Sequence to Sequence Learning with Neural Networks subtitle: seq2seq categories: Paper Review tags: NLP date: 2021-01-19 12:59:59 +0000 last_modified_at: 2021-01-19 12:59:59 +0000 — Archive Link: https://arxiv.org/abs/1409.3215 Created: Sep 21, 2020 3:14 PM Field: NLP Paper Link: https://arxiv.org/pdf/1409.3215.pdf Status: completed Submit Date: Sep...","categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/Sequence-to-Sequence-Learning-with-Neural-Networks/",
        "teaser": null
      },{
        "title": "Subword Level Word Vector Representation For Korean",
        "excerpt":"Subword-level Word Vector Representation for Korean title: Subword-level Word Vector Representation for Korean subtitle: Korean BPE categories: Paper Review tags: NLP Korean date: 2021-01-19 13:00:25 +0000 last_modified_at: 2021-01-19 13:00:25 +0000 — Archive Link: https://www.aclweb.org/anthology/P18-1226/ Created: Sep 21, 2020 3:20 PM Field: NLP Paper Link: https://www.aclweb.org/anthology/P18-1226.pdf Status: completed Submit Date: Jul...","categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/Subword-level-Word-Vector-Representation-for-Korean/",
        "teaser": null
      },{
        "title": "Xlnet Generalized Autoregressive Pretraining For Language Understanding",
        "excerpt":"XLNet: Generalized Autoregressive Pretraining for Language Understanding title: XLNet: Generalized Autoregressive Pretraining for Language Understanding subtitle: XLNet categories: Paper Review tags: NLP date: 2021-01-19 12:59:48 +0000 last_modified_at: 2021-01-19 12:59:48 +0000 — Archive Link: https://arxiv.org/abs/1906.08237 Created: Sep 21, 2020 3:18 PM Field: NLP Paper Link: https://arxiv.org/pdf/1906.08237.pdf Status: not checked Submit Date:...","categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/XLNet-Generalized-Autoregressive-Pretraining-for-Language-Understanding/",
        "teaser": null
      },{
        "title": "test title v2",
        "excerpt":"\\[\\sum^N_{i=1}i\\] ","categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/test/",
        "teaser": null
      },{
        "title": "Neural Machine Translation of Rare Words with Subword Units",
        "excerpt":"Archive Link: https://arxiv.org/abs/1508.07909 Created: Sep 21, 2020 3:12 PM Field: NLP Paper Link: https://arxiv.org/pdf/1508.07909.pdf Status: completed Submit Date: Aug 15, 2015 Backgrounds BLEU Score (Bilingual Evaluation Understudy) score \\[BLEU=min\\left(1,\\frac{\\text{output length}}{\\text{reference_length}}\\right)\\left(\\prod_{i=1}^4precision_i\\right)^{\\frac{1}{4}}\\] reference sentence와 output sentence의 일치율을 나타내는 score이다. 3단계 절차를 거쳐 최종 BLEU Score를 도출해낸다. n-gram에서 순서쌍의 겹치는 정도 (Precision) Example...","categories": ["Paper Review"],
        "tags": ["NLP"],
        "url": "http://0.0.0.0:4000/paper%20review/Neural-Machine-Translation-of-Rare-Words-with-Subword-Units/",
        "teaser": null
      },{
        "title": "An Empirical Study Of Tokenization Strategies For Various Korean Nlp Tasks",
        "excerpt":"Archive Link: https://arxiv.org/abs/2010.02534 Created: Oct 10, 2020 11:29 PM Field: NLP Paper Link: https://arxiv.org/pdf/2010.02534.pdf Status: completed Submit Date: Oct 6, 2020 Introduction NLP에서 Tokenization은 전처리 과정에서 가장 중요한 issue 중 하나이다. 가장 적절한 Tokenization 전략을 찾기 위한 연구는 수도 없이 이루어져 왔다. 그 중 가장 대표적인 방식이 BPE이다. BPE는 많은...","categories": ["Paper","Review"],
        "tags": ["NLP","Korean"],
        "url": "http://0.0.0.0:4000/paper/review/An-Empirical-Study-of-Tokenization-Strategies-for-Various-Korean-NLP-Tasks/",
        "teaser": null
      },{
        "title": "Neural Machine Translation By Jointly Learning To Align And Translate",
        "excerpt":"Archive Link: https://arxiv.org/abs/1409.0473 Created: Sep 21, 2020 3:15 PM Field: NLP Paper Link: https://arxiv.org/pdf/1409.0473.pdf Status: completed Submit Date: Sep 1, 2014 Abstract 기존의 seq2seq model에서 사용된 LSTM을 사용한 encoder-decoder model은 sequential problem에서 뛰어난 성능을 보였다. 하지만 encoder에서 생성해낸 context vector를 decoder에서 sentence로 만들어내는 위와 같은 방식에서 고정된 vector size는 긴...","categories": ["Paper","Review"],
        "tags": ["NLP"],
        "url": "http://0.0.0.0:4000/paper/review/Neural-Machine-Translation-By-Jointly-Learning-To-Align-And-Translate/",
        "teaser": null
      }]

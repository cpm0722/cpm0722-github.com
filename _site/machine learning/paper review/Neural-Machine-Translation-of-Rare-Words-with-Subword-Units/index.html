<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.21.0 by Michael Rose
  Copyright 2013-2020 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="ko" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>[NLP 논문 리뷰] Neural Machine Translation of Rare Words with Subword Units (BPE) | Hansu Kim’s Dev Blog</title>
<meta name="description" content="Paper Info">


  <meta name="author" content="Hansu Kim">
  
  <meta property="article:author" content="Hansu Kim">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="ko_KR">
<meta property="og:site_name" content="Hansu Kim's Dev Blog">
<meta property="og:title" content="[NLP 논문 리뷰] Neural Machine Translation of Rare Words with Subword Units (BPE)">
<meta property="og:url" content="http://0.0.0.0:4000/machine%20learning/paper%20review/Neural-Machine-Translation-of-Rare-Words-with-Subword-Units/">


  <meta property="og:description" content="Paper Info">







  <meta property="article:published_time" content="2020-05-02T19:00:00-05:00">



  <meta property="article:modified_time" content="2020-05-02T19:00:00-05:00">



  

  


<link rel="canonical" href="http://0.0.0.0:4000/machine%20learning/paper%20review/Neural-Machine-Translation-of-Rare-Words-with-Subword-Units/">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": "Hansu Kim",
      "url": "http://0.0.0.0:4000/"
    
  }
</script>






<!-- end _includes/seo.html -->


<link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Hansu Kim's Dev Blog Feed">

<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css">

<!--[if IE]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->





    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single">
    <nav class="skip-links">
  <h2 class="screen-reader-text">Skip links</h2>
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
          Hansu Kim's Dev Blog
          <span class="site-subtitle">기술 블로그</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/machine-learning/">Machine Learning</a>
            </li><li class="masthead__menu-item">
              <a href="/operating-system/">Operating System</a>
            </li><li class="masthead__menu-item">
              <a href="/about-me/">About Me</a>
            </li></ul>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">토글 메뉴</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      



<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person">

  
    <div class="author__avatar">
      
        <img src="/assets/images/profile.jpg" alt="Hansu Kim" itemprop="image">
      
    </div>
  

  <div class="author__content">
    
      <h3 class="author__name" itemprop="name">Hansu Kim</h3>
    
    
      <div class="author__bio" itemprop="description">
        <p>ML, NLP를 주로 공부하는 학부생입니다.</p>

      </div>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">참고</button>
    <ul class="author__urls social-icons">
      
        <li itemprop="homeLocation" itemscope itemtype="https://schema.org/Place">
          <i class="fas fa-fw fa-map-marker-alt" aria-hidden="true"></i> <span itemprop="name">Republic of Korea</span>
        </li>
      

      
        
          
            <li><a href="mailto:cpm0722@gmail.com" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-envelope-square" aria-hidden="true"></i><span class="label">Email</span></a></li>
          
        
          
            <li><a href="https://www.facebook.com/profile.php?id=100003380546271" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-facebook-square" aria-hidden="true"></i><span class="label">Facebook</span></a></li>
          
        
          
            <li><a href="https://github.com/cpm0722" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i><span class="label">GitHub</span></a></li>
          
        
      

      

      
        <li>
          <a href="mailto:cpm0722@gmail.com">
            <meta itemprop="email" content="cpm0722@gmail.com" />
            <i class="fas fa-fw fa-envelope-square" aria-hidden="true"></i><span class="label">Email</span>
          </a>
        </li>
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="[NLP 논문 리뷰] Neural Machine Translation of Rare Words with Subword Units (BPE)">
    <meta itemprop="description" content="Paper Info">
    <meta itemprop="datePublished" content="2020-05-02T19:00:00-05:00">
    <meta itemprop="dateModified" content="2020-05-02T19:00:00-05:00">

		<script type="text/javascript" async
		src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
		</script>

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">[NLP 논문 리뷰] Neural Machine Translation of Rare Words with Subword Units (BPE)
</h1>
          


        </header>
      

      <section class="page__content" itemprop="text">
        
        <h2 id="paper-info">Paper Info</h2>

<p><a href="https://arxiv.org/abs/1508.07909">Archive Link</a></p>

<p><a href="https://arxiv.org/pdf/1508.07909.pdf">Paper Link</a></p>

<p>Submit Date: Aug 15, 2015</p>

<hr />

<h1 id="backgrounds">Backgrounds</h1>

<h2 id="bleu-score-bilingual-evaluation-understudy-score">BLEU Score (Bilingual Evaluation Understudy) score</h2>

\[BLEU=min\left(1,\frac{\text{output length}}{\text{reference_length}}\right)\left(\prod_{i=1}^4precision_i\right)^{\frac{1}{4}}\]

<p>reference sentence와 output sentence의 일치율을 나타내는 score이다. 3단계 절차를 거쳐 최종 BLEU Score를 도출해낸다.</p>

<ol>
  <li>n-gram에서 순서쌍의 겹치는 정도 (Precision)
    <ul>
      <li>Example
        <ul>
          <li>
            <p>output sentence</p>

            <p><strong>빛이 쐬는</strong> 노인은 <strong>완벽한</strong> 어두운 곳에서 <strong>잠든 사람과 비교할 때</strong> 강박증이 <strong>심해질</strong> 기회가 <strong>훨씬 높았다</strong></p>
          </li>
          <li>
            <p>true sentence</p>

            <p><strong>빛이 쐬는</strong> 사람은 <strong>완벽한</strong> 어둠에서 <strong>잠든 사람과 비교할 때</strong> 우울증이 <strong>심해질</strong> 가능성이 <strong>훨씬 높았다</strong></p>
          </li>
        </ul>
      </li>
      <li>
        <p>1-gram precision</p>

\[\frac{\text{\# of correct 1-gram in output sentence}}{\text{all 1-gram pair in output sentence}}=\frac{10}{14}\]
      </li>
      <li>
        <p>2-gram precision</p>

\[\frac{\text{\# of correct 2-gram in output sentence}}{\text{all 2-gram pair in output sentence}}=\frac{5}{13}\]
      </li>
      <li>
        <p>3-gram precision</p>

\[\frac{\text{\# of correct 3-gram in output sentence}}{\text{all 3-gram pair in output sentence}}=\frac{2}{12}\]
      </li>
      <li>
        <p>4-gram precision</p>

\[\frac{\text{\# of correct 4-gram in output sentence}}{\text{all 4-gram pair in output sentence}}=\frac{1}{11}\]
      </li>
    </ul>
  </li>
  <li>같은 단어에 대한 보정 (Clipping)
    <ul>
      <li>Example
        <ul>
          <li>
            <p>output sentence</p>

            <p><strong>The more</strong> decomposition <strong>the more</strong> flavor <strong>the</strong> food has</p>
          </li>
          <li>
            <p>true sentence</p>

            <p><strong>The more the</strong> merrier I always say</p>
          </li>
        </ul>
      </li>
      <li>
        <p>1-gram precision</p>

\[\frac{\text{\# of 1-gram in output sentence}}{\text{all 1-gram pair in output sentence}}=\frac{5}{9}\]
      </li>
      <li>
        <p>Clipping 1-gram precision</p>

\[\frac{\text{\# of 1-gram in output sentence}}{\text{all 1-gram pair in output sentence}}=\frac{3}{9}\]
      </li>
    </ul>
  </li>
  <li>문장 길이에 대한 보정 (Brevity Penalty)
    <ul>
      <li>Example
        <ul>
          <li>
            <p>output sentence</p>

            <p><strong>빛이 쐬는</strong> 노인은 <strong>완벽한</strong> 어두운 곳에서 잠듬</p>
          </li>
          <li>
            <p>true sentence</p>

            <p><strong>빛이 쐬는</strong> 사람은 <strong>완벽한</strong> 어둠에서 잠든 사람과 비교할 때 우울증이 심해질 가능성이 훨씬 높았다</p>
          </li>
        </ul>
      </li>
      <li>
        <p>brevity penalty</p>

\[min\left(1,\frac{\text{\# of words in output sentence}}{\text{\# of words in true sentence}}\right)=min\left(1,\frac{6}{14}\right)=\frac{3}{7}\]
      </li>
    </ul>
  </li>
  <li>최종 BLEU Score
    <ul>
      <li>Example
        <ul>
          <li>
            <p>output sentence</p>

            <p><strong>빛이 쐬는</strong> 노인은 완벽한 어두운 곳에서 <strong>잠든 사람과 비교할 때</strong> 강박증이 <strong>심해질</strong> 기회가 <strong>훨씬 높았다</strong></p>
          </li>
          <li>
            <p>true sentence</p>

            <p><strong>빛이 쐬는</strong> 사람은 <strong>완벽한</strong> 어둠에서 <strong>잠든 사람과 비교할 때</strong> 우울증이 <strong>심해질</strong> 가능성이 <strong>훨씬 높았다</strong></p>
          </li>
        </ul>
      </li>
      <li>
        <p>BLEU Score</p>

\[BLEU=min\left(1,\frac{\text{output length}}{\text{reference length}}\right)\left(\prod_{i=1}^4precision_i\right)^{\frac{1}{4}}\\=min\left(1,\frac{14}{14}\right)\times\left(\frac{10}{14}\times\frac{5}{13}\times\frac{2}{12}\times\frac{1}{11}\right)^{\frac{1}{4}}\]
      </li>
    </ul>
  </li>
</ol>

<p>출처: <a href="https://donghwa-kim.github.io/BLEU.html">https://donghwa-kim.github.io/BLEU.html</a></p>

<h1 id="abstract">Abstract</h1>

<p>기존의 NMT (Neural machine translation)는 모두 고정된 개수의 vocabulary 안에서 작업했다. 하지만 translation은 vocabulary 개수의 제한이 없는 open-vocabulary problem이기 OOV(out of vocabulary) word가 많이 발생할 수밖에 없다. 본 논문에서는 이러한 OOV 문제를 subword unit 활용해 해결하고자 했다.</p>

<h1 id="introduction">Introduction</h1>

<p>기존의 NMT Model은 OOV words에 대해 back-off model 사용해왔다. back-off model 대신 본 논문에서 제시할 subword unit을 사용할 경우 OOV 문제를 더 확실히 해결해 open-vocabulary problem에서 성능 향상을 이끌어낼 수 있다.</p>

<h1 id="subword-translation">Subword Translation</h1>

<p>현재의 language model에서 translatable하지 않더라도, 다른 language의 translation의 sub word를 사용하면 translate이 가능하다.</p>

<ol>
  <li>이름 등의 고유 명사는 음절 별로 대응시킨다.
    <ul>
      <li>Barack Obama (English; German)</li>
      <li>Барак Обама (Russian)</li>
      <li>バラク・オバマ (ba-ra-ku o-ba-ma) (Japanese)</li>
    </ul>
  </li>
  <li>동의어, 외래어 등 같은 origin을 갖는 단어들은 일정한 규칙을 갖고 변형되므로, character-level translation 사용한다.
    <ul>
      <li>claustrophobia (English)</li>
      <li>Klaustrophobie (German)</li>
      <li>Клаустрофобия (Klaustrofobiâ) (Russian)</li>
    </ul>
  </li>
  <li>복합어는 각각의 sub-word를 번역한 후 결합한다.
    <ul>
      <li>solar system (English)</li>
      <li>Sonnensystem (Sonne + System) (German)</li>
      <li>Naprendszer (Nap + Rendszer) (Hungarian)</li>
    </ul>
  </li>
</ol>

<p>위와 같은 규칙으로 german training data에서 가장 빈도 낮은 100개의 word를 분석하면 english data를 통해 56개의 복합어, 21개의 고유명사, 6개의 외래어 등을 찾아낼 수 있었다.</p>

<h2 id="related-work">Related Work</h2>

<p>OOV는 고유명사 (사람 이름, 지역명), 외래어 등에 대해서 자주 발생한다. 이를 해결하기 위해 character level로 word를 분리한 뒤, 각 character들이 일정한 기준을 충족할 경우 하나의 token으로 묶어 표현하는 방식을 채택했다. 이를 통해 text size는 줄어들게 된다. 이 때 단어를 subword로 구분하는 기존의 Segmentation algorithm을 사용하되,  좀 더 aggressive한 기준을 적용하고자 했다. vocabulary size와 text size는 서로 trade-off 관계이므로 vocabulary size가 감소한다면 시간/공간 복잡도는 낮아지겠지만  unknown word의 개수가 증가하게 된다.</p>

<h2 id="byte-pair-encoding-bpe">Byte Pair Encoding (BPE)</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
</pre></td><td class="rouge-code"><pre><span class="kn">import</span> <span class="nn">re</span><span class="p">,</span> <span class="n">collections</span>

<span class="k">def</span> <span class="nf">get_stats</span><span class="p">(</span><span class="n">vocab</span><span class="p">):</span>
	<span class="n">pairs</span> <span class="o">=</span> <span class="n">collections</span><span class="p">.</span><span class="n">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
	<span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">freq</span> <span class="ow">in</span> <span class="n">vocab</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
		<span class="n">symbols</span> <span class="o">=</span> <span class="n">word</span><span class="p">.</span><span class="n">split</span><span class="p">()</span>
		<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">symbols</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>
			<span class="n">pairs</span><span class="p">[</span><span class="n">symbols</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">symbols</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]]</span> <span class="o">+=</span> <span class="n">freq</span>
	<span class="k">return</span> <span class="n">pairs</span>

<span class="k">def</span> <span class="nf">merge_vocab</span><span class="p">(</span><span class="n">pair</span><span class="p">,</span> <span class="n">v_in</span><span class="p">):</span>
	<span class="n">v_out</span> <span class="o">=</span> <span class="p">{}</span>
	<span class="n">bigram</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="n">escape</span><span class="p">(</span><span class="s">' '</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">pair</span><span class="p">))</span>
	<span class="n">p</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span><span class="sa">r</span><span class="s">'(?&lt;!\S)'</span> <span class="o">+</span> <span class="n">bigram</span> <span class="o">+</span> <span class="sa">r</span><span class="s">'(?!\S)'</span><span class="p">)</span>
	<span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">v_in</span><span class="p">:</span>
		<span class="n">w_out</span> <span class="o">=</span> <span class="n">p</span><span class="p">.</span><span class="n">sub</span><span class="p">(</span><span class="s">''</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">pair</span><span class="p">),</span> <span class="n">word</span><span class="p">)</span>
		<span class="n">v_out</span><span class="p">[</span><span class="n">w_out</span><span class="p">]</span> <span class="o">=</span> <span class="n">v_in</span><span class="p">[</span><span class="n">word</span><span class="p">]</span>
	<span class="k">return</span> <span class="n">v_out</span>

<span class="n">vocab</span> <span class="o">=</span> <span class="p">{</span><span class="s">'low&lt;/w&gt;'</span> <span class="p">:</span> <span class="mi">5</span><span class="p">,</span> <span class="s">'lower&lt;/w&gt;'</span> <span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
				 <span class="s">'newest&lt;/w&gt;'</span> <span class="p">:</span> <span class="mi">6</span><span class="p">,</span> <span class="s">'widest&lt;/w&gt;'</span> <span class="p">:</span> <span class="mi">3</span><span class="p">}</span>

<span class="n">num_merges</span> <span class="o">=</span> <span class="mi">10</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_merges</span><span class="p">):</span>
	<span class="n">pairs</span> <span class="o">=</span> <span class="n">get_stats</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span>
	<span class="n">best</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">pairs</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="n">pairs</span><span class="p">.</span><span class="n">get</span><span class="p">)</span>
	<span class="n">vocab</span> <span class="o">=</span> <span class="n">merge_vocab</span><span class="p">(</span><span class="n">best</span><span class="p">,</span> <span class="n">vocab</span><span class="p">)</span>
	<span class="k">print</span><span class="p">(</span><span class="n">best</span><span class="p">)</span>

<span class="c1"># r .  -&gt;  r.
# l o  -&gt;  lo
# lo w -&gt;  low
# e r. -&gt;  er.
</span></pre></td></tr></tbody></table></code></pre></div></div>

<p>BPE는 가장 빈도가 높은 pair of bytes부터 하나의 single byte로 치환해 저장하는 압축 algorithm이다.</p>

<p>BPE는 다음과 같은 과정을 따른다.</p>

<ol>
  <li>word를 character의 sequence로 변환 후 end symbol  ·  추가</li>
  <li>모든 character의 pair를 센 후 가장 빈도가 높은 pair of character (‘A’, ‘B’)를 새로운 symbol ‘AB’ (character n-gram)로 치환</li>
  <li>2번 단계를 원하는 횟수만큼(vocabulary size만큼 token이 생성될 때까지) 반복</li>
</ol>

<p>BPE의 반복 횟수는 vocabulary size라는 hyperparameter에 따라 결정된다.</p>

<ul>
  <li>예시
    <ol>
      <li>
        <p>train sentences</p>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre></td><td class="rouge-code"><pre> <span class="n">sentence</span> <span class="o">=</span> <span class="p">[</span> 
 <span class="s">'black bug bit a black bear but is the black bear that the big black bug bit'</span><span class="p">,</span>
 <span class="s">'a big bug bit the little beetle but the little beetle bit the big bug back'</span><span class="p">.</span>
 <span class="s">'the better with the butter is the batter that is better'</span>
 <span class="p">]</span>
</pre></td></tr></tbody></table></code></pre></div>        </div>
      </li>
      <li>
        <p>count segments</p>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre></td><td class="rouge-code"><pre> <span class="p">[(</span><span class="s">'t h e &lt;/w&gt;'</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span> <span class="p">(</span><span class="s">'b l a c k &lt;/w&gt;'</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="p">(</span><span class="s">'b i t &lt;/w&gt;'</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span>
  <span class="p">(</span><span class="s">'i s &lt;/w&gt;'</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="p">(</span><span class="s">'b i g &lt;/w&gt;'</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="p">(</span><span class="s">'b e a r &lt;/w&gt;'</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
  <span class="p">(</span><span class="s">'b u t &lt;/w&gt;'</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="p">(</span><span class="s">'t h a t &lt;/w&gt;'</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="p">(</span><span class="s">'l i t t l e &lt;/w&gt;'</span><span class="p">,</span> <span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
  <span class="p">(</span><span class="s">'b e e t l e &lt;/w&gt;'</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="p">(</span><span class="s">'b e t t e r &lt;/w&gt;'</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="p">(</span><span class="s">'b a c k &lt;/w&gt;'</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
  <span class="p">(</span><span class="s">'w i t h &lt;/w&gt;'</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="s">'b u t t e r &lt;/w&gt;'</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="s">'b a t t e r &lt;/w&gt;'</span><span class="p">,</span> <span class="mi">1</span><span class="p">)]</span>
</pre></td></tr></tbody></table></code></pre></div>        </div>
      </li>
      <li>
        <p>count bi-grams</p>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre></td><td class="rouge-code"><pre> <span class="p">[((</span><span class="s">'t'</span><span class="p">,</span> <span class="s">'h'</span><span class="p">),</span> <span class="mi">11</span><span class="p">),</span>
  <span class="p">((</span><span class="s">'h'</span><span class="p">,</span> <span class="s">'e'</span><span class="p">),</span> <span class="mi">8</span><span class="p">).</span>
  <span class="p">((</span><span class="s">'t'</span><span class="p">,</span> <span class="s">'&lt;/w&gt;'</span><span class="p">),</span> <span class="mi">8</span><span class="p">),</span>
  <span class="p">((</span><span class="s">'g'</span><span class="p">,</span> <span class="s">'&lt;/w&gt;'</span><span class="p">),</span> <span class="mi">7</span><span class="p">)]</span>
</pre></td></tr></tbody></table></code></pre></div>        </div>
      </li>
      <li>
        <p>add merge-rules</p>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre></td><td class="rouge-code"><pre> <span class="p">(</span><span class="s">'t'</span><span class="p">,</span> <span class="s">'h'</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">th</span>
 <span class="p">(</span><span class="s">'h'</span><span class="p">,</span> <span class="s">'e'</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">he</span>
 <span class="p">(</span><span class="s">'t'</span><span class="p">,</span> <span class="s">'&lt;/w&gt;'</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">t</span><span class="o">&lt;/</span><span class="n">w</span><span class="o">&gt;</span>
 <span class="p">(</span><span class="s">'g'</span><span class="p">,</span> <span class="s">'&lt;/w&gt;'</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">g</span><span class="o">&lt;/</span><span class="n">w</span><span class="o">&gt;</span>
</pre></td></tr></tbody></table></code></pre></div>        </div>
      </li>
    </ol>
  </li>
</ul>

<h1 id="evaluation">Evaluation</h1>

<h2 id="subword-statistics">Subword statistics</h2>

<p><img src="/assets/images/2020-05-03-Neural-Machine-Translation-of-Rare-Words-with-Subword-Units/01.jpg" alt="01.jpg" /></p>

<ul>
  <li>
    <h1 id="tokens-text-size">tokens: text size</h1>
  </li>
  <li>
    <h1 id="types-vocabulary-size-token-개수">types: vocabulary size, token 개수</h1>
  </li>
  <li>
    <h1 id="unk-unknown-word-oov-word의-개수">UNK: unknown word (OOV word)의 개수</h1>
  </li>
</ul>

<h2 id="translation-experiments">Translation experiments</h2>

<p><img src="/assets/images/2020-05-03-Neural-Machine-Translation-of-Rare-Words-with-Subword-Units/02.jpg" alt="02.jpg" /></p>

<p><img src="/assets/images/2020-05-03-Neural-Machine-Translation-of-Rare-Words-with-Subword-Units/03.jpg" alt="03.jpg" /></p>

<ul>
  <li>W Unk: back-off dictionary를 사용하지 않은 model이다.</li>
  <li>W Dict: back-off dictionary를 사용한 model이다.</li>
  <li>C2-50k: char-bigram을 사용한 model이다.</li>
  <li>CHR F3: 인간의 판단과 일치율</li>
  <li>unigram F1: BLEU unigram(brevity penalty 제외)와 Recall의 조합</li>
</ul>

<p>source와 target 각각 따로 BPE를 수행하는 BPE보다 동시에 수행하는 BPE joint가 더 좋은 성능을 보였다.</p>

<h1 id="analysis">Analysis</h1>

<h2 id="unigram-accuracy">Unigram accuracy</h2>

<p><img src="/assets/images/2020-05-03-Neural-Machine-Translation-of-Rare-Words-with-Subword-Units/04.jpg" alt="04.jpg" /></p>

<p><img src="/assets/images/2020-05-03-Neural-Machine-Translation-of-Rare-Words-with-Subword-Units/05.jpg" alt="05.jpg" /></p>

<h2 id="manual-analysis">Manual Analysis</h2>

<p><img src="/assets/images/2020-05-03-Neural-Machine-Translation-of-Rare-Words-with-Subword-Units/06.jpg" alt="06.jpg" /></p>

<p><img src="/assets/images/2020-05-03-Neural-Machine-Translation-of-Rare-Words-with-Subword-Units/07.jpg" alt="07.jpg" /></p>

<h1 id="conclusion">Conclusion</h1>

<p>OOV 문제를 해결해 NMT와 같은 open-vocabulary translation에서 좋은 성능을 보였다. 기존에 OOV를 해결하기 위해 사용되던 back-off translation model보다 더 좋은 성능을 보였다.</p>

        
      </section>

      <footer class="page__meta">
        
        
  


  
  
  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> 태그: </strong>
    <span itemprop="keywords">
    
      
      
      <a href="/tags/#nlp" class="page__taxonomy-item" rel="tag">NLP</a>
    
    </span>
  </p>




  


  
  
  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> 카테고리: </strong>
    <span itemprop="keywords">
    
      
      
      <a href="/categories/#machine-learning" class="page__taxonomy-item" rel="tag">Machine Learning</a><span class="sep">, </span>
    
      
      
      <a href="/categories/#paper-review" class="page__taxonomy-item" rel="tag">Paper Review</a>
    
    </span>
  </p>


        
  <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> 업데이트:</strong> <time datetime="2020-05-03">May 3, 2020</time></p>


      </footer>

      <!--<section class="page__share">
  
    <h4 class="page__share-title">공유하기</h4>
  

  <a href="https://twitter.com/intent/tweet?text=%5BNLP+%EB%85%BC%EB%AC%B8+%EB%A6%AC%EB%B7%B0%5D+Neural+Machine+Translation+of+Rare+Words+with+Subword+Units+%28BPE%29%20http%3A%2F%2F0.0.0.0%3A4000%2Fmachine%2520learning%2Fpaper%2520review%2FNeural-Machine-Translation-of-Rare-Words-with-Subword-Units%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="공유하기 Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2F0.0.0.0%3A4000%2Fmachine%2520learning%2Fpaper%2520review%2FNeural-Machine-Translation-of-Rare-Words-with-Subword-Units%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="공유하기 Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=http%3A%2F%2F0.0.0.0%3A4000%2Fmachine%2520learning%2Fpaper%2520review%2FNeural-Machine-Translation-of-Rare-Words-with-Subword-Units%2F" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="공유하기 LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>
-->

      
<nav class="pagination">
	
		<a href="#" class="pagination--pager disabled">이전</a>
	
	
		<a href="/machine%20learning/paper%20review/Sequence-to-Sequence-Learning-with-Neural-Networks/" class="pagination--pager" title="[NLP 논문 리뷰] Sequence To Sequence Learning With Neural Networks (Seq2Seq)
">다음</a>
	
</nav>


    </div>

    
  </article>

  
  
    <div class="page__related">
      <h4 class="page__related-title">관련 포스트</h4>
      <div class="grid__wrapper">
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/machine%20learning/paper%20review/Deep-contextualized-word-representations/" rel="permalink">[NLP 논문 리뷰] Deep Contextualized Word Representations (ELMo)
</a>
      
    </h2>
    


    <p class="archive__item-excerpt" itemprop="description">Paper Info
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/machine%20learning/paper%20review/Efficient-Estimation-of-Word-Representations-in-Vector-Space/" rel="permalink">[NLP 논문 리뷰] Efficient Estimation Of Word Representations In Vector Space (Word2Vec)
</a>
      
    </h2>
    


    <p class="archive__item-excerpt" itemprop="description">Paper Info
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/machine%20learning/paper%20review/KR-BERT-A-Small-Scale-Korean-Specific-Language-Model/" rel="permalink">[NLP 논문 리뷰] KR-BERT: A Small Scale Korean Specific Language Model
</a>
      
    </h2>
    


    <p class="archive__item-excerpt" itemprop="description">Paper Info
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/machine%20learning/paper%20review/An-Empirical-Study-of-Tokenization-Strategies-for-Various-Korean-NLP-Tasks/" rel="permalink">[NLP 논문 리뷰] An Empirical Study of Tokenization Strategies for Various Korean NLP Tasks
</a>
      
    </h2>
    


    <p class="archive__item-excerpt" itemprop="description">Paper Info
</p>
  </article>
</div>

        
      </div>
    </div>
  
  
</div>

    </div>

    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>참고:</strong></li>
    

    
      
        
          <li><a href="mailto:cpm0722@gmail.com" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-envelope-square" aria-hidden="true"></i> Email</a></li>
        
      
        
          <li><a href="https://www.facebook.com/profile.php?id=100003380546271" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-facebook-square" aria-hidden="true"></i> Facebook</a></li>
        
      
        
          <li><a href="https://github.com/cpm0722" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
    

    <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> RSS</a></li>
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2021 Hansu Kim. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>







    
  <script>
    var disqus_config = function () {
      this.page.url = "http://0.0.0.0:4000/machine%20learning/paper%20review/Neural-Machine-Translation-of-Rare-Words-with-Subword-Units/";  /* Replace PAGE_URL with your page's canonical URL variable */
      this.page.identifier = "/machine%20learning/paper%20review/Neural-Machine-Translation-of-Rare-Words-with-Subword-Units"; /* Replace PAGE_IDENTIFIER with your page's unique identifier variable */
    };
    (function() { /* DON'T EDIT BELOW THIS LINE */
      var d = document, s = d.createElement('script');
      s.src = 'https://cpm0722.disqus.com/embed.js';
      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
    })();
  </script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>


  





  </body>
</html>

<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.21.0 by Michael Rose
  Copyright 2013-2020 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="ko" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>[NLP 논문 리뷰] Attention Is All You Need (Transformer) | Hansu Kim’s Dev Blog</title>
<meta name="description" content="Paper Info">


  <meta name="author" content="Hansu Kim">
  
  <meta property="article:author" content="Hansu Kim">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="ko_KR">
<meta property="og:site_name" content="Hansu Kim's Dev Blog">
<meta property="og:title" content="[NLP 논문 리뷰] Attention Is All You Need (Transformer)">
<meta property="og:url" content="http://0.0.0.0:4000/machine%20learning/paper%20review/Attention-is-All-You-Need/">


  <meta property="og:description" content="Paper Info">







  <meta property="article:published_time" content="2020-07-04T19:00:00-05:00">



  <meta property="article:modified_time" content="2020-07-04T19:00:00-05:00">



  

  


<link rel="canonical" href="http://0.0.0.0:4000/machine%20learning/paper%20review/Attention-is-All-You-Need/">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": "Hansu Kim",
      "url": "http://0.0.0.0:4000/"
    
  }
</script>






<!-- end _includes/seo.html -->


<link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Hansu Kim's Dev Blog Feed">

<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css">

<!--[if IE]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->





    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single">
    <nav class="skip-links">
  <h2 class="screen-reader-text">Skip links</h2>
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
          Hansu Kim's Dev Blog
          <span class="site-subtitle">기술 블로그</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/machine-learning/">Machine Learning</a>
            </li><li class="masthead__menu-item">
              <a href="/operating-system/">Operating System</a>
            </li><li class="masthead__menu-item">
              <a href="/system-programming/">System Programming</a>
            </li><li class="masthead__menu-item">
              <a href="/about-me/">About Me</a>
            </li></ul>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">토글 메뉴</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      



<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person">

  
    <div class="author__avatar">
      
        <img src="/assets/images/profile.jpg" alt="Hansu Kim" itemprop="image">
      
    </div>
  

  <div class="author__content">
    
      <h3 class="author__name" itemprop="name">Hansu Kim</h3>
    
    
      <div class="author__bio" itemprop="description">
        <p>ML, NLP를 주로 공부하는 학부생입니다.</p>

      </div>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">팔로우</button>
    <ul class="author__urls social-icons">
      
        <li itemprop="homeLocation" itemscope itemtype="https://schema.org/Place">
          <i class="fas fa-fw fa-map-marker-alt" aria-hidden="true"></i> <span itemprop="name">Republic of Korea</span>
        </li>
      

      
        
          
            <li><a href="mailto:cpm0722@gmail.com" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-envelope-square" aria-hidden="true"></i><span class="label">Email</span></a></li>
          
        
          
            <li><a href="https://www.facebook.com/profile.php?id=100003380546271" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-facebook-square" aria-hidden="true"></i><span class="label">Facebook</span></a></li>
          
        
          
            <li><a href="https://github.com/cpm0722" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i><span class="label">GitHub</span></a></li>
          
        
      

      

      
        <li>
          <a href="mailto:cpm0722@gmail.com">
            <meta itemprop="email" content="cpm0722@gmail.com" />
            <i class="fas fa-fw fa-envelope-square" aria-hidden="true"></i><span class="label">Email</span>
          </a>
        </li>
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="[NLP 논문 리뷰] Attention Is All You Need (Transformer)">
    <meta itemprop="description" content="Paper Info">
    <meta itemprop="datePublished" content="2020-07-04T19:00:00-05:00">
    <meta itemprop="dateModified" content="2020-07-04T19:00:00-05:00">

		<script type="text/javascript" async
		src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
		</script>

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">[NLP 논문 리뷰] Attention Is All You Need (Transformer)
</h1>
          


        </header>
      

      <section class="page__content" itemprop="text">
        
        <h2 id="paper-info">Paper Info</h2>

<p><a href="https://arxiv.org/abs/1706.03762">Archive Link</a></p>

<p><a href="https://arxiv.org/pdf/1706.03762.pdf">Paper Link</a></p>

<p>Submit Date: Jun 12, 2017</p>

<hr />

<h1 id="introduction">Introduction</h1>

<p>RNN과 LSTM을 사용한 Neural Network 접근 방식은 Sequencial Transduction Problem에서 매우 좋은 성능을 달성했다. 그 중 특히 Encoder-Decoder를 사용한 Attention Model이 state-of-art를 달성했다. 하지만 Recurrent Model은 본질적으로 한계가 존재하는데, 바로 Parallelization이 불가능하다는 문제점이다. 이는 Sequence의 길이가 긴 상황에서 매우 큰 단점이 된다. 최근의 연구들이 computation을 최소화하는 방향으로 Model의 성능 향상을 이뤄내기는 했지만, 결국 Recurrent Model의 본질적인 문제는 해결하지 못했다. 본 논문에서 소개하는 Transformer Model은 RNN을 제거해 Recurrent Model의 문제점에서 벗어났고,  Parallelization을 가능하게 했다. 이에 따라 매우 좋은 성능을 달성했다.</p>
<h1 id="model-architecture">Model Architecture</h1>

<p>Transformer Model은 attention seq2seq model과 비슷한 구조를 지닌다. Encoder-Decoder가 존재하고, fixed length의 하나의 context vector를 사용하는 방식이 아닌, 하나의 output word마다 각기 다른 새로운 attention을 갖는 context vector를 생성해 활용한다. 차이점은 RNN을 제거했다는 점이다. NLP에서 RNN을 사용하는 가장 큰 이유는 sequential 정보를 유지하기 위함(각 단어들의 순서 및 위치 정보를 활용하기 위함)이다. Transformer에서는 RNN 대신 FC Layer를 사용하되, 각 word vector마다 positional Encoding 과정을 추가해 각 word의 position 정보를 word vector 안에 추가했다.</p>

<p><img src="/assets/images/2020-07-05-Attention-is-All-You-Need/01.jpg" alt="01.jpg" /></p>

<h3 id="encoder">Encoder</h3>

<p>Transformer의 Encoder는 6개의 동일한 Encoder Layer를 Stack 구조로 쌓아올린 형태이다. 각각의 Encoder Layer는 2개의 SubLayer로 구분되는데, Self Attention Layer와 Feed Forward Layer이다. Self Attention Layer에서는 word vector에 attention을 담는다. 이 결과를 Feed Forward Layer를 거쳐 다음 Encoder Layer의 input으로 넣는다. 이러한 과정을 6회 반복해 최종 output을 Decoder로 넘겨준다. 각각의 Self Attention Layer는 Attention을 word vector에 담는 기능을 수행하는데, 이를 중첩해 반복하면 단어 단위가 아닌 점차 전체 문맥에서의 Attention을 담게 된다. Self Attention Layer의 동작은 아래에서 자세히 살펴보겠다. 각 SubLayer의 결과에는 항상 Normalization이 수행되는데, 이 때 SubLayer를 통과하기 전 input이 그대로 더해지게 된다. 이를 Residual Connection이라고 한다. Residual Connection은  BackProp시 gradient vanishing을 최소화시켜 Model 성능 향상을 돕는다.</p>

<h3 id="self-attention-layer">Self Attention Layer</h3>

<p><img src="/assets/images/2020-07-05-Attention-is-All-You-Need/02.jpg" alt="02.jpg" /></p>

<p><img src="/assets/images/2020-07-05-Attention-is-All-You-Need/03.jpg" alt="03.jpg" /></p>

\[Attention\left(Q,K,V\right)=softmax\left(\frac{QK^T}{\sqrt{d_k}}\right)V\]

<p>Self Attention Layer는 word vector에 attention을 추가하는 layer이다. Query, Key, Value가 사용된다. Query, Key, Value는 각각 input word에 대해 특정한 weight matrix를 곱한 결과값이다. 해당 weight matrix들은 학습되는 parameter들이다. 우선 각 단어들을 기준으로 나눠 살펴보자.</p>

<p>현재 단어의 word vector를 \(x_i\)라고 하면, query는 \(x_i\)에 대한 query \(q_i\)이다. input word \(x_i\)에 대한 연관성을 묻는 Query(질의)이다. Key와 Value는 \(x_i\)와 연관성을 검사하는 대상 \(x_j\)에 대한 값이다. 사실 Key와 Value는 동일한 값인데, 사용되는 위치만이 다르다. \(q_i\)와 \(k_j\)를 곱하면 \(x_i\)와 \(x_j\)가 연관된 수치를 의미하는 score가 생성된다. 이를 Dependency라고 한다. Dependency를 softmax에 넣으면 확률값이 생성된다. 이를 \(v_j\)와 곱하고, 해당 값을 모두 더하면 attention이 담기긴 vector가 완성된다.</p>

<p>query, key, value는 모두 위의 단어 예시에서 vector 단위였다. 이를 concatenate해 하나의 matrix로 만들 수 있다. 그렇다면 dot product 연산으로 모든 sentence 전체에 대한 attention matrix를 구할 수 있다. 이렇게 병렬 처리를 함으로써 model의 학습 속도를 증가시켰다. 위의 좌측 이미지는 이러한 과정을 표현한 것이다. Scale은 \(d_k\)의 제곱근으로 Q와 K의 dot product 결과값을 나누는 것인데, 절댓값이 너무 커져 softmax에서 gradient가 줄어드는 것을 방지하기 위함이다.</p>

<p>사실 이런 dot product를 통해 attention을 구하는 하나의 과정 자체도 병렬적으로 동시에 여러 번 수행된다. 위의 우측 이미지를 보면 dot product 연산을 병렬적으로 h번 수행해 모두 concatuation을 해 하나의 matrix를 생성한다는 것을 알 수 있다. 해당 matrix를 기존의 input matrix size로 변경하기 위해 다른 weight matrix \(W\)와 dot product를 수행한 뒤 그 결과를 다음 Feed Forward Layer에 넘기게 된다.</p>

<h3 id="decoder">Decoder</h3>

<p><img src="/assets/images/2020-07-05-Attention-is-All-You-Need/04.jpg" alt="04.jpg" /></p>

<p>출처: <a href="https://jalammar.github.io/illustrated-transformer/">https://jalammar.github.io/illustrated-transformer/</a></p>

<p>Decoder는 Encoder와 매우 비슷한 구조를 갖는다. 동일한 6개의 Decoder Layer를 Stack 구조로 쌓아올린 형태이고, 각각의 Decoder Layer에는 Encoder Layer와 같은 Self Attention Layer와 Feed Forward Layer가 존재한다. Encoder와의 차이점은 Self Attention Layer 이전에 Masked Layer가 있다는 점이다. 6개의 Decoder Layer를 거친 최종 output 값을 softmax를 사용해 확률값으로 구하는데, 이는 i번째 output word에 대한 조건부 확률값이다. 6개의 Decoder Layer는 모두 마지막 Encoder Layer의 output인 context vector를 input으로 받고, 동시에 이전 단계의 Decoder Layer에서 생성된 attention vector도 input으로 받는다. Masked Attention Layer에서는 현재 word의 기준으로 이후 word에 mask를 씌운 vector값이다. 현재 시점의 attention을 생성할 때 이후 word들의 영향을 없애기 위함이다.</p>

        
      </section>

      <footer class="page__meta">
        
        
  


  
  
  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> 태그: </strong>
    <span itemprop="keywords">
    
      
      
      <a href="/tags/#nlp" class="page__taxonomy-item" rel="tag">NLP</a>
    
    </span>
  </p>




  


  
  
  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> 카테고리: </strong>
    <span itemprop="keywords">
    
      
      
      <a href="/categories/#machine-learning" class="page__taxonomy-item" rel="tag">Machine Learning</a><span class="sep">, </span>
    
      
      
      <a href="/categories/#paper-review" class="page__taxonomy-item" rel="tag">Paper Review</a>
    
    </span>
  </p>


        
  <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> 업데이트:</strong> <time datetime="2020-07-05">July 5, 2020</time></p>


      </footer>

      <!--<section class="page__share">
  
    <h4 class="page__share-title">공유하기</h4>
  

  <a href="https://twitter.com/intent/tweet?text=%5BNLP+%EB%85%BC%EB%AC%B8+%EB%A6%AC%EB%B7%B0%5D+Attention+Is+All+You+Need+%28Transformer%29%20http%3A%2F%2F0.0.0.0%3A4000%2Fmachine%2520learning%2Fpaper%2520review%2FAttention-is-All-You-Need%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="공유하기 Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2F0.0.0.0%3A4000%2Fmachine%2520learning%2Fpaper%2520review%2FAttention-is-All-You-Need%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="공유하기 Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=http%3A%2F%2F0.0.0.0%3A4000%2Fmachine%2520learning%2Fpaper%2520review%2FAttention-is-All-You-Need%2F" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="공유하기 LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>
-->

      
<nav class="pagination">
	
		<a href="/system%20programming/unix/Sync-Manager/" class="pagination--pager" title="[System Programming] 동기화 프로그램 구현
">이전</a>
	
	
		<a href="/machine%20learning/paper%20review/BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding/" class="pagination--pager" title="[NLP 논문 리뷰] BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding
">다음</a>
	
</nav>


    </div>

    
  </article>

  
  
    <div class="page__related">
      <h4 class="page__related-title">최근 포스트</h4>
      <div class="grid__wrapper">
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/machine%20learning/paper%20review/Deep-contextualized-word-representations/" rel="permalink">[NLP 논문 리뷰] Deep Contextualized Word Representations (ELMo)
</a>
      
    </h2>
    


    <p class="archive__item-excerpt" itemprop="description">Paper Info
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/machine%20learning/paper%20review/Efficient-Estimation-of-Word-Representations-in-Vector-Space/" rel="permalink">[NLP 논문 리뷰] Efficient Estimation Of Word Representations In Vector Space (Word2Vec)
</a>
      
    </h2>
    


    <p class="archive__item-excerpt" itemprop="description">Paper Info
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/machine%20learning/paper%20review/KR-BERT-A-Small-Scale-Korean-Specific-Language-Model/" rel="permalink">[NLP 논문 리뷰] KR-BERT: A Small Scale Korean Specific Language Model
</a>
      
    </h2>
    


    <p class="archive__item-excerpt" itemprop="description">Paper Info
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/machine%20learning/paper%20review/An-Empirical-Study-of-Tokenization-Strategies-for-Various-Korean-NLP-Tasks/" rel="permalink">[NLP 논문 리뷰] An Empirical Study of Tokenization Strategies for Various Korean NLP Tasks
</a>
      
    </h2>
    


    <p class="archive__item-excerpt" itemprop="description">Paper Info
</p>
  </article>
</div>

        
      </div>
    </div>
  
  
</div>

    </div>

    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>팔로우</strong></li>
    

    
      
        
          <li><a href="mailto:cpm0722@gmail.com" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-envelope-square" aria-hidden="true"></i> Email</a></li>
        
      
        
          <li><a href="https://www.facebook.com/profile.php?id=100003380546271" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-facebook-square" aria-hidden="true"></i> Facebook</a></li>
        
      
        
          <li><a href="https://github.com/cpm0722" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
    

    <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> RSS</a></li>
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2021 Hansu Kim. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>







    
  <script>
    var disqus_config = function () {
      this.page.url = "http://0.0.0.0:4000/machine%20learning/paper%20review/Attention-is-All-You-Need/";  /* Replace PAGE_URL with your page's canonical URL variable */
      this.page.identifier = "/machine%20learning/paper%20review/Attention-is-All-You-Need"; /* Replace PAGE_IDENTIFIER with your page's unique identifier variable */
    };
    (function() { /* DON'T EDIT BELOW THIS LINE */
      var d = document, s = d.createElement('script');
      s.src = 'https://cpm0722.disqus.com/embed.js';
      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
    })();
  </script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>


  





  </body>
</html>

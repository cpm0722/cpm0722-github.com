<?xml version="1.0" encoding="utf-8"?><?xml-stylesheet type="text/xml" href="https://cpm0722.github.io/feed.xslt.xml"?><feed xmlns="http://www.w3.org/2005/Atom"><generator uri="http://jekyllrb.com" version="3.2.1">Jekyll</generator><link href="https://cpm0722.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://cpm0722.github.io/" rel="alternate" type="text/html" /><updated>2021-01-29T09:33:58-06:00</updated><id>https://cpm0722.github.io/</id><title type="html">Hansu Kim</title><subtitle>Hansu Kim's Development Blog</subtitle><entry><title type="html">[NLP 논문 구현] pytorch로 구현하는 Transformer (Attention is All You Need)</title><link href="https://cpm0722.github.io/Transformer-in-pytorch/" rel="alternate" type="text/html" title="[NLP 논문 구현] pytorch로 구현하는 Transformer (Attention is All You Need)" /><published>2021-01-27T18:00:00-06:00</published><updated>2021-01-27T18:00:00-06:00</updated><id>https://cpm0722.github.io/Transformer-in-pytorch</id><content type="html" xml:base="https://cpm0722.github.io/Transformer-in-pytorch/">&lt;h3 id=&quot;paper-link&quot;&gt;&lt;a href=&quot;https://arxiv.org/pdf/1706.03762.pdf&quot;&gt;Paper Link&lt;/a&gt;&lt;/h3&gt;

&lt;h1 id=&quot;why-transformer&quot;&gt;Why Transformer?&lt;/h1&gt;

&lt;p&gt;Transformer는 2017년에 등장해 NLP 분야에서 혁신적인 성과를 이끌어낸 논문이다. 비단 NLP뿐만이 아니라 다른 ML Domain 내에서도 많은 insight를 주어 활용되고 있다.&lt;/p&gt;

&lt;p&gt;Transformer의 가장 큰 contribution은 이전의 RNN(Recurrent Neural Network) model이 불가능했던 병렬 처리를 가능케 했다는 점이다. GPU를 사용함으로써 얻는 가장 큰 이점은 병렬 처리를 한다는 것인데, RNN과 같은 model은 GPU 발전의 혜택을 제대로 누리지 못했다. 앞으로 GPU의 발전은 더욱 가속화될 것이기에, Recurrent network의 한계는 점점 더 두드러질 것이다. Recurrent network를 사용하는 이유는 텍스트, 음성 등의 sequential한 data를 처리하기 위함인데, sequential하다는 것은 등장 시점(또는 위치)을 정보로 취급한다는 의미이다. 따라서 context vector를 앞에서부터 순차적으로 생성해내고, 그 context vector를 이후 시점에서 활용하게 방식으로 구현한다. 즉, 이후 시점의 연산은 앞 시점의 연산에 의존적이다. 따라서 앞 시점의 연산이 끝나지 않을 경우, 그 뒤의 연산을 수행할 수 없다. 이러한 이유로 RNN 계열의 model은 병렬 처리를 제대로 수행할 수 없다.&lt;/p&gt;

&lt;p&gt;Transformer는 이를 극복했다. Attention 개념을 도입해 어떤 특정 시점에 집중하고, Positional Encoding을 사용해 sequential한 위치 정보를 보존했으며, 이후 시점에 대해 masking을 적용해 이전 시점의 값만이 이후에 영향을 미치도록 제한했다. 그러면서도 모든 과정을 병렬처리 가능하도록 구현했다. Transformer를 직접 pytorch를 사용해 구현하고, 학습시키며 이러한 특징들을 이해해보자. 본 포스트의 모든 code는 본인이 직접 작성했으나, 다소 참조한 부분의 출처는 포스트 하단의 Reference에 명시한다.&lt;/p&gt;

&lt;h1 id=&quot;prerequisite&quot;&gt;Prerequisite&lt;/h1&gt;

&lt;p&gt;Machine Learning에 대한 기본적인 지식(Back Propagation, Activation Function, Optimizer, Softmax, KL Divergence, Drop-out, Normalization, Regularization, RNN 등)과 NLP의 기본적인 지식(tokenizing, word embedding, vocabulary, Machine Translation, BLEU Score 등)을 안다고 가정한다. 또한 Python, pytorch를 사용해 간단한 model을 만들어낼 수 있다는 것을 전제로 한다.&lt;/p&gt;

&lt;h1 id=&quot;model-of-transformer&quot;&gt;Model of Transformer&lt;/h1&gt;

&lt;h2 id=&quot;transformer의-개괄적인-구조&quot;&gt;Transformer의 개괄적인 구조&lt;/h2&gt;

&lt;p&gt;Transformer는 input sentence를 넣어 output sentence를 생성해내는 model이다. input과 동일한 sentence를 만들어낼 수도, input의 역방향 sentence를 만들어낼 수도, 같은 의미의 다른 언어로 된 sentence를 만들어낼 수도 있다. 이는 model의 train 과정에서 정해지는 것으로, label을 어떤 sentence로 정할 것인가에 따라 달라진다. 결국 Transformer는 sentence 형태의 input을 사용해 sentence 형태의 output을 만들어내는 함수로 이해할 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-28-Transformer-in-pytorch/transformer_simple.png&quot; alt=&quot;transformer_simple.png&quot; /&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y=\text{Transformer}(x)\\x,\ y\text{ : sentence}&lt;/script&gt;

&lt;p&gt;전체적인 생김새를 살펴보자.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-28-Transformer-in-pytorch/transformer_structure_in_paper.png&quot; alt=&quot;transformer_structure_in_paper.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;출처: Attention is All You Need [&lt;a href=&quot;https://arxiv.org/pdf/1706.03762.pdf&quot;&gt;https://arxiv.org/pdf/1706.03762.pdf&lt;/a&gt;]&lt;/p&gt;

&lt;p&gt;Transformer는 크게 Encoder와 Decoder로 구분된다. 부수적인 다른 구성 요소들이 있으나, Encoder와 Decoder가 가장 핵심이다. Encoder는 위 그림에서 좌측, Decoder는 위 그림에서 우측을 의미한다.&lt;/p&gt;

&lt;p&gt;Encoder와 Decoder를 자세히 분석하기 이전에, 각각을 함수 형태로 이해해보자. Encoder는 sentence를 input으로 받아 하나의 vector를 생성해는 함수이다. 이러한 과정을 Encoding이라고 한다. Encoding으로 생성된 vector는 context라고 부르는데, 말그대로 문장의 ‘문맥’을 함축해 담은 vector이다. Encoder는 이러한 context를 제대로 생성(문장의 정보들을 빠뜨리지 않고 제대로 압축)해내는  것을 목표로 학습된다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-28-Transformer-in-pytorch/encoder_simple.png&quot; alt=&quot;encoder_simple.png&quot; /&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;c=\text{Encoder}(x)\\x\text{ : sentence}\\c\text{ : context}&lt;/script&gt;

&lt;p&gt;Decoder는 Encoder와 방향이 반대이다. context를 input으로 받아 sentence를 output으로 생성해낸다. 이러한 과정을 Decoding이라고 한다. 사실 Decoder는 input으로 context만을 받지는 않고, output으로 생성해내는 sentence를 right shift한 sentence도 함께 입력받지만, 자세한 것은 당장 이해할 필요 없이 단순히 어떤 sentence도 함께 input으로 받는 다는 개념만 잡고 넘어가자. 정리하자면, Decoder는 sentence, context를 input으로 받아 sentence를 만들어내는 함수이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-28-Transformer-in-pytorch/decoder_simple.png&quot; alt=&quot;decoder_simple.png&quot; /&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y=\text{Decoder}(c,z)\\y,\ z\text{ : sentence}\\c\text{ : context}&lt;/script&gt;

&lt;p&gt;Encoder와 Decoder에 모두 context vector가 등장하는데, Encoder는 context를 생성해내고, Decoder는 context를 사용한다. 이러한 흐름으로 Encoder와 Decoder가 연결되어 전체 Transformer를 구성하는 것이다.&lt;/p&gt;

&lt;p&gt;지금까지의 개념을 바탕으로 아주 간단한 Transformer model을 pytorch로 구현해보자. encoder와 decoder가 각각 완성되어 있다고 가정하고, 이를 class 생성자의 인자로 받는다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Transformer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;

	&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;encoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;decoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
		&lt;span class=&quot;nb&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Transformer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
		&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encoder&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;encoder&lt;/span&gt;
		&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;decoder&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;decoder&lt;/span&gt;

	&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;decoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&quot;encoder&quot;&gt;Encoder&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-28-Transformer-in-pytorch/encoder.png&quot; alt=&quot;encoder.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Encoder는 위와 같은 구조로 이루어져 있다. Encoder Layer가 &lt;script type=&quot;math/tex&quot;&gt;N&lt;/script&gt;개 쌓여진 형태이다. 논문에서는 &lt;script type=&quot;math/tex&quot;&gt;N=6&lt;/script&gt;을 사용했다. Encoder Layer는 input과 output의 형태가 동일하다. 어떤 matrix를 input으로 받는다고 했을 때, Encoder Layer가 도출해내는 output은 input과 완전히 동일한 shape를 갖는 matrix가 된다. Encoder Layer &lt;script type=&quot;math/tex&quot;&gt;N&lt;/script&gt;개가 쌓여 Encoder를 이룬다고 했을 때, 첫번째 Encoder Layer의 input은 전체 Encoder의 input으로 들어오는 문장 embdding이 된다. 첫번째 layer가 output을 생성해내면 이를 두번째 layer가 input으로 사용하고, 또 그 output을 세번째 layer가 사용하는 식으로 연결되며, 가장 마지막 &lt;script type=&quot;math/tex&quot;&gt;N&lt;/script&gt;번째 layer의 output이 전체 Encoder의 output, 즉, context가 된다. 이러한 방식으로 layer들이 연결되기 때문에, Encoder Layer의 input과 output의 shape는 필연적으로 반드시 동일해야만 한다. 여기서 주목해야 하는 지점은 위에서 계속 언급했던 context 역시 Encoder의 input sentence embedding과 동일한 shape를 가진다는 것이다. 즉, 어떤 matrix가 Encoder를 거쳐간다 하더라도 최종 matrix의 shape는 처음의 것과 반드시 같다.&lt;/p&gt;

&lt;p&gt;Encoder는 왜 여러 개의 layer를 겹쳐 쌓는 것일까? 각 Encoder Layer의 역할은 무엇일까? 결론부터 말하자면, 각 Encoder Layer는 input으로 들어오는 vector에 대해 더 높은 차원(넓은 관점)에서의 context를 담는다. 높은 차원에서의 context라는 것은 더 추상적인 정보라는 의미이다. Encoder Layer는 내부적으로 어떠한 Mechanism을 사용해 context를 담아내는데, Encoder Layer가 겹겹이 쌓이다 보니 처음에는 원본 문장에 대한 낮은 수준의 context였겠지만 이후 context에 대한 context, context의 context에 대한 context … 와 같은 식으로 점차 높은 차원의 context가 저장되게 된다. Encoder Layer의 내부적인 작동 방식은 곧 살펴볼 것이기에, 여기서는 직관적으로 Encoder Layer의 역할, Encoder 내부의 전체적인 구조만 이해하고 넘어가자.&lt;/p&gt;

&lt;p&gt;지금까지의 개념을 바탕으로 Encoder를 간단하게 code로 작성해보자.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Encoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;

	&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;encoder_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;# n_layer: Encoder Layer의 개수&lt;/span&gt;
		&lt;span class=&quot;nb&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Encoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
		&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
			&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;copy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;deepcopy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encoder_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

	&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;layer&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;forward 함수를 주목해보자. Encoder Layer들을 순서대로 실행하면서, 이전 layer의 output을 이후 layer의 input으로 넣는다. 첫 layer의 input은 Encoder 전체의 input인 x가 된다. 이후 가장 마지막 layer의 output (context)를 return한다.&lt;/p&gt;

&lt;h3 id=&quot;encoder-layer&quot;&gt;Encoder Layer&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-28-Transformer-in-pytorch/encoder_layer.png&quot; alt=&quot;encoder_layer.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Encoder Layer는 크게 Multi-Head Attention Layer, Position-wise Feed-Forward Layer로 구성된다. 각각의 layer에 대한 자세한 설명은 아래에서 살펴보도록 하고, 우선은 Encoder Layer의 큰 구조만을 사용해 간단하게 구현해보자.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;EncoderLayer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;

	&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;multi_head_attention_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;position_wise_feed_forward_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
		&lt;span class=&quot;nb&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;EncoderLayer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
		&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;multi_head_attention_layer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;multi_head_attention_layer&lt;/span&gt;
		&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;position_wise_feed_forward_layer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;position_wise_feed_forward_layer&lt;/span&gt;

	&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;multi_head_attention_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;position_wise_feed_forward_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;what-is-self-attention&quot;&gt;What is Self-Attention?&lt;/h3&gt;

&lt;p&gt;Multi-Head Attention은 Self-Attention을 병렬적으로 여러 개 수행하는 layer이다. 때문에 Multi-Head Attention을 이해하기 위해서는 Self-Attention에 대해 먼저 알아야만 한다. Attention이라는 것은 넓은 범위의 전체 data에서 특정한 부분에 집중한다는 의미이다. 다음의 문장을 통해 Attention의 개념을 이해해보자.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The animal didn’t cross the street, because it was too tired.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;위 문장에서 ‘it’은 무엇을 지칭하는 것일까? 사람이라면 직관적으로 ‘animal’과 연결지을 수 있지만, 컴퓨터는 ‘it’이 ‘animal’을 가리키는지, ‘street’를 가리키는지 알지 못한다. Self-Attention은 이러한 문제를 해결하기 위해 같은 문장 내에서 두 token 사이의 연관성을 찾아내는 방법론이다. Self가 붙는 이유는 문장 내에서 (같은 문장 내의 다른 token에 대한) Attention을 구하기 때문이다.&lt;/p&gt;

&lt;h4 id=&quot;rnn-vs-self-attention&quot;&gt;RNN vs Self-Attention&lt;/h4&gt;

&lt;p&gt;Transformer에서 벗어나, 이전 RNN의 개념을 다시 생각해보자. RNN은 이전 시점까지 나온 token들에 대한 hidden state 내부에 이전 정보들을 저장했다. RNN의 경우 hidden state를 활용해 이번에 등장한 ‘it’이 이전의 ‘The Animal’을 가리킨다는 것을 알아낼 것이다. Self-Attention 역시 동일한 효과를 내는 것을 목적으로 하나, Recurrent Network에 비해 크게 아래와 같은 2가지 장점을 갖는다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Recurrent Network는 &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;시점의 hidden state &lt;script type=&quot;math/tex&quot;&gt;h_i&lt;/script&gt;를 구하기 위해서는 &lt;script type=&quot;math/tex&quot;&gt;h_{i-1}&lt;/script&gt;가 필요했다. 결국, 앞에서부터 순차 계산을 해나가 &lt;script type=&quot;math/tex&quot;&gt;h_0, h_1, ... , h_n&lt;/script&gt;을 구하는 방법밖에 없었기에 병렬 처리가 불가능했다. 하지만 Self-Attention은 모든 token 쌍 사이의 attention을 한 번의 행렬 곱으로 구해내기 때문에 손쉽게 병렬 처리가 가능하다.&lt;/li&gt;
  &lt;li&gt;Recurrent Network는 시간이 진행될수록 오래된 시점의 token에 대한 정보가 점차 희미해져간다. 위 문장의 예시에서 현재 ‘didn’t’의 시점에서 hidden state를 구한다고 했을 때, 바로 직전의 token인 ‘animal’에 대한 정보는 뚜렷하게 남아있다. 하지만 점차 앞으로 나아갈수록, ‘because’나 ‘it’의 시점에서는 ‘didn’t’ 시점보다는 ‘animal’에 대한 정보가 희미하게 남게 된다. 결국, 서로 거리가 먼 token 사이의 관계에 대한 정보가 제대로 반영되지 못하는 것이다. 반면, Self-Attention은 문장에 token이 &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt;개 있다고 가정할 경우, &lt;script type=&quot;math/tex&quot;&gt;n \times n&lt;/script&gt; 번 연산을 수행해 모든 token들 사이의 관계를 직접 구해낸다. 중간의 다른 token들을 거치지 않고 바로 direct한 관계를 구하는 것이기 때문에 Recurrent Network에 비해 더 명확하게 관계를 잡아낼 수 있다.&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;query-key-value&quot;&gt;Query, Key, Value&lt;/h4&gt;

&lt;p&gt;지금까지는 추상적으로 Self-Attention에 대한 개념 및 장단점을 살펴봤다. 이제 구체적으로 어떤 방식으로 행렬 곱셈을 사용해 Self-Attention이 수행되는지 알아보자. 우선은 matrix level이 아닌 token 단위의 vector-level에서 이해해보자.&lt;/p&gt;

&lt;p&gt;Self-Attention에서는 총 3개의 vector가 새로 등장한다. Query, Key, Value이다. 각각의 역할은 다음과 같다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Query: 현재 시점의 token을 의미&lt;/li&gt;
  &lt;li&gt;Key: attention을 구하고자 하는 대상 token을 의미&lt;/li&gt;
  &lt;li&gt;Value: attention을 구하고자 하는 대상 token을 의미 (Key와 동일한 token)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;위의 예시 문장으로 다시 되돌아가보자. ‘it’이 어느 것을 지칭하는지 알아내고자 한다. 그렇다면 ‘it’ token과 문장 내 다른 모든 token들에 대해 attention을 구해야 한다. 이 경우에는 Query는 ‘it’으로 고정이다. Key, Value는 서로 완전히 같은 token을 가리키는데, 문장의 시작부터 끝까지 모든 token들 중 하나가 될 것이다. Key와 Value가 ‘The’를 가리킬 경우 ‘it’과 ‘The’ 사이의 attention을 구하는 것이고, Key와 Value가 마지막 ‘tired’를 가리킬 경우 ‘it’과 ‘tired’ 사이의 attention을 구하는 것이 된다. 즉, Key와 Value는 문장의 처음부터 끝까지 탐색한다고 이해하면 된다. Query는 고정되어 하나의 token을 가리키고, Query와 가장 부합하는 (Attention이 높은) token을 찾기 위해서 Key, Value를 문장의 처음부터 끝까지 탐색시키는 것이다. 각각의 의미는 이해했으나, Key와 Value가 완전히 같은 token을 가리킨다면 왜 두 개가 따로 존재하는지 의문이 들 수 있다. 이는 이후에 다룰 것이나, 결론부터 말하자면 Key와 Value의 실제 값은 다르지만 의미적으로는 여전히 같은 token을 의미한다. Key와 Value는 이후 Attention 계산 과정에서 별개로 사용된다.&lt;/p&gt;

&lt;p&gt;Query, Key, Value가 각각 어떤 token을 가리키는지는 이해가 됐을 것이다. 하지만, 그래서 각 vector의 값은 어떻게 만들어지는지는 우리는 아직 알지 못한다. 정말 간단하게도, input으로 들어오는 token embedding vector를 fully connected layer에 넣어 세 vector를 만들어낸다. 세 vector를 생성해내는 FC layer는 모두 다르기 때문에, 결국 self-attention에서는 Query, Key, Value를 구하기 위해 3개의 서로 다른 FC layer가 존재한다. 이 FC layer들은 모두 같은 input dimension, output dimension을 갖는다. input dimension이 같은 이유는 당연하게도 모두 다 token embedding vector를 input으로 받기 때문이다. output dimension이 같다는 것은 각각 별개의 FC layer로 구해진 Query, Key, Value가 값은 다를지언정 같은 dimension을 갖는 vector가 된다는 것을 알 수 있다. 즉, &lt;strong&gt;Query, Key, Value의 shape는 모두 동일&lt;/strong&gt;하다. 앞으로 이 세 vector의 dimension을 &lt;script type=&quot;math/tex&quot;&gt;d_k&lt;/script&gt;로 명명한다. 여기서 &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt;는 Key를 의미하는데, 굳이 Query, Key, Value 중 Key를 이름으로 채택한 이유는 특별히 있지 않고, 단지 논문의 notation에서 이를 채택했기 때문이다. 정리하자면, Query, Key, Value는 모두 &lt;script type=&quot;math/tex&quot;&gt;d_k&lt;/script&gt;의 dimension을 갖는 vector이다. 이제 위에서 얘기했던 Key, Value가 다른 값을 갖는 이유를 이해할 수 있다. input은 같은 token embedding vector였을지라도 서로 다른 FC layer를 통해서 각각 Key, Value가 구해지기 때문에 같은 token을 가리키면서 다른 값을 갖는 것이다.&lt;/p&gt;

&lt;h4 id=&quot;how-to-calculate&quot;&gt;How to Calculate?&lt;/h4&gt;

&lt;p&gt;이제 Query, Key, Value를 활용해 Attention을 계산해보자. Attention이라고 한다면 어떤 것에 대한 Attention인지 불명확하다. 구체적으로, Query에 대한 Attention이다. 이 점을 꼭 인지하고 넘어가자. 이후부터는 Query, Key, Value를 각각 &lt;script type=&quot;math/tex&quot;&gt;Q&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;K&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt;로 축약해 부른다. Query의 Attention은 다음과 같은 수식으로 계산된다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\text{Query's Attention}\left( Q, K, V \right) = \text{softmax}\left( \frac{QK^T}{\sqrt{d_k}} \right) V&lt;/script&gt;

&lt;p&gt;그림으로 계산의 흐름을 표현하면 다음과 같다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-28-Transformer-in-pytorch/scaled_dot_production_in_paper.png&quot; alt=&quot;scaled_dot_production_in_paper.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;출처: Attention is All You Need [&lt;a href=&quot;https://arxiv.org/pdf/1706.03762.pdf&quot;&gt;https://arxiv.org/pdf/1706.03762.pd&lt;/a&gt;f]&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;Q&lt;/script&gt;는 현재 시점의 token을, &lt;script type=&quot;math/tex&quot;&gt;K&lt;/script&gt;와 &lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt;는 Attention을 구하고자 하는 대상 token을 의미했다. 우선은 빠른 이해를 돕기 위해 &lt;script type=&quot;math/tex&quot;&gt;Q&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;K&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt;가 모두 구해졌다고 가정한다. 위의 예시 문장을 다시 가져와 ‘it’과 ‘animal’ 사이의 Attention을 구한다고 해보자. &lt;script type=&quot;math/tex&quot;&gt;d_k=3&lt;/script&gt;이라고 한다면, 아래와 같은 모양일 것이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-28-Transformer-in-pytorch/qkv_vector.png&quot; alt=&quot;qkv_vector.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그렇다면 &lt;script type=&quot;math/tex&quot;&gt;Q&lt;/script&gt;와 &lt;script type=&quot;math/tex&quot;&gt;K&lt;/script&gt;를 MatMul(행렬곱)한다는 의미는 어떤 의미일까? 이 둘을 곱한다는 것은 둘의 Attention Score를 구한다는 것이다. &lt;script type=&quot;math/tex&quot;&gt;Q&lt;/script&gt;와 &lt;script type=&quot;math/tex&quot;&gt;K&lt;/script&gt;의 shape를 생각해보면, 둘 모두 &lt;script type=&quot;math/tex&quot;&gt;d_k&lt;/script&gt;를 dimension으로 갖는 vector이다. 이 둘을 곱한다고 했을 때(정확히는 &lt;script type=&quot;math/tex&quot;&gt;K&lt;/script&gt;를 transpose한 뒤 곱함, 즉 두 vector의 내적), 결과값은 어떤 scalar 값이 나오게 될 것이다. 이 값을 Attention Score라고 한다. 이후 scaling을 수행하는데, 값의 크기가 너무 커지지 않도록 &lt;script type=&quot;math/tex&quot;&gt;\sqrt{d_k}&lt;/script&gt;로 나눠준다. 값이 너무 클 경우 gradient vanishing이 발생할 수 있기 때문이다. scaling을 제외한 연산 과정을 아래와 같다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-28-Transformer-in-pytorch/attention_score_scalar.png&quot; alt=&quot;attention_score_scalar.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;지금까지는 &lt;script type=&quot;math/tex&quot;&gt;1:1&lt;/script&gt; Attention을 구했다면, 이를 확장시켜  &lt;script type=&quot;math/tex&quot;&gt;1:N&lt;/script&gt; Attention을 구해보자. 그 전에 &lt;script type=&quot;math/tex&quot;&gt;Q&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;K&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt;에 대한 개념을 다시 되짚어보자. &lt;script type=&quot;math/tex&quot;&gt;Q&lt;/script&gt;는 고정된 token을 가리키고, &lt;script type=&quot;math/tex&quot;&gt;Q&lt;/script&gt;가 가리키는 token과 가장 높은 Attention을 갖는 token을 찾기 위해 &lt;script type=&quot;math/tex&quot;&gt;K&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt;를 문장의 첫 token부터 마지막 token까지 탐색시키게 된다. 즉, Attention을 구하는 연산이 &lt;script type=&quot;math/tex&quot;&gt;Q&lt;/script&gt; 1개에 대해서 수행된다고 가정했을 때, &lt;script type=&quot;math/tex&quot;&gt;K&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt;는 문장의 길이 &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt;만큼 반복되게 된다. &lt;script type=&quot;math/tex&quot;&gt;Q&lt;/script&gt; vector 1개에 대해서 Attention을 계산한다고 했을 때, &lt;script type=&quot;math/tex&quot;&gt;K&lt;/script&gt;와 &lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt;는 각각 &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt;개의 vector가 되는 것이다. 이 때 &lt;script type=&quot;math/tex&quot;&gt;Q&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;K&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt; vector의 dimension은 모두 &lt;script type=&quot;math/tex&quot;&gt;d_k&lt;/script&gt;로 동일할 것이다. 위의 예시 문장을 다시 갖고 와 ‘it’에 대한 Attention을 구하고자 할 때에는 &lt;script type=&quot;math/tex&quot;&gt;Q&lt;/script&gt;는 ‘it’, &lt;script type=&quot;math/tex&quot;&gt;K&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt;는 문장 전체이다. &lt;script type=&quot;math/tex&quot;&gt;K&lt;/script&gt;와 &lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt;를 각각 &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt;개의 vector가 아닌 1개의 matrix로 표현한다고 하면 vector들을 concatenate해 &lt;script type=&quot;math/tex&quot;&gt;n \times d_k&lt;/script&gt;의 matrix로 변환하면 된다. 그 결과 아래와 같은 shape가 된다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-28-Transformer-in-pytorch/qkv_matrix_1.png&quot; alt=&quot;qkv_matrix_1.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그렇다면 이들의 Attention Score는 아래와 같이 계산될 것이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-28-Transformer-in-pytorch/attention_score_vector.png&quot; alt=&quot;attention_score_vector.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그 결과 Attention Score는 &lt;script type=&quot;math/tex&quot;&gt;1 \times n&lt;/script&gt;의 matrix가 되는데, 이는 &lt;script type=&quot;math/tex&quot;&gt;Q&lt;/script&gt;의 token과 문장 내 모든 token들 사이의 Attention Score를 각각 계산한 뒤 concatenate한 것과 동일하다. 이를 행렬곱 1회로 수행한 것이다.&lt;/p&gt;

&lt;p&gt;이렇게 구한 Attention Scores를 softmax를 사용해 확률값으로 변환하게 된다. 그 결과 각 Attention Score는 모두 더하면 1인 확률값이 된다. 이 값들의 의미는 &lt;script type=&quot;math/tex&quot;&gt;Q&lt;/script&gt;의 token과 해당 token이 얼마나 Attention을 갖는지(얼마나 연관성이 짙은지)에 대한 비율(확률값)이 된다. 임의로 Attention Probability라고 부른다(논문에서 사용하는 표현은 아니고, 이해를 돕기 위해 임의로 붙인 명칭이다). Attention Probability를 최종적으로 &lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt;와 곱하게 되는데, &lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt;(Attention을 구하고자 하는 대상 token, 다시 한 번 강조하지만 &lt;script type=&quot;math/tex&quot;&gt;K&lt;/script&gt;와 &lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt;는 같은 token을 의미한다.)를 Attention Probability만큼만 반영하겠다는 의미이다. 연산은 다음과 같이 이루어진다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-28-Transformer-in-pytorch/attention_vector.png&quot; alt=&quot;attention_vector.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이렇게 구해진 최종 result는 기존의 &lt;script type=&quot;math/tex&quot;&gt;Q&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;K&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt;와 같은 dimension(&lt;script type=&quot;math/tex&quot;&gt;d_k&lt;/script&gt;)를 갖는 vector 1개임을 주목하자. 즉, input으로 &lt;script type=&quot;math/tex&quot;&gt;Q&lt;/script&gt; vector 1개를 받았는데, 연산의 최종 output이 input과 같은 shape를 갖는 것이다.&lt;/p&gt;

&lt;p&gt;지금까지의 Attention 연산은 ‘it’이라는 한 token에 대한 Attention을 구한 것이다. 그러나 우리는 문장 내에서 ‘it’에 대한 Attention만 구하고자 하는 것이 아니다. 모든 token에 대한 Attention을 구해내야만 한다. 따라서 Query 역시 1개의 vector가 아닌 모든 token에 대한 matrix로 확장시켜야 한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-28-Transformer-in-pytorch/qkv_matrix_2.png&quot; alt=&quot;qkv_matrix_2.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그렇다면 Attention을 구하는 연산은 아래와 같이 진행된다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-28-Transformer-in-pytorch/attention_score_matrix.png&quot; alt=&quot;attention_score_matrix.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-28-Transformer-in-pytorch/attention_matrix.png&quot; alt=&quot;attention_matrix.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이제 여기까지 왔으면 &lt;script type=&quot;math/tex&quot;&gt;Q&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;K&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt;가 주어졌을 때에 어떻게 Attention이 계산되는지 이해했을 것이다. 주목해야 할 점은, Attention 계산에서 input(&lt;script type=&quot;math/tex&quot;&gt;Q&lt;/script&gt;)와 output(&lt;script type=&quot;math/tex&quot;&gt;Q&lt;/script&gt;’s Attention)이 같은 shape라는 것이다. 즉, Self-Attention 계산을 하나의 함수로 본다면, Self-Attention 함수는 input의 shape를 보존한다(Attention을 함수라고 했을 때 syntax 측면에서 엄밀히 따지자면 input은 &lt;script type=&quot;math/tex&quot;&gt;Q&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;K&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt; 총 3개이다. 하지만 개념 상으로는 &lt;script type=&quot;math/tex&quot;&gt;Q&lt;/script&gt;에 대한 Attention을 의미하는 것이므로 semantic 측면에서 input은 &lt;script type=&quot;math/tex&quot;&gt;Q&lt;/script&gt;라고 볼 수 있다).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-28-Transformer-in-pytorch/self_attention.png&quot; alt=&quot;self_attention.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그렇다면 &lt;script type=&quot;math/tex&quot;&gt;Q&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;K&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt;는 어떻게 구해지는 것일까? Self-Attention 개념 이전에 설명했듯이, 각각 서로 다른 FC layer에 의해 구해진다. FC layer의 input은 word embedding vector들이고, output은 각각 &lt;script type=&quot;math/tex&quot;&gt;Q&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;K&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt;이다. word embedding의 dimension이 &lt;script type=&quot;math/tex&quot;&gt;d_{e}&lt;/script&gt;라고 한다면, input의 shape는 &lt;script type=&quot;math/tex&quot;&gt;n \times d_e&lt;/script&gt;이고, output의 shape는 &lt;script type=&quot;math/tex&quot;&gt;n \times d_k&lt;/script&gt;이다. 각각의 FC layer는 서로 다른 weight matrix (&lt;script type=&quot;math/tex&quot;&gt;d_{embed} \times d_k&lt;/script&gt;)를 갖고 있기 때문에 output의 shape는 모두 동일할지라도, &lt;script type=&quot;math/tex&quot;&gt;Q&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;K&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt;의 실제 값들은 모두 다르다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-28-Transformer-in-pytorch/qkv_fc_layer.png&quot; alt=&quot;qkv_fc_layer.png&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;pad-masking&quot;&gt;Pad Masking&lt;/h4&gt;

&lt;p&gt;뜬금없이 masking이 왜 나오는 것일까? 사실 논문의 figure에 따르면 Attention 계산에는 masking 과정이 포함되어 있다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-28-Transformer-in-pytorch/scaled_dot_production_in_paper.png&quot; alt=&quot;scaled_dot_production_in_paper.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;출처: Attention is All You Need [&lt;a href=&quot;https://arxiv.org/pdf/1706.03762.pdf&quot;&gt;https://arxiv.org/pdf/1706.03762.pd&lt;/a&gt;f]&lt;/p&gt;

&lt;p&gt;pad는 무엇을 의미하는 것일까? 예시 문장을 다시 가져와보자.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The animal didn’t cross the street, because it was too tired.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;문장을 word 단위로 tokenize(단순히 python의 split() 사용)한다면 token의 개수는 총 11개이다. 만약의 각 token의 embedding dimension이 &lt;script type=&quot;math/tex&quot;&gt;d_{embed}&lt;/script&gt;라고 한다면, 문장 전체의 embedding matrix는 (&lt;script type=&quot;math/tex&quot;&gt;11 \times d_{embed}&lt;/script&gt;)일 것이다. 그런데 문장의 길이가 더 길거나 짧다면 그 때마다 input의 shape는 바뀌게 된다. 실제 model 학습 과정에서는 한 문장 씩이 아닌 mini-batch씩 여러 문장와야 하는데 각 문장 마다의 length가 다를 경우 batch를 만들어낼 수 없다. 이러한 문제를 해결하기 위해 &lt;script type=&quot;math/tex&quot;&gt;\text{seq_len}&lt;/script&gt;(해당 mini-batch 내 token 개수의 최대 값)을 지정하게 되는데, 만약 &lt;script type=&quot;math/tex&quot;&gt;\text{seq_len}&lt;/script&gt;이 20이라고 한다면 위 문장에서는 9개의 빈 token이 있게 된다. 이러한 빈 token을 pad token이라고 한다. 그런데, 이러한 pad token에는 attention이 부여되어서는 안된다. 실제로는 존재하지도 않는 token과 다른 token 사이의 attention을 찾아서 계산하고, 이를 반영하는 것은 직관적으로도 말이 안된다는 것을 알 수 있다. 따라서 이러한 pad token들에 대해 attention이 부여되지 않도록 처리하는 것이 pad masking이다. masking은 &lt;script type=&quot;math/tex&quot;&gt;(\text{seq_len} \times \text{seq_len})&lt;/script&gt; shape의 matrix를 곱하는 방식으로 이뤄지는데 masking matrix에서 pad token에 해당하는 row, column의 모든 값은 &lt;script type=&quot;math/tex&quot;&gt;-\inf&lt;/script&gt;이다. 그 외에는 모두 1이다. 이러한 연산은 scaling과 softmax 사이에 수행하게 되는데, 사실은 scaling 이전, 이후 언제 적용하든 차이는 없다. scaling은 단순히 모든 값을 &lt;script type=&quot;math/tex&quot;&gt;d_k&lt;/script&gt;로 일괄 나누는 작업이기 때문이다. 대신 반드시 &lt;script type=&quot;math/tex&quot;&gt;Q&lt;/script&gt;와 &lt;script type=&quot;math/tex&quot;&gt;K&lt;/script&gt;의 행렬곱 이후, softmax 이전에 적용되어야 한다. masking matrix와 같은 shape는 &lt;script type=&quot;math/tex&quot;&gt;Q&lt;/script&gt;와 &lt;script type=&quot;math/tex&quot;&gt;K&lt;/script&gt;의 행렬곱 연산 이후에나 등장하기 때문이다. 또한 softmax는 등장하는 모든 값들을 반영해 확률값을 계산하게 되는데, 이 때 pad token의 값이 반영되어서는 안되므로 softmax 이전에는 반드시 masking이 수행되어야 한다.&lt;/p&gt;

&lt;h4 id=&quot;self-attention-code-in-pytorch&quot;&gt;Self-Attention Code in Pytorch&lt;/h4&gt;

&lt;p&gt;Self-Attention을 pytorch code로 구현해보자. Self-Attention은 Transformer에서의 가장 핵심적인 code이므로 반드시 이해하고 넘어가자. 여기서 주의해야 할 점은 실제 model에 들어오는 input은 한 개의 문장이 아니라 mini-batch이기 때문에 &lt;script type=&quot;math/tex&quot;&gt;Q&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;K&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt;의 shape가 &lt;script type=&quot;math/tex&quot;&gt;\text{n_batch} \times \text{seq_len} \times d_k&lt;/script&gt;라는 것이다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;calculate_attention&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;query&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;c&quot;&gt;# query, key, value's shape: (n_batch, seq_len, d_k)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;d_k&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# get d_k&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;attention_score&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matmul&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;query&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transpose&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Q x K^T, attention_score's shape: (n_batch, seq_len, seq_len)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;attention_score&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;attention_score&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;math&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d_k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# scaling&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mask&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;attention_score&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;masked_fill&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mask&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1e9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# masking&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;attention_prob&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;softmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# softmax, attention_prob's shape: (n_batch, seq_len, seq_len)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matmul&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;attention_prob&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Attention_Prob x V, out's shape: (n_batch, seq_len, d_k)&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;함수의 인자로 query, key, value, mask를 받는다. mask는 pad masking matrix일 것이다. query, key, value는 서로 다른 FC Layer를 거쳐 &lt;script type=&quot;math/tex&quot;&gt;\text{n_batch} \times \text{max_seq_len} \times d_k&lt;/script&gt;로 변형되었다.&lt;/p&gt;

&lt;h3 id=&quot;multi-head-attention-layer&quot;&gt;Multi-Head Attention Layer&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-28-Transformer-in-pytorch/multi_head_attention_in_paper.png&quot; alt=&quot;multi_head_attention_in_paper.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;출처: Attention is All You Need [&lt;a href=&quot;https://arxiv.org/pdf/1706.03762.pdf&quot;&gt;https://arxiv.org/pdf/1706.03762.pdf&lt;/a&gt;]&lt;/p&gt;

&lt;p&gt;지금까지의 Self-Attention에 대한 개념은 모두 Multi-Head Attention Layer를 이해하기 위한 것이었다. Attention 계산을 논문에서는 Scaled Dot-Product Attention이라고 명명한다. Transformer는 Scaled Dot Attention을 한 Encoder Layer마다 1회씩 수행하는 것이 아니라 &lt;script type=&quot;math/tex&quot;&gt;h&lt;/script&gt;회 수행한 뒤, 그 결과를 종합해 사용한다. 이 것이 Multi-Head Attention Layer이다. 이러한 작업을 수행하는 이유는 여러 Attention을 잘 반영하기 위해서이다. 만약 하나의 Attention만 반영한다고 했을 때, 예시 문장에서 ‘it’의 Attention에는 ‘animal’의 것이 대부분을 차지하게 될 것이다. 하지만 여러 종류의 attention을 반영한다고 했을 때 ‘tired’에 집중한 Attention까지 반영된다면, 최종적인 ‘it’의 Attention에는 ‘animal’을 지칭한다는 정보, ‘tired’ 상태라는 정보까지 모두 담기게 될 것이다. 이 것이 Multi-Head Attention을 사용하는 이유이다.&lt;/p&gt;

&lt;p&gt;구체적인 연산 방법을 살펴보자. 논문에서는 &lt;script type=&quot;math/tex&quot;&gt;h=8&lt;/script&gt;을 채택했다. Scaled Dot-Product Attention에서는 &lt;script type=&quot;math/tex&quot;&gt;Q&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;K&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt;를 위해 FC layer가 총 3개 필요했었는데, 이를 &lt;script type=&quot;math/tex&quot;&gt;h&lt;/script&gt;회 수행한다고 했으므로 &lt;script type=&quot;math/tex&quot;&gt;3*h&lt;/script&gt;개의 FC layer가 필요하게 된다. 각각 연산의 최종 output은 &lt;script type=&quot;math/tex&quot;&gt;n \times d_k&lt;/script&gt;의 shape(실제로는 &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt;은 &lt;script type=&quot;math/tex&quot;&gt;\text{max_seq_len}&lt;/script&gt;이지만 notation의 통일성을 위해 &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt;을 그대로 사용한다)인데, 총 &lt;script type=&quot;math/tex&quot;&gt;h&lt;/script&gt;개의 &lt;script type=&quot;math/tex&quot;&gt;n \times d_k&lt;/script&gt; matrix를 모두 concatenate해서 &lt;script type=&quot;math/tex&quot;&gt;n \times (d_k*h)&lt;/script&gt;의 shape를 갖는 matrix를 만들어낸다. 이 때 &lt;script type=&quot;math/tex&quot;&gt;d_k*h&lt;/script&gt;를 &lt;script type=&quot;math/tex&quot;&gt;d_{model}&lt;/script&gt;로 명명한다. &lt;script type=&quot;math/tex&quot;&gt;d_{model}=d_k*h&lt;/script&gt; 수식은 실제 코드 구현에서 매우 중요한 개념이므로 꼭 기억하고 넘어가자.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-28-Transformer-in-pytorch/multi_head_attention_concat.png&quot; alt=&quot;multi_head_attention_concat.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;사실 위의 설명은 개념 상의 이해를 돕기 위한 것이고, 실제 연산은 병렬 처리를 위해 더 효율적인 방식으로 수행된다. 기존의 설명에서 &lt;script type=&quot;math/tex&quot;&gt;Q&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;K&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt;를 구하기 위한 FC layer는 &lt;script type=&quot;math/tex&quot;&gt;d_{embed}&lt;/script&gt;를 &lt;script type=&quot;math/tex&quot;&gt;d_k&lt;/script&gt;로 변환했다. 이렇게 구해낸 &lt;script type=&quot;math/tex&quot;&gt;Q&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;K&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt;로 각각의 Self-Attention을 계산해 concatenate하는 방식은 별개의 Self-Attention 연산을 총 &lt;script type=&quot;math/tex&quot;&gt;h&lt;/script&gt;회 수행해야 한다는 점에서 매우 비효율적이다. 따라서 실제로는 &lt;script type=&quot;math/tex&quot;&gt;Q&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;K&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt; 자체를 &lt;script type=&quot;math/tex&quot;&gt;n \times d_k&lt;/script&gt;가 아닌, &lt;script type=&quot;math/tex&quot;&gt;n \times d_{model}&lt;/script&gt;로 생성해내서 한 번의 Self-Attention 계산으로 &lt;script type=&quot;math/tex&quot;&gt;n \times d_{model}&lt;/script&gt;의 output을 만들어내게 된다. 때문에 &lt;script type=&quot;math/tex&quot;&gt;Q&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;K&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt;를 생성해내기 위한 &lt;script type=&quot;math/tex&quot;&gt;d_{embed} \times d_k&lt;/script&gt;의 weight matrix를 갖는 FC layer를 &lt;script type=&quot;math/tex&quot;&gt;3*h&lt;/script&gt;개 운용할 필요 없이 &lt;script type=&quot;math/tex&quot;&gt;d_{embed} \times d_{model}&lt;/script&gt;의 weight matrix를 갖는 FC layer를 &lt;script type=&quot;math/tex&quot;&gt;3&lt;/script&gt;개만 운용하면 된다.&lt;/p&gt;

&lt;p&gt;여기서 우리가 알 수 있는 것은 여러 Attention을 반영한다는 Multi-Head Attention Layer의 개념적인 의미는 사실 단지 &lt;script type=&quot;math/tex&quot;&gt;d_k&lt;/script&gt;의 크기를 &lt;script type=&quot;math/tex&quot;&gt;d_{model}&lt;/script&gt;로 확장시키는 단순한 구현으로 끝나는 것이다. &lt;script type=&quot;math/tex&quot;&gt;Q&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;K&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt; vector에는 담을 수 있는 정보의 양이 &lt;script type=&quot;math/tex&quot;&gt;d_k&lt;/script&gt;의 dimension으로는 작기 때문에 더 많은 정보(Attention)을 담아내기 위해 &lt;script type=&quot;math/tex&quot;&gt;Q&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;K&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt; vector의 dimension을 늘린 것으로 이해하면 된다.&lt;/p&gt;

&lt;p&gt;다시 본론으로 되돌아와서 최종적으로 생성해된 matrix를 FC layer에 넣어 multi-head attention의 input과 같은 shape(&lt;script type=&quot;math/tex&quot;&gt;n \times d_{embed}&lt;/script&gt;)의 matrix로 변환하는 과정이 필요하다. 따라서 마지막 FC layer의 input dimension은 &lt;script type=&quot;math/tex&quot;&gt;d_{model}&lt;/script&gt;, output dimension은 &lt;script type=&quot;math/tex&quot;&gt;d_{embed}&lt;/script&gt;가 된다. 이는 multi-head attention layer도 하나의 함수라고 생각했을 때, input의 shape와 output의 shape가 동일하게 하기 위함이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-28-Transformer-in-pytorch/multi_head_attention_fc_layer.png&quot; alt=&quot;multi_head_attention_fc_layer.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-28-Transformer-in-pytorch/multi_head_attention.png&quot; alt=&quot;multi_head_attention.png&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;multi-head-attention-code-in-pytorch&quot;&gt;Multi-Head Attention Code in Pytorch&lt;/h4&gt;

&lt;p&gt;Multi-Head Attention Layer를 실제 code로 구현해보자. 위에서 구현했던 calculate_attention 함수를 사용한다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;MultiHeadAttentionLayer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;qkv_fc_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fc_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
		&lt;span class=&quot;c&quot;&gt;# qkv_fc_layer's shape: (d_embed, d_model)&lt;/span&gt;
		&lt;span class=&quot;c&quot;&gt;# fc_layer's shape: (d_model, d_embed)&lt;/span&gt;
		&lt;span class=&quot;nb&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MultiHeadAttentionLayer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
		&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d_model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d_model&lt;/span&gt;
		&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;
		&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;query_fc_layer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;copy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;deepcopy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;qkv_fc_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;key_fc_layer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;copy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;deepcopy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;qkv_fc_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value_fc_layer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;copy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;deepcopy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;qkv_fc_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fc_layer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fc_layer&lt;/span&gt;

		&lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;우선 생성자를 살펴보자. qkv_fc_layer 인자로 &lt;script type=&quot;math/tex&quot;&gt;d_{embed} \times d_{model}&lt;/script&gt;의 weight matrix를 갖는 FC Layer를 받아 멤버 변수로 &lt;script type=&quot;math/tex&quot;&gt;Q&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;K&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt;에 대해 각각 copy.deepcopy를 호출해 저장한다. deepcopy를 호출하는 이유는 실제로는 서로 다른 weight를 갖고 별개로 운용되게 하기 위함이다. copy 없이 하나의 FC Layer로 &lt;script type=&quot;math/tex&quot;&gt;Q&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;K&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt;를 모두 구하게 되면 &lt;script type=&quot;math/tex&quot;&gt;Q&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;K&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt;가 모두 같은 값일 것이다. fc_layer는 attention 계산 이후 거쳐가는 FC Layer로, &lt;script type=&quot;math/tex&quot;&gt;d_{model} \times d_{embed}&lt;/script&gt;의 weight matrix를 갖는다.&lt;/p&gt;

&lt;p&gt;가장 중요한 forward 함수이다. Transformer 구현에서 가장 핵심적인 부분이므로 반드시 이해하고 넘어가자.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; &lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;MultiHeadAttentionLayer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;

		&lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;

	&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;query&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mask&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
		&lt;span class=&quot;c&quot;&gt;# query, key, value's shape: (n_batch, seq_len, d_embed)&lt;/span&gt;
		&lt;span class=&quot;c&quot;&gt;# mask's shape: (n_batch, seq_len, seq_len)&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;n_batch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;query&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# get n_batch&lt;/span&gt;

		&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fc_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# reshape (n_batch, seq_len, d_embed) to (n_batch, h, seq_len, d_k)&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fc_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# out's shape: (n_batch, seq_len, d_model)&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;view&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d_model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;//&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# out's shape: (n_batch, seq_len, h, d_k)&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transpose&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# out's shape: (n_batch, h, seq_len, d_k)&lt;/span&gt;
			&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;

		&lt;span class=&quot;n&quot;&gt;query&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;query&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;query_fc_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# query, key, value's shape: (n_batch, h, seq_len ,d_k)&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;key&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;key_fc_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value_fc_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

		&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mask&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;mask&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mask&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unsqueeze&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# mask's shape: (n_batch, 1, seq_len, seq_len)&lt;/span&gt;

		&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;calculate_attention&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;query&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# out's shape: (n_batch, h, seq_len, d_k)&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transpose&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# out's shape: (n_batch, seq_len, h, d_k)&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;contiguous&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;view&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# out's shape: (n_batch, seq_len, d_model)&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fc_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# out's shape: (n_batch, seq_len, d_embed)&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;인자로 받은 query, key, value는 실제 &lt;script type=&quot;math/tex&quot;&gt;Q&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;K&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt; matrix가 아니다. &lt;script type=&quot;math/tex&quot;&gt;Q&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;K&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt; 계산을 위해서는 각각 FC Layer 에 input으로 sentence(실제로는 mini-batch이므로 다수의 sentence)를 넣어줘야 하는데, 이 sentence를 의미하는 것이다. Self-Attention이기에 당연히 &lt;script type=&quot;math/tex&quot;&gt;Q&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;K&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt;는 같은 sentence embedding에서 나오게 되는데 왜 별개의 인자로 받는지 의문일 수 있다. 이는 Decoder의 작동 원리를 알고 나면 이해할 것이다. 인자로 받은 query, key, value는 sentence이므로 shape는 (&lt;script type=&quot;math/tex&quot;&gt;\text{n_batch} \times \text{seq_len} \times d_{embed}&lt;/script&gt;)이다. masking은 기본적으로 한 문장에 대해 (&lt;script type=&quot;math/tex&quot;&gt;\text{seq_len} \times \text{seq_len}&lt;/script&gt;)의 shape를 갖는데, mini-batch이므로 (&lt;script type=&quot;math/tex&quot;&gt;\text{n_batch} \times \text{seq_len} \times \text{seq_len}&lt;/script&gt;)의 shape를 갖는다.&lt;/p&gt;

&lt;p&gt;transform은 &lt;script type=&quot;math/tex&quot;&gt;Q&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;K&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt;를 구해주는 함수이다. 따라서 input shape는 (&lt;script type=&quot;math/tex&quot;&gt;\text{n_batch} \times \text{seq_len} \times d_{embed}&lt;/script&gt;)이고, output shape는 (&lt;script type=&quot;math/tex&quot;&gt;\text{n_batch} \times \text{seq_len} \times d_{model}&lt;/script&gt;)이어야 한다. 하지만 실제로는 단순히 FC Layer만 거쳐가는 것이 아닌 추가적인 변형이 일어난다. 우선 &lt;script type=&quot;math/tex&quot;&gt;d_{model}&lt;/script&gt;을 &lt;script type=&quot;math/tex&quot;&gt;h&lt;/script&gt;와 &lt;script type=&quot;math/tex&quot;&gt;d_k&lt;/script&gt;로 분리하고, 각각을 하나의 dimension으로 분리한다. 따라서 shape는 (&lt;script type=&quot;math/tex&quot;&gt;\text{n_batch} \times \text{seq_len} \times h \times d_k&lt;/script&gt;)가 된다. 이후 이를 transpose해 (&lt;script type=&quot;math/tex&quot;&gt;\text{n_batch} \times h \times \text{seq_len} \times d_k&lt;/script&gt;)로 변환한다. 이러한 작업을 수행하는 이유는 위에서 작성했던 calculate_attention 함수가 input으로 받고자 하는 shape가 (&lt;script type=&quot;math/tex&quot;&gt;\text{n_batch} \times ... \times \text{seq_len} \times d_k&lt;/script&gt;)이기 때문이다. 아래에서 calculate_attention의 code를 다시 살펴보자.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;calculate_attention&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;query&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;c&quot;&gt;# query, key, value's shape: (n_batch, seq_len, d_k)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;d_k&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# get d_k&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;attention_score&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matmul&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;query&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transpose&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Q x K^T, attention_score's shape: (n_batch, seq_len, seq_len)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;attention_score&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;attention_score&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;math&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d_k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# scaling&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mask&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;attention_score&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;masked_fill&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mask&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1e9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# masking&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;attention_prob&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;softmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# softmax, attention_prob's shape: (n_batch, seq_len, seq_len)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matmul&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;attention_prob&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Attention_Prob x V, out's shape: (n_batch, seq_len, d_k)&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;우선 &lt;script type=&quot;math/tex&quot;&gt;d_k&lt;/script&gt;를 중심으로 &lt;script type=&quot;math/tex&quot;&gt;Q&lt;/script&gt;와 &lt;script type=&quot;math/tex&quot;&gt;K&lt;/script&gt; 사이 행렬곱 연산을 수행하기 때문에 &lt;script type=&quot;math/tex&quot;&gt;Q&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;K&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt;의 마지막 dimension은 반드시 &lt;script type=&quot;math/tex&quot;&gt;d_k&lt;/script&gt;여야만 한다. 또한 attention_score의 shape는 마지막 두 dimension이 반드시 (&lt;script type=&quot;math/tex&quot;&gt;\text{seq_len} \times \text{seq_len}&lt;/script&gt;)이어야만 masking이 적용될 수 있기 때문에 &lt;script type=&quot;math/tex&quot;&gt;Q&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;K&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt;의 마지막 직전 dimension은 반드시 &lt;script type=&quot;math/tex&quot;&gt;\text{seq_len}&lt;/script&gt;이어야만 한다.&lt;/p&gt;

&lt;p&gt;다시 forward 함수로 되돌아와서, transform 함수를 사용해 query, key, value를 구하고 나면 mask 역시 변형을 가해야 한다. (&lt;script type=&quot;math/tex&quot;&gt;\text{n_batch} \times \text{seq_len} \times \text{seq_len}&lt;/script&gt;) 형태를 (&lt;script type=&quot;math/tex&quot;&gt;\text{n_batch} \times 1 \times \text{seq_len} \times \text{seq_len}&lt;/script&gt;)로 변경하게 된다. 이는 calculate_attention 함수 내에서 masking을 수행할 때 broadcasting이 제대로 수행되게 하기 위함이다.&lt;/p&gt;

&lt;p&gt;calculate_attention 함수를 사용해 attention을 계산하고 나면 그 shape는 (&lt;script type=&quot;math/tex&quot;&gt;\text{n_batch} \times h \times \text{seq_len} \times d_k&lt;/script&gt;)이다. Multi-Head Attention Layer의 최종 output은 input의 것과 같은 (&lt;script type=&quot;math/tex&quot;&gt;\text{n_batch} \times \text{seq_len} \times d_{embed}&lt;/script&gt;)여야만 하기 때문에 shape를 맞춰줘야 한다. 이를 위해 &lt;script type=&quot;math/tex&quot;&gt;h&lt;/script&gt;와 &lt;script type=&quot;math/tex&quot;&gt;\text{seq_len}&lt;/script&gt;의 순서를 뒤바꾸고 다시 &lt;script type=&quot;math/tex&quot;&gt;h&lt;/script&gt;와 &lt;script type=&quot;math/tex&quot;&gt;d_k&lt;/script&gt;를 &lt;script type=&quot;math/tex&quot;&gt;d_{model}&lt;/script&gt;로 결합한다. 이후 FC Layer를 거쳐 &lt;script type=&quot;math/tex&quot;&gt;d_{model}&lt;/script&gt;을 &lt;script type=&quot;math/tex&quot;&gt;d_{embed}&lt;/script&gt;로 변환하게 된다.&lt;/p&gt;

&lt;p&gt;Encoder Layer로 다시 되돌아가보자. Encoder Layer에서는 Multi-Head Attention Layer의 forward 함수의 input이 1개일 것으로 가정하고 code를 작성했는데, 실제로는 query, key, value를 받아야 하므로 이를 수정해준다. 이에 더해 mask 역시 인자로 받게 될 것이다. mask는 Transformer model 외부에서(mini-batch 생성할 때) 생성되게 될 것이다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;EncoderLayer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;

	&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;multi_head_attention_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;position_wise_feed_forward_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
		&lt;span class=&quot;nb&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;EncoderLayer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
		&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;multi_head_attention_layer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;multi_head_attention_layer&lt;/span&gt;
		&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;position_wise_feed_forward_layer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;position_wise_feed_forward_layer&lt;/span&gt;

	&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;multi_head_attention_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;position_wise_feed_forward_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;mask 인자를 받기 위해 Encoder 역시 수정이 가해진다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Encoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;

	&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;encoder_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;# n_layer: Encoder Layer의 개수&lt;/span&gt;
		&lt;span class=&quot;nb&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Encoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
		&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
			&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;copy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;deepcopy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encoder_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

	&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;layer&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Transformer 역시 수정해야 한다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Transformer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;

	&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;encoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;decoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
		&lt;span class=&quot;nb&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Transformer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
		&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encoder&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;encoder&lt;/span&gt;
		&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;decoder&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;decoder&lt;/span&gt;

	&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;src&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;trg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;encoder_output&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;src&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;decoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;trg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;encoder_output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;position-wise-feed-forward-layer&quot;&gt;Position-wise Feed Forward Layer&lt;/h3&gt;

&lt;p&gt;단순하게 2개의 FC Layer를 갖는 Layer이다. 각 FC Layer는 (&lt;script type=&quot;math/tex&quot;&gt;d_{embed} \times d_{ff}&lt;/script&gt;), (&lt;script type=&quot;math/tex&quot;&gt;d_{ff} \times d_{embed}&lt;/script&gt;)의 weight matrix를 갖는다. 즉, Feed Forward Layer 역시 input의 shape를 그대로 유지한다. 다음 Encoder Layer에 shape를 유지한 채 넘겨줘야 하기 때문이다. 정리하자면, Feed Forward Layer는 Multi-Head Attention Layer의 output을 input으로 받아 연산을 수행하고, 다음 Encoder에게 output을 넘겨준다. 논문에서의 수식을 참고하면 첫번째 FC Layer의 output에 ReLU를 적용하게 된다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\text{FFN}(x)=\text{max}(0, xW_1+b_1)W_2 + b_2&lt;/script&gt;

&lt;p&gt;code로 구현하면 다음과 같다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;PositionWiseFeedForwardLayer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;first_fc_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;second_fc_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
		&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;first_fc_layer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;first_fc_layer&lt;/span&gt;
		&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;second_fc_layer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;second_fc_layer&lt;/span&gt;
	
	&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;first_fc_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;relu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dropout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;second_fc_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;생성자의 인자로 받는 두 FC Layer는 (&lt;script type=&quot;math/tex&quot;&gt;d_{embed} \times d_{ff}&lt;/script&gt;), (&lt;script type=&quot;math/tex&quot;&gt;d_{ff} \times d_{embed}&lt;/script&gt;)의 shape를 가져야만 한다.&lt;/p&gt;

&lt;h3 id=&quot;norm-layerresidual-connection&quot;&gt;Norm Layer(Residual Connection)&lt;/h3&gt;

&lt;p&gt;Encoder Layer의 구조를 다시 가져와 살펴보자.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-28-Transformer-in-pytorch/encoder_layer.png&quot; alt=&quot;encoder_layer.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Encoder Layer는 위에서 다뤘던 Multi-Head Attention Layer와 Feed-Forwad Layer로 구성된다. 그러나 사실은 Encoder Layer를 구성하는 두 layer는 Residual Connection으로 둘러싸여 있다. Residual Connection이라는 것은 정말 단순하다. &lt;script type=&quot;math/tex&quot;&gt;y = f(x)&lt;/script&gt;를 &lt;script type=&quot;math/tex&quot;&gt;y=f(x)+x&lt;/script&gt;로 변경하는 것이다. 즉, output을 그대로 사용하지 않고, 원래 input을 더해 사용하게 된다. 이로 인해 얻을 수 있는 이점은 명확하다. Back Propagation 도중 발생할 수 있는 Gradient Vanishing을 방지할 수 있다. 개념적으로는 이 것이 전부이다. 여기에 더해 논문에서 채택한 Layer Normalization까지 추가한다. 간단하게 코드로 구현해보자.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;ResidualConnectionLayer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;norm_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
		&lt;span class=&quot;nb&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ResidualConnectionLayer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
		&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;norm_layer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;norm_layer&lt;/span&gt;

	&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sub_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sub_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;norm_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;forward 함수에서 sub_layer까지 인자로 받는다.&lt;/p&gt;

&lt;p&gt;따라서 EncoderLayer의 코드가 아래와 같이 변경되게 된다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;EncoderLayer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;

	&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;multi_head_attention_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;position_wise_feed_forward_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;norm_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
		&lt;span class=&quot;nb&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;EncoderLayer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
		&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;multi_head_attention_layer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;multi_head_attention_layer&lt;/span&gt;
		&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;position_wise_feed_forward_layer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;position_wise_feed_forward_layer&lt;/span&gt;
		&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;residual_connection_layers&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ResidualConnectionLayer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;copy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;deepcopy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;norm_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;

	&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;residual_connection_layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;](&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;multi_head_attention_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;residual_connection_layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;](&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;position_wise_feed_forward_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;ResidualConnectionLayer의 forward 함수에 sub_layer를 전달할 때에는 lambda 식의 형태로 전달한다.&lt;/p&gt;

&lt;h2 id=&quot;decoder&quot;&gt;Decoder&lt;/h2&gt;

&lt;p&gt;Transformer의 Decoder는 Encoder를 완벽히 이해했다면 큰 무리없이 이해할 수 있다. Encoder의 Layer를 그대로 가져와 사용하고, 몇몇 변경만 가해주는 정도이기 때문이다. 우선 Decoder를 이해하기 위해서는 Decoder의 input과 output이 무엇인지부터 명확히 해 무엇을 하는 module인지 파악하는 것이 최우선이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-28-Transformer-in-pytorch/decoder_simple.png&quot; alt=&quot;decoder_simple.png&quot; /&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y=\text{Decoder}(c,z)\\y,\ z\text{ : sentence}\\c\text{ : context}&lt;/script&gt;

&lt;p&gt;가장 처음에 Transformer의 전체 구조를 이야기할 때 봤던 Decoder의 구조이다. Context와 Some Sentence를 input으로 받아 Output Sentence를 출력한다. Context는 Encoder의 output이라는 것은 이해했다. Transformer model의 목적을 다시 상기시켜 보자. input sentence를 받아와 output sentence를 만들어내는 model이다. 대표적으로 번역과 같은 task를 처리할 수 있을 것이다. 번역이라고 가정한다면, Encoder는 Context를 생성해내는 것, 즉 input sentence의 정보를 압축해 담아내는 것을 목적으로 하고, Decoder는 Context를 활용해 output sentence를 만들어내는 것을 목적으로 한다. 그렇다면 Decoder는 input으로 Context만 받아야 하지, 왜 다른 추가적인 sentence를 받을까? 또 이 sentence는 도대체 무엇일까? 이에 대해 알아보자.&lt;/p&gt;

&lt;h3 id=&quot;decoders-input&quot;&gt;Decoder’s Input&lt;/h3&gt;

&lt;h4 id=&quot;context&quot;&gt;Context&lt;/h4&gt;

&lt;p&gt;위에서 언급했듯이, Decoder의 input으로는 Context와 sentence가 있다. Context는 Encoder에서 생성된 것이다. Encoder 내부에서 Multi-Head Attention Layer나 Position-wise Feed-Forward Layer 모두 input의 shape를 보존했음을 주목하자. 때문에 Encoder Layer 자체도 input의 shape를 보존할 것이고, Encoder Layer가 쌓인 Encoder 전체도 input의 shape를 보존한다. 따라서 Encoder의 output인 Context는 Encoder의 input인 sentence embedding과 동일한 shape를 갖는다. 이 점만 기억하고 넘어가면, 이후 Decoder에서 Context를 사용할 때 이해가 훨씬 수월하다. 이제 Decoder input 중 Context가 아닌 sentence에 대해서 알아보자.&lt;/p&gt;

&lt;h4 id=&quot;teacher-forcing&quot;&gt;Teacher Forcing&lt;/h4&gt;

&lt;p&gt;Decoder의 input에 추가적으로 들어오는 sentence를 이해하기 위해서는 Teacher Forcing라는 개념에 대해 알고 있어야 한다. RNN 계열이든, Transformer 계얼이든 번역 model이 있다고 생각해보자. 결국에는 새로운 sentence를 생성해내야만 한다. 힘들게 만들어낸 model이 초창기 학습을 진행하는 상황이다. random하게 초기화된 parameter들의 값 때문에 엉터리 결과가 나올 것이다. RNN으로 생각을 해봤을 때, 첫번째 token을 생성해내고 이를 다음 token을 생성할 때의 input으로 활용하게 된다. 즉, 현재 token을 생성할 때 이전 token들을 활용하는 것이다. 그런데 model의 학습 초반 성능은 말그대로 엉터리 결과일 것이기 떄문에, model이 도출해낸 엉터리 token을 이후 학습에 사용하게 되면 점점 결과물은 미궁으로 빠질 것이다. 엉터리 token으로 유추해낸 다음 token, 또 그들을 사용해 다음 token을 만들어내면 학습이 불가능하다. 이를 위해서 Teacher Forcing을 사용하게 된다. Teacher Forcing이란, Supervised Learning에서 label data를 input으로 활용하는 것이다. RNN으로 번역 model을 만든다고 할 때, 학습 과정에서 model이 생성해낸 token을 다음 token 생성 때 사용하는 것이 아닌, 실제 label data의 token을 사용하게 되는 것이다. 우선 정확도 100%를 달성하는 이상적인 model이라고 생각해보자.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-28-Transformer-in-pytorch/teacher_forcing_ideal.png&quot; alt=&quot;teacher_forcing_ideal.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;우리의 일반적인 생각대로 RNN 이전 cell의 output을 활용해 다음 cell에서 token을 생성해낼 수 있다. 그런데 이런 model을 실존하지 않는다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-28-Transformer-in-pytorch/teacher_forcing_incorrect.png&quot; alt=&quot;teacher_forcing_incorrect.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;현실에서는, 특히나 model 학습 초창기에는 위처럼 잘못된 token을 생성해내고, 그 이후 계속적으로 잘못된 token이 생성될 것이다. 초반에 하나의 token이 잘못 도출되었다고 이후 token이 모두 다 잘못되게 나온다면 제대로 된 학습이 진행되기 힘들 것이다. 따라서 이를 위해 Teacher Forcing을 사용한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-28-Transformer-in-pytorch/teacher_forcing_correct.png&quot; alt=&quot;teacher_forcing_correct.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;실제 labeled data(Ground Truth)를 RNN cell의 input으로 사용하는 것이다. 정확히는 Ground Truth의 [:-1]로 slicing을 한 것이다(마지막 token인 &lt;EOS&gt;를 제외하는 것이다). 이를 통해서 model이 잘못된 token을 생성해내더라도 이후 제대로 된 token을 생성해내도록 유도할 수 있다.&lt;/EOS&gt;&lt;/p&gt;

&lt;p&gt;하지만 이는 model 학습 과정에서 Ground Truth를 포함한 dataset을 갖고 있을 때에나 가능한 것이기에 Test나 실제로 Real-World에 Deliever될 때에는 model이 생성해낸 이전 token을 사용하게 된다.&lt;/p&gt;

&lt;p&gt;이처럼 학습 과정과 실제 사용에서의 괴리가 발생하기는 하지만, model의 학습 성능을 비약적으로 향상시킬 수 있다는 점에서 대부분의 NLP model에서 필수적으로 사용하는 기법이다.&lt;/p&gt;

&lt;h4 id=&quot;teacher-forcing-in-transformer-subsequent-masking&quot;&gt;Teacher Forcing in Transformer (Subsequent Masking)&lt;/h4&gt;

&lt;p&gt;Teacher Forcing 개념을 이해하고 나면 Transformer Decoder에 input으로 들어오는 sentence가 어떤 것인지 이해할 수 있다. ground truth[:-1]의 sentence embedding일 것이다. 하지만 이러한 방식으로 Teacher Forcing이 Transformer에 그대로 적용될 수 있을까? 결론부터 말하자면 그래서는 안된다. 위에서 Teacher Forcing에서 예시를 든 RNN Model은 이전 cell의 output을 이후 cell에서 사용할 수 있었다. 앞에서부터 순선대로 RNN cell이 실행되기 때문에 이러한 방식이 가능했다. 하지만 Transformer가 RNN에 비해 갖는 가장 큰 장점은 병렬 연산이 가능하다는 것이었다. ground truth의 embedding을 matrix로 만들어 input으로 그대로 사용하게 되면, Decoder에서 Self-Attention 연산을 수행하게 될 때 현재 출력해내야 하는 token의 정답까지 알고 있는 상황이 발생한다. 따라서 masking을 적용해야 한다. &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;번째 token이라고 한다면, &lt;script type=&quot;math/tex&quot;&gt;1 \thicksim i-1&lt;/script&gt;의 token은 보이지 않도록 처리를 해야 하는 것이다. 이러한 masking 기법을 subsequent masking이라고 한다. pytorch code로 구현해보자.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;subsequent_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;atten_shape&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;mask&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;triu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ones&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;atatn_shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;astype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'uint8'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# masking with upper triangle matrix&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;from_numpy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# reverse (masking=False, non-masking=True)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;make_std_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tgt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;tgt_mask&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tgt&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# pad masking&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;tgt_mask&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tgt_mask&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unsqueeze&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# reshape (n_batch, seq_len) -&amp;gt; (n_batch, 1, seq_len)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;tgt_mask&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tgt_mask&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Variable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subsequent_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tgt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;type_as&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tgt_mask&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# pad_masking &amp;amp; subsequent_masking&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tgt_mask&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;make_std_mask함수는 subsequent_mask 함수를 호출해 subsequent masking을 생성하고, 이를 pad masking과 결합한다. 위의 code는 Transformer 내부가 아닌 Batch class 내에서 실행되는 것이 바람직할 것이다. Transformer 내부 작동이 아닌 전처리 과정이기 때문이다. 따라서 Encoder에 적용되는 pad masking도 동일하게 Batch class 내에서 생성될 것이다. 이는 결국 Transformer 외부에서 넘어와야 하기 때문에 Transformer code를 다음과 같이 수정한다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Transformer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;

	&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;encoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;decoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
		&lt;span class=&quot;nb&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Transformer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
		&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encoder&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;encoder&lt;/span&gt;
		&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;decoder&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;decoder&lt;/span&gt;

	&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;src&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;trg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;src_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;trg_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;encoder_output&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;src&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;decoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;trg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;encoder_output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;decoder-layer&quot;&gt;Decoder Layer&lt;/h3&gt;

&lt;p&gt;Decoder 역시 Encoder와 마찬가지로 &lt;script type=&quot;math/tex&quot;&gt;N&lt;/script&gt;개의 Decoder Layer가 겹겹이 쌓인 구조이다. 이 때 주목해야 하는 점은 Encoder에서 넘어오는 Context가 각 Decoder Layer마다 input으로 들어간다는 것이다. 그 외에는 Encoder와 차이가 전혀 없다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-28-Transformer-in-pytorch/decoder.png&quot; alt=&quot;decoder.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그렇다면 각 Decoder Layer는 어떻게 생겼을까?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-28-Transformer-in-pytorch/decoder_layer.png&quot; alt=&quot;decoder_layer.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Decoder Layer는 Encoder Layer와 달리 Multi-Head Attention Layer가 2개가 존재한다. 첫번째 layer는 Masked Multi-Head Attention Layer라고 부르는데, 이는 위에서 언급했던 subsequent masking이 적용되기 떄문이다. 두번째 layer는 특징이 Encoder에서 넘어온 Context를 input으로 받아 사용한다는 것이다. 즉, Encoder의 Context는 Decoder 내 각 Decoder Layer의 두번째 Multi-Head Attention Layer에서 사용되게 된다. 마지막 Position-wise Feed-Forward Layer는 Encoder Layer의 것과 완전히 동일하므로 설명을 생략한다. 이제 두 Multi-Head Attention Layer에 대해서 Encoder의 것과 비교하며 특징을 살펴보자.&lt;/p&gt;

&lt;h3 id=&quot;masked-multi-head-attention-layer&quot;&gt;Masked Multi-Head Attention Layer&lt;/h3&gt;

&lt;p&gt;Masked Multi-Head Attention Layer에 대한 설명은 특별한 것이 없다. Encoder의 것과 완전히 동일한데 다만 mask로 들어오는 인자가 일반적인 pad masking에 더해 subsequent masking까지 적용되어 있다는 점만이 차이일 뿐이다. 즉, 이 layer는 Self-Attention을 수행하는 layer이다. ‘&lt;strong&gt;Self&lt;/strong&gt;‘에 주목하자. &lt;strong&gt;같은&lt;/strong&gt; sentence 내 token들 사이의 attention을 찾는 것이다. 이는 다음 Multi-Head Attention Layer와 가장 큰 차이점이다.&lt;/p&gt;

&lt;h3 id=&quot;multi-head-attention-layer-1&quot;&gt;Multi-Head Attention Layer&lt;/h3&gt;

&lt;p&gt;Decoder의 가장 핵심적인 부분이다. Decoder Layer 내 이전 Masked Multi-Head Attention Layer에서 넘어온 output을 input으로 받는다. 여기에 추가적으로 Encoder에서 도출된 Context도 input으로 받는다. 두 input의 사용 용도는 완전히 다르다. &lt;strong&gt;Decoder Layer 내부에서 전달된 input&lt;/strong&gt;(teacher forcing으로 넘어온 input)&lt;strong&gt;은 Query로써 사용&lt;/strong&gt;하고, Encoder에서 넘어온 &lt;strong&gt;Context는 Key, Value로써 사용&lt;/strong&gt;하게 된다. 이 점을 반드시 기억하고 넘어가자. 정리하자면 Decoder Layer의 2번째 layer는 Decoder에서 넘어온 input과 Encoder에서 넘어온 input 사이의 Attention을 계산하는 것이다. 따라서 Self-Attention이 아니다. 우리가 Decoder에서 도출해내고자 하는 최종 output은 teacher forcing으로 넘어온 sentence와 최대한 유사한 sentence이다. 따라서 Decoder Layer 내 이전 layer에서 넘어오는 input이 Query가 되고, 이에 상응하는 Encoder에서의 Attention을 찾기 위해 Context를 Key, Value로 두게 된다. 번역 task를 생각했을 때 가장 직관적으로 와닿는다. 만약 English를 French로 번역하고자 한다면, Encoder의 input은 English sentence일 것이고, Encoder가 도출해낸 Context는 English에 대한 Context일 것이다. Decoder의 input(teacher forcing)과 output은 French sentence일 것이다. 따라서 이 경우에는 Query가 French, Key와 Value는 English가 되어야 한다.&lt;/p&gt;

&lt;p&gt;지금 와서 Multi-Head Attention Layer를 보면 이전과 다르게 보일 것이다. query, key, value를 굳이 각각 별개의 인자로 받은 이유는 Decoder Layer 내에서 query와 key, value는 서로 다른 embedding에서 넘어오기 때문이다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; &lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;MultiHeadAttentionLayer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;

		&lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;

	&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;query&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mask&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
		
		&lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;decoder-code-in-pytorch&quot;&gt;Decoder Code in Pytorch&lt;/h3&gt;

&lt;p&gt;이제 Decoder와 Decoder Layer에 대한 code를 완성해보자. Position-wise Feed Forward Network와 Residual Connection Layer는 모두 동일하게 사용한다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Decoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sub_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
		&lt;span class=&quot;nb&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Decoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
		&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
			&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;copy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;deepcopy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sub_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

	&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;encoder_output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;encoder_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;layer&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;encoder_output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;encoder_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;주목해야 할 점은 encoder_mask (encoder에서 사용한 pad masking)는 decoder에서도 사용된다는 것이다. 결국 Decoder의 forward함수는 Decoder의 input sentence (x), Decoder의 subsequent mask(mask), encoder의 Context (encoder_output), encoder의 mask (encoder_mask)를 모두 인자로 받아야 한다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;DecoderLayer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;masked_multi_head_attention_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;multi_head_attention_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;position_wise_feed_forward_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;norm_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
		&lt;span class=&quot;nb&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DecoderLayer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
		&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;masked_multi_head_attention_layer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ResidualConnectionLayer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;masked_multi_head_attention_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;copy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;deepcopy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;norm_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
		&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;multi_head_attention_layer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ResidualConnectionLayer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;multi_head_attention_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;copy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;deepcopy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;norm_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
		&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;position_wise_feed_forward_layer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ResidualConnectionLayer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;position_wise_feed_forward_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;copy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;deepcopy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;norm_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

	&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;encoder_output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;encoder_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;masked_multi_head_attention_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;query&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mask&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;multi_head_attention_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;query&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encoder_output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encoder_output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mask&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encoder_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;position_wise_feed_forward_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;DecoderLayer에서 masked_multi_head_attention_layer에서는 query, key, value를 모두 decoder의 input sentence (x)로 사용한다. 반면 multi_head_attention_layer에서는 query는 masked_multi_head_attention_layer에서의 결과(output)를, key, value는 Context(encoder_output)으로 사용한다. 이 때 mask는 subsequent_mask가 아닌 일반 pad mask(encoder_mask)를 넘겨준다.&lt;/p&gt;

&lt;p&gt;Transformer도 다음과 같이 수정된다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Transformer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;

	&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;encoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;decoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
		&lt;span class=&quot;nb&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Transformer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
		&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encoder&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;encoder&lt;/span&gt;
		&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;decoder&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;decoder&lt;/span&gt;

	&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;src&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;trg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;src_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;trg_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;encoder_output&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;src&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;src_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;decoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;trg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;trg_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;encoder_output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;src_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&quot;transformers-input-positional-encoding&quot;&gt;Transformer’s Input (Positional Encoding)&lt;/h2&gt;

&lt;p&gt;지금까지 Encoder와 Decoder의 내부 구조가 어떻게 이루어져 있는지 분석하고 code로 구현까지 마쳤다. 이번에는 Encoder와 Decoder의 input으로는 sentence는 어떤 형태인지 알아보자. Transformer의 input은 단순한 sentence embedding에 더해 Positional Encoding이 추가되게 된다. 전체 TransformerEmbedding은 단순 Embedding과 PositionalEncoding의 sequential이다. code는 단순하다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;TransformerEmbedding&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;embedding&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;positional_encoding&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
		&lt;span class=&quot;nb&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;TransformerEmbedding&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
		&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;embedding&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Sequential&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;embedding&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;positional_encoding&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

	&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;embedding&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Embedding 역시 단순하다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Embedding&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d_embed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vocab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
		&lt;span class=&quot;nb&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Embedding&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
		&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;embedding&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Embedding&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vocab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d_embed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vocab&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vocab&lt;/span&gt;
		&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d_embed&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d_embed&lt;/span&gt;

	&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;embedding&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;math&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d_embed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;vocabulary와 &lt;script type=&quot;math/tex&quot;&gt;d_{embed}&lt;/script&gt;를 사용해 embedding을 생성해낸다. 주목할 점은 embedding에도 scaling을 적용한다는 점이다. forward 함수에서 &lt;script type=&quot;math/tex&quot;&gt;\sqrt{d_{embed}}&lt;/script&gt;를 곱해주게 된다.&lt;/p&gt;

&lt;p&gt;마지막으로 PositionalEncoding을 살펴보자.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;PositionalEncoding&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d_embed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;max_seq_len&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
		&lt;span class=&quot;nb&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;PositionalEncoding&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;encoding&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;max_seq_len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d_embed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;position&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;max_seq_len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unsqueeze&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;div_term&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d_embed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;math&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;10000.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d_embed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;encoding&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;position&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;div_term&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;encoding&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cos&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;position&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;div_term&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encoding&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;encoding&lt;/span&gt;
	
	&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Variable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encoding&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;requires_grad&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dropout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;code가 난해한데, 직관적으로 작동 원리만 이해하고 넘어가도 충분하다. PositionalEncoding의 목적은 positional정보(대표적으로 token의 순서, 즉 index number)를 정규화시키기 위한 것이다. 단순하게 index number를 positionalEncoding으로 사용하게 될 경우, 만약 training data에서는 최대 문장의 길이가 30이었는데 test data에서 길이 50인 문장이 나오게 된다면 30~49의 index는 model이 학습한 적이 없는 정보가 된다. 이는 제대로 된 성능을 기대하기 어려우므로, positonal 정보를 일정한 범위 안의 실수로 제약해두는 것이다. 여기서 &lt;script type=&quot;math/tex&quot;&gt;sin&lt;/script&gt;함수와 &lt;script type=&quot;math/tex&quot;&gt;cos&lt;/script&gt;함수를 사용하는데, 짝수 index에는 &lt;script type=&quot;math/tex&quot;&gt;sin&lt;/script&gt;함수를, 홀수 index에는 &lt;script type=&quot;math/tex&quot;&gt;cos&lt;/script&gt;함수를 사용하게 된다. 이를 사용할 경우 항상 -1에서 1 사이의 값만이 positional 정보로 사용되게 된다.&lt;/p&gt;

&lt;p&gt;구현 상에서 주의할 점은 forward 함수 내에서 생성하는 Variable이 학습이 되지 않도록 requires_grad=False 옵션을 부여해야 한다는 것이다. PositionalEncoding은 학습되는 parameter가 아니기 때문이다.&lt;/p&gt;

&lt;p&gt;이렇게 생성해낸 embedding을 Transformer에 추가해주자. code를 수정한다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Transformer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;

	&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;src_embed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;trg_embed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;encoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;decoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
		&lt;span class=&quot;nb&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Transformer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
		&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;src_embed&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;src_embed&lt;/span&gt;
		&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;trg_embed&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;trg_embed&lt;/span&gt;
		&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encoder&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;encoder&lt;/span&gt;
		&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;decoder&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;decoder&lt;/span&gt;

	&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;src&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;trg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;src_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;trg_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;encoder_output&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;src_embed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;src&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;src_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;decoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;trg_embed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;trg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;trg_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;encoder_output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;src_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&quot;after-decoder-generator&quot;&gt;After Decoder (Generator)&lt;/h2&gt;

&lt;p&gt;Decoder의 output이 그대로 Transformer의 최종 output이 되는 것은 아니다. 추가적인 layer를 거쳐간다. 이 layer들을 generator라고 부른다.&lt;/p&gt;

&lt;p&gt;우리가 결국 해내고자 하는 목표는 Decoder의 output이 sentence, 즉 token의 sequence가 되는 것이다. 그런데 Decoder의 output은 그저 (&lt;script type=&quot;math/tex&quot;&gt;\text{n_batch} \times \text{seq_len} \times \text{d_model}&lt;/script&gt;)의 shape를 갖는 matrix일 뿐이다. 이를 vocabulary를 사용해 실제 token으로 변환할 수 있도록 차원을 수정해야 한다. 따라서 FC Layer를 거쳐 마지막 dimension &lt;script type=&quot;math/tex&quot;&gt;\text{d_model}&lt;/script&gt;을 &lt;script type=&quot;math/tex&quot;&gt;\text{len(vocab)}&lt;/script&gt;으로 변경한다. 그래야 실제 vocabulary 내 token에 대응시킬 수 있는 값이 되기 때문이다. 이후 softmax 함수를 사용해 각 vocabulary에 대한 확률값으로 변환하게 되는데, 이 때 log_softmax를 사용해 성능을 향상시킨다.&lt;/p&gt;

&lt;p&gt;Generator를 직접 Transformer code에 추가해보자.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Transformer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;

	&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;src_embed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;trg_embed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;encoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;decoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fc_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
		&lt;span class=&quot;nb&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Transformer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
		&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;src_embed&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;src_embed&lt;/span&gt;
		&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;trg_embed&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;trg_embed&lt;/span&gt;
		&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encoder&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;encoder&lt;/span&gt;
		&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;decoder&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;decoder&lt;/span&gt;
		&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fc_layer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fc_layer&lt;/span&gt;

	&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;src&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;trg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;src_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;trg_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;encoder_output&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;src_embed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;src&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;src_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;decoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;trg_embed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;trg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;trg_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;encoder_output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;src_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fc_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log_softmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;log_softmax에서는 dim=-1이 되는데, 마지막 dimension인 len(vocab)에 대한 확률값을 구해야 하기 때문이다.&lt;/p&gt;

&lt;h2 id=&quot;make-model&quot;&gt;Make Model&lt;/h2&gt;

&lt;p&gt;Transformer를 생성하는 예제 함수는 다음과 같이 작성할 수 있다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;make_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;src_vocab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
    &lt;span class=&quot;n&quot;&gt;trg_vocab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
    &lt;span class=&quot;n&quot;&gt;d_embed&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;512&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
    &lt;span class=&quot;n&quot;&gt;n_layer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
    &lt;span class=&quot;n&quot;&gt;d_model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;512&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
    &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
    &lt;span class=&quot;n&quot;&gt;d_ff&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2048&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;

		&lt;span class=&quot;n&quot;&gt;cp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;copy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;deepcopy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

		&lt;span class=&quot;c&quot;&gt;# multi_head_attention_layer 생성한 뒤 copy해 사용&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;multi_head_attention_layer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MultiHeadAttentionLayer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
                                    &lt;span class=&quot;n&quot;&gt;d_model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                    &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                    &lt;span class=&quot;n&quot;&gt;qkv_fc_layer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d_embed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
                                    &lt;span class=&quot;n&quot;&gt;fc_layer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d_embed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

		&lt;span class=&quot;c&quot;&gt;# position_wise_feed_forward_layer 생성한 뒤 copy해 사용    &lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;position_wise_feed_forward_layer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PositionWiseFeedForwardLayer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
                                         &lt;span class=&quot;n&quot;&gt;first_fc_layer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d_embed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d_ff&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
                                         &lt;span class=&quot;n&quot;&gt;second_fc_layer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d_ff&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d_embed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    
		&lt;span class=&quot;c&quot;&gt;# norm_layer 생성한 뒤 copy해 사용&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;norm_layer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;LayerNorm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d_embed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eps&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1e-6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

		&lt;span class=&quot;c&quot;&gt;# 실제 model 생성&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Transformer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;src_embed&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;TransformerEmbedding&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;    &lt;span class=&quot;c&quot;&gt;# SRC embedding 생성&lt;/span&gt;
                                &lt;span class=&quot;n&quot;&gt;embedding&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Embedding&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
                                                &lt;span class=&quot;n&quot;&gt;d_embed&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d_embed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                                                &lt;span class=&quot;n&quot;&gt;vocab&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;src_vocab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; 
                                &lt;span class=&quot;n&quot;&gt;positional_encoding&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PositionalEncoding&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
                                                &lt;span class=&quot;n&quot;&gt;d_embed&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d_embed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt; 
	
                &lt;span class=&quot;n&quot;&gt;trg_embed&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;TransformerEmbedding&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;    &lt;span class=&quot;c&quot;&gt;# TRG embedding 생성&lt;/span&gt;
                                &lt;span class=&quot;n&quot;&gt;embedding&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Embedding&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
                                                &lt;span class=&quot;n&quot;&gt;d_embed&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d_embed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                                                &lt;span class=&quot;n&quot;&gt;vocab&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;trg_vocab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; 
                                &lt;span class=&quot;n&quot;&gt;positional_encoding&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PositionalEncoding&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
                                                &lt;span class=&quot;n&quot;&gt;d_embed&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d_embed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;encoder&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Encoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;                    &lt;span class=&quot;c&quot;&gt;# Encoder 생성&lt;/span&gt;
                                &lt;span class=&quot;n&quot;&gt;sub_layer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;EncoderLayer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
                                                &lt;span class=&quot;n&quot;&gt;multi_head_attention_layer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;multi_head_attention_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
                                                &lt;span class=&quot;n&quot;&gt;position_wise_feed_forward_layer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;position_wise_feed_forward_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
                                                &lt;span class=&quot;n&quot;&gt;norm_layer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;norm_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;
                                &lt;span class=&quot;n&quot;&gt;n_layer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;decoder&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Decoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;                    &lt;span class=&quot;c&quot;&gt;# Decoder 생성&lt;/span&gt;
                                &lt;span class=&quot;n&quot;&gt;sub_layer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DecoderLayer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
                                                &lt;span class=&quot;n&quot;&gt;masked_multi_head_attention_layer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;multi_head_attention_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
                                                &lt;span class=&quot;n&quot;&gt;multi_head_attention_layer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;multi_head_attention_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
                                                &lt;span class=&quot;n&quot;&gt;position_wise_feed_forward_layer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;position_wise_feed_forward_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
                                                &lt;span class=&quot;n&quot;&gt;norm_layer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;norm_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;
                                &lt;span class=&quot;n&quot;&gt;n_layer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;fc_layer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;trg_vocab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;    &lt;span class=&quot;c&quot;&gt;# Generator의 FC Layer 생성&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h1 id=&quot;전체-구현-code&quot;&gt;전체 구현 Code&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;본 포스트 내에서의 code와 세부적인 부분에서 다소 차이가 있을 수 있다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;h4 id=&quot;github-link&quot;&gt;&lt;a href=&quot;https://github.com/cpm0722/NLP/tree/main/transformer&quot;&gt;GitHub Link&lt;/a&gt;&lt;/h4&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;reference&quot;&gt;Reference&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;h4 id=&quot;harvard-nlp&quot;&gt;&lt;a href=&quot;http://nlp.seas.harvard.edu/2018/04/03/attention.html&quot;&gt;Harvard NLP&lt;/a&gt;&lt;/h4&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;h4 id=&quot;wikidocs&quot;&gt;&lt;a href=&quot;https://wikidocs.net/31379&quot;&gt;WikiDocs&lt;/a&gt;&lt;/h4&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><category term="NLP" /><category term="Pytorch" /><summary type="html">Paper Link</summary></entry><entry><title type="html">[NLP 논문 리뷰] Deep Contextualized Word Representations (ELMo)</title><link href="https://cpm0722.github.io/Deep-contextualized-word-representations/" rel="alternate" type="text/html" title="[NLP 논문 리뷰] Deep Contextualized Word Representations (ELMo)" /><published>2021-01-01T18:00:00-06:00</published><updated>2021-01-01T18:00:00-06:00</updated><id>https://cpm0722.github.io/Deep-contextualized-word-representations</id><content type="html" xml:base="https://cpm0722.github.io/Deep-contextualized-word-representations/">&lt;h2 id=&quot;paper-info&quot;&gt;Paper Info&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1802.05365&quot;&gt;Archive Link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1802.05365.pdf&quot;&gt;Paper Link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Submit Date: Feb 15, 2018&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;word2vec이나 glove와 같은 기존의 word embedding 방식은 다의어의 모든 의미를 담아내기 곤란하다는 심각한 한계점을 갖고 있다. ELMo(Embeddings from Language Models)는 이러한 한계점을 극복하기 위해 embedding에 sentence의 전체 context를 담도록 했다. pre-train된 LSTM layer에 sentence 전체를 넣어 각 word의 embedding을 구해내는 model이다. 이로 인해 기존의 embedding에 비해 복잡한 syntax, semantic한 특징들을 담아낼 수 있었고, 다의어 문제도 해결했다. LSTM은 forward, backward 2가지 방향을 사용하는데 각 LSTM은 다음 word를 예측하는 model이다. 이러한 LSTM layer를 다층으로 구성해 context-dependent한 word embedding을 생성해내게 된다. ELMo를 사용한 word embedding은 수많은 NLP task에서 성능 향상을 이끌어낼 수 있다.&lt;/p&gt;

&lt;h1 id=&quot;elmo-embeddings-from-language-models&quot;&gt;ELMo: Embeddings from Language Models&lt;/h1&gt;

&lt;p&gt;ELMo와 여타 word embedding model과의 가장 큰 차이점은 각 단어마다 고정된 word embedding을 사용하는 것이 아닌 pre-trained model 자체를 하나의 function 개념으로 사용한다는 것이다. 이전의 word2vec 등은 model을 train시킨 뒤 각 word마다의 embedding vector만을 추출해 사용했지만, ELMo는 word마다 embedding vector가 특정되지 않기에 이와 같은 방식이 불가능하고, ELMo model을 NLP model의 앞에 연결하는 방식으로 사용하게 된다.&lt;/p&gt;

&lt;h2 id=&quot;bidirectional-language-models&quot;&gt;Bidirectional language models&lt;/h2&gt;

&lt;p&gt;input sequence가 &lt;script type=&quot;math/tex&quot;&gt;N&lt;/script&gt;개의 token들 (&lt;script type=&quot;math/tex&quot;&gt;t_1, t_2, ..., t_N&lt;/script&gt;)이라고 가정하자. &lt;script type=&quot;math/tex&quot;&gt;t_k&lt;/script&gt;는 모두 각 word에 대한 context-independent token이다.&lt;/p&gt;

&lt;p&gt;forward LSTM은 &lt;script type=&quot;math/tex&quot;&gt;t_1, ... , t_{k-1}&lt;/script&gt;이 주어졌을 때 &lt;script type=&quot;math/tex&quot;&gt;t_k&lt;/script&gt;를 예측하는 model이다.&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt;번째 LSTM layer에서 &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt;번째 token에 대한 forward LSTM output은 &lt;script type=&quot;math/tex&quot;&gt;\overrightarrow{h}_{k,j}&lt;/script&gt;이다. 마지막 LSTM layer의 output인 &lt;script type=&quot;math/tex&quot;&gt;\overrightarrow{h}_{k,L}&lt;/script&gt;을 softmax에 넣어 최종적으로 &lt;script type=&quot;math/tex&quot;&gt;t_{k+1}&lt;/script&gt;을 예측하게 된다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(t_1, t_2, ... , t_N) = \prod^N_{k=1}{p(t_k \vert t_1, t_2, ..., t_{k-1})}&lt;/script&gt;

&lt;p&gt;backward LSTM은 &lt;script type=&quot;math/tex&quot;&gt;t_{k+1}, t_{k+2}, ... ,t_N&lt;/script&gt;이 주어졌을 때 &lt;script type=&quot;math/tex&quot;&gt;t_k&lt;/script&gt;를 예측하는 model이다.&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt;번째 LSTM layer에서 &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt;번째 token에 대한 backward LSTM output은 &lt;script type=&quot;math/tex&quot;&gt;\overleftarrow{h}_{k,j}&lt;/script&gt;이다. 마지막 LSTM layer의 output인 &lt;script type=&quot;math/tex&quot;&gt;\overleftarrow{h}_{k,L}&lt;/script&gt;을 softmax에 넣어 최종적으로 &lt;script type=&quot;math/tex&quot;&gt;t_{k-1}&lt;/script&gt;을 예측하게 된다.&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;&lt;/script&gt;p(t_1, t_2, … , t_N) = \prod^N_{k=1}{p(t_k \vert t_{k+1}, t_{k+2}, …, t_{N})}&lt;script type=&quot;math/tex&quot;&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;biLM은 위의 두 LSTM을 결합한 것이다. 두 방향에 대한 log likelihood를 최대화하는 것을 목표로 한다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum^N_{k=1} {(\log p(t_k \vert t_1, ... , t_{k-1} ; \Theta_x, \overrightarrow{\Theta}_{\text{LSTM}}, \Theta_s) + \log p(t_k \vert t_{k+1}, ... , t_{N} ; \Theta_x, \overleftarrow{\Theta}_{\text{LSTM}}, \Theta_s))}&lt;/script&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\Theta_x&lt;/script&gt;는 token representation(&lt;script type=&quot;math/tex&quot;&gt;t_1, ... , t_N&lt;/script&gt;)에 대한 parameter이고, &lt;script type=&quot;math/tex&quot;&gt;\Theta_s&lt;/script&gt;는 softmax layer에 대한 parameter이다. 이 두 parameter는 전체 direction에 관계 없이 같은 값을 공유하지만, LSTM의 parameter들은 두 LSTM model이 서로 다른 값을 갖는다.&lt;/p&gt;

&lt;h2 id=&quot;elmo&quot;&gt;ELMo&lt;/h2&gt;

&lt;p&gt;ELMo에서는 새로운 representation을 사용하는데, 이를 얻기 위해서는 LSTM layer의 개수를 &lt;script type=&quot;math/tex&quot;&gt;L&lt;/script&gt;이라고 했을 때 총 &lt;script type=&quot;math/tex&quot;&gt;2L+1&lt;/script&gt;개의 representation을 concatenate해야 한다. input representation layer 1개와 forward, backward LSTM 각각 &lt;script type=&quot;math/tex&quot;&gt;L&lt;/script&gt;개이다.&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;&lt;/script&gt;R_k = {x_k, \overrightarrow{h}&lt;em&gt;{k,j}, \overleftarrow{h}&lt;/em&gt;{k,j} \vert j=1, … , L}&lt;script type=&quot;math/tex&quot;&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;input representation layer를 &lt;script type=&quot;math/tex&quot;&gt;j=0&lt;/script&gt;으로, &lt;script type=&quot;math/tex&quot;&gt;\overrightarrow{h}_{k,j}&lt;/script&gt;와 &lt;script type=&quot;math/tex&quot;&gt;\overleftarrow{h}_{k,j}&lt;/script&gt;의 concatenation을 &lt;script type=&quot;math/tex&quot;&gt;h_{k,j}&lt;/script&gt;로 표현한다면 다음과 같은 일반화된 수식으로 ELMO representation을 표현할 수 있다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;R_k = \{h_{k,j} \vert j=0, ..., L\}&lt;/script&gt;

&lt;p&gt;결국 &lt;script type=&quot;math/tex&quot;&gt;R_k&lt;/script&gt;는 &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt;번째 token에 대한 모든 representation이 연결되어 있는 것인데, 이를 사용해 최종 ELMo embedding vector를 만들어내게 된다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\text{ELMo}^{task}_k=E(R_k;\Theta^{task}) = \gamma^{task} \sum^L_{j=0} {s_j^{task}h_{k,j}}&lt;/script&gt;

&lt;p&gt;각 LSTM layer의 output인 &lt;script type=&quot;math/tex&quot;&gt;h_{k,j}&lt;/script&gt;를 모두 더하는데 이 때 각각에 softmax-normalized weights &lt;script type=&quot;math/tex&quot;&gt;s_j&lt;/script&gt;를 곱한 뒤 더하게 된다. 당연하게도 &lt;script type=&quot;math/tex&quot;&gt;\sum^L_{j=0}s_j^{task}=1&lt;/script&gt;이다. 마지막에 최종적으로 &lt;script type=&quot;math/tex&quot;&gt;\gamma&lt;/script&gt;를 곱하게 되는데, scale parameter이다. &lt;script type=&quot;math/tex&quot;&gt;s_j&lt;/script&gt;와 &lt;script type=&quot;math/tex&quot;&gt;\gamma&lt;/script&gt;는 모두 learnable parameter이면서 optimization에서 매우 중요한 역할을 담당한다.&lt;/p&gt;

&lt;h2 id=&quot;using-bilms-for-supervised-nlp-tasks&quot;&gt;Using biLMs for supervised NLP tasks&lt;/h2&gt;

&lt;p&gt;supervised downstream task에 ELMo를 적용하는 구체적인 방법은 간단하다. 대부분의 supervised NLP model들의 input은 모두 context-independent token들의 sequence이다. 이러한 공통점 덕분에 동일한 방법으로 대부분의 task에 ELMo를 적용할 수 있다. 우선 NLP model의 input layer와 다음 layer 사이에 ELMo를 삽입한다. 이후 ELMo 이후 layer에서는 input을 &lt;script type=&quot;math/tex&quot;&gt;[x_k;\text{ELMo}_k^{task}]&lt;/script&gt;로 사용한다. 즉, input token과 ELMo embedding vector의 concatenation을 사용하는 것이다. NLP model을 train할 때에는 ELMo의 weight들은 모두 freeze시킨다. 즉, ELMo model은 NLP model이 train될 때 함께 train되지 않는다.&lt;/p&gt;

&lt;p&gt;SNLI, SQuAD와 같은 특정 task에서는 RNN의 output &lt;script type=&quot;math/tex&quot;&gt;h_k&lt;/script&gt;를 &lt;script type=&quot;math/tex&quot;&gt;[h_k;\text{ELMo}_k^{task}]&lt;/script&gt;로 교체했을 때 더 좋은 성능을 보이기도 했다. 또한 일반적으로 dropout과 &lt;script type=&quot;math/tex&quot;&gt;\lambda \Vert w \Vert^2_2&lt;/script&gt;를 loss에 더하는 regularization을 사용했을 때 더 좋은 성능을 보였다.&lt;/p&gt;

&lt;h2 id=&quot;pre-trained-bidirectional-language-model-architecture&quot;&gt;Pre-trained bidirectional language model architecture&lt;/h2&gt;

&lt;p&gt;ELMo는 기존의 pre-trained biLM와 큰 구조는 비슷하지만 몇가지 차이점이 존재하는데, 가장 큰 차이점은 LSTM layer 사이에 residual connection을 사용했다는 점이다. 이를 통해 input의 feature를 더 잘 전달하고 gradient vanishing을 해결할 수 있다. &lt;script type=&quot;math/tex&quot;&gt;L=2&lt;/script&gt;개의 biLSTM layer를 사용했고, 4096개의 unit과 512차원의 projection을 사용했다. biLSTM의 input으로는 2048 character n-gram을 CNN에 넣는 char-CNN embedding을 사용했다.&lt;/p&gt;

&lt;h1 id=&quot;evaluation&quot;&gt;Evaluation&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-02-Deep-contextualized-word-representations/01.jpg&quot; alt=&quot;01.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ELMo를 단순하게 추가하는 것만으로도 baseline model에 비해 성능이 향상됐고, 이를 통해 SOTA를 달성할 수 있었다.&lt;/p&gt;

&lt;h1 id=&quot;analysis&quot;&gt;Analysis&lt;/h1&gt;

&lt;h2 id=&quot;alternate-layer-weighting-schemes&quot;&gt;Alternate layer weighting schemes&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-02-Deep-contextualized-word-representations/02.jpg&quot; alt=&quot;02.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ELMo representation을 사용하지 않고 단순하게 LSTM의 마지막 layer의 output (&lt;script type=&quot;math/tex&quot;&gt;h_{k,L}&lt;/script&gt;)을 사용하는 방법도 있다. 이러한 방식은 biLM, CoVe 등 기존의 많은 연구에서 시도되었는데 이와 ELMo representation을 사용한 경우를 비교해본다. Table 2의 Last Only는 마지막 LSTM layer의 output만을 word embedding으로 사용하는 경우이다.&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt;는 softmax-normalized weights &lt;script type=&quot;math/tex&quot;&gt;s_j&lt;/script&gt;에 대한 regularization parameter인데, 0~1 사이의 값을 갖는다. &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt;가 1에 가까울수록 각 layer에서의 output들을 평균에 가깝게 계산해 최종 vector를 생성해내고 (&lt;script type=&quot;math/tex&quot;&gt;s_j&lt;/script&gt;가 모두 유사한 값), &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt;가 0에 가까울수록 각 layer에서의 output들에 다양한 값들이 곱해서 더해진다.&lt;/p&gt;

&lt;p&gt;Table 2에서는 task에 관계 없이 동일한 경향성을 보이는데, baseline model보다 CoVe와 같은 마지막 LSTM layer의 output을 word embedding으로 사용한 model이 더 좋은 성능을 보였다. 또한 CoVe보다 ELMo가 더 좋은 성능을 보였는데, 이 중 &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt;가 낮은 경우가 더 좋은 결과를 보였다.&lt;/p&gt;

&lt;h2 id=&quot;where-to-include-elmo&quot;&gt;Where to include ELMo?&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-02-Deep-contextualized-word-representations/03.jpg&quot; alt=&quot;03.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위에서 언급했듯 supervised NLP model에 ELMo를 적용할 때에는 input layer의 직후에 ELMo를 삽입했다. SQuAD, SNLI, SRL의 baseline model은 모두 biRNN model인데, ELMo를 biRNN 직후에도 삽입을 한 뒤 성능을 비교했다. SQuAD와 SNLI에 있어서는 ELMo를 biRNN 이후에도 추가하는 것이 더 좋은 성능을 보여줬는데, 이는 SNLI와 SQuAD는 biRNN 직후 attention layer가 있는데, biRNN과 attention layer 사이에 ELMo를 추가함으로써 ELMo representation에 attention이 직접적으로 반영됐기 때문이라고 유추해 볼 수 있다.&lt;/p&gt;

&lt;h2 id=&quot;what-information-is-captured-by-the-bilms-representations&quot;&gt;What information is captured by the biLM’s representations?&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-02-Deep-contextualized-word-representations/04.jpg&quot; alt=&quot;04.jpg&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;word-sense-disambiguation&quot;&gt;Word sense disambiguation&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-02-Deep-contextualized-word-representations/05.jpg&quot; alt=&quot;05.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;WSD는 다의어의 의미를 구분짓는 task로 embedding이 얼마나 semantic 정보를 잘 담고 있는지에 대한 지표이다. ELMo는 WSD-specific한 model과 동등한 수치를, CoVe보다는 월등히 높은 수치를 달성했다. 주목할만한 점은 ELMo의 first LSTM layer의 output보다는 second layer (top layer)의 output이 WSD에서 좋은 성능을 보였다는 점이다.&lt;/p&gt;

&lt;h3 id=&quot;pos-tagging&quot;&gt;POS tagging&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-02-Deep-contextualized-word-representations/06.jpg&quot; alt=&quot;06.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;POS tagging은 word의 품사를 tagging하는 task로 embedding이 얼마나 syntax 정보를 잘 담고 있는지에 대한 지표이다. 여기서도 ELMo는 POS tagging-specific model과 동둥한 수준의 성능을, CoVe보다는 월등히 높은 성능을 보여줬다. WSD와는 다르게 오히려 first LSTM layer의 output이 top layer의 output보다 POS tagging에서 더 좋은 성능을 보였다는 점이 주목할 만하다.&lt;/p&gt;

&lt;h3 id=&quot;implications-for-supervised-tasks&quot;&gt;Implications for supervised tasks&lt;/h3&gt;

&lt;p&gt;결론적으로 ELMo에서 각 layer는 담고 있는 정보의 종류가 다르다고 할 수 있는데, 층이 낮은 layer(input layer에 가까운 layer)일수록 syntax 정보를, 층이 높은 layer(output layer에 가까운 layer)일수록 semantic 정보를  저장한다.&lt;/p&gt;

&lt;h2 id=&quot;sample-efficiency&quot;&gt;Sample efficiency&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-02-Deep-contextualized-word-representations/07.jpg&quot; alt=&quot;07.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ELMo의 사용은 일정 수준 이상의 성능 달성에 필요한 parameter update 횟수 및 전체 training set size를 획기적으로 줄여준다. SRL task에 있어서 ELMo 사용 이전 baseline model의 경우에는 486 epoch가 지나서야 score가 수렴했는데, ELMo를 추가하고 난 뒤에는 10 epoch만에 baseline model의 score를 능가했다.&lt;/p&gt;

&lt;p&gt;Figure 1에서는 같은 크기의 dataset에서 ELMo를 사용하는 경우가 훨씬 더 좋은 성능을 낸다는 것을 보여준다. 심지어 SRL task에서는 ELMo를 사용한 model이 training dataset의 단 1%을 학습했을 때 달성한 수치와 baseline model이 training dataset의 10%를 학습했을 때의 수치가 동일하다는 것을 확인할 수 있다.&lt;/p&gt;

&lt;h2 id=&quot;visualization-of-learned-weights&quot;&gt;Visualization of learned weights&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-02-Deep-contextualized-word-representations/08.jpg&quot; alt=&quot;08.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;softmax-normalized parameter &lt;script type=&quot;math/tex&quot;&gt;s_j&lt;/script&gt;를 시각화한 것이다. ELMo를 biRNN의 input과 output에 사용했을 때를 각각 나눠 비교했다. ELMo가 input에 사용된 경우에는 대개 first LSTM layer가 선호되는 경향을 보였다. 특히나 SQuAD에서 이러한 경향성이 가장 두드러지게 나타났다. 반면 ELMO가 output에 사용된 경우에는 weight가 균형있게 분배되었지만 낮은 layer가 조금 더 높은 선호를 보였다.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;biLM을 사용해 높은 수준의 context를 학습하는 ELMo model을 제안했다. ELMo model을 사용하면 대부분의 NLP task에서 성능이 향상되었다. 또한 layer의 층이 올라갈수록 syntax보다는 semantic한 정보를 담아낸다는 사실도 발견해냈다. 때문에 어느 한 layer를 사용하는 것보다는 모든 layer의 representation을 결합해 사용하는 것이 전반적인 성능 향상에 도움이 된다는 결론을 내릴 수 있다.&lt;/p&gt;</content><category term="NLP" /><summary type="html">Paper Info</summary></entry><entry><title type="html">[운영체제] Disk</title><link href="https://cpm0722.github.io/Disk/" rel="alternate" type="text/html" title="[운영체제] Disk" /><published>2020-12-17T18:00:00-06:00</published><updated>2020-12-17T18:00:00-06:00</updated><id>https://cpm0722.github.io/Disk</id><content type="html" xml:base="https://cpm0722.github.io/Disk/">&lt;hr /&gt;

&lt;p&gt;숭실대학교 컴퓨터학부 홍지만 교수님의 2020-2학기 운영체제 강의를 정리 및 재구성했다.&lt;/p&gt;

&lt;h1 id=&quot;hard-disk&quot;&gt;Hard Disk&lt;/h1&gt;

&lt;p&gt;hard disk는 가장 범용적으로 사용되는 저장 장치이다. main memory와 다르게 영속적(persistent)으로 data를 저장할 수 있다. hard disk는 물리적으로 회전(rotation)하면서 data를 저장할 장소를 찾는다. 전체 구성 요소는 다음과 같다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-12-18-Disk/01.png&quot; alt=&quot;01.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;hard disk는 여러 층으로 이루어져 있다. 각 층은 platter라는 하나의 원판으로 구성되고, 모든 원판의 중심을 통과하는 spindle이 있다. spindle은 실제로 rotation을 수행하는 장치이다. 각 platter는 여러 track으로 구성되는데, 원하는 track을 선택하기 위해 disk arm이 이동하게 된다. disk arm의 끝에 달려있는 disk head가 실제로 data를 읽게 된다. 각 track은 sector로 구분되는데, sector는 가상 memory에서의 frame 또는 block과 같이 data가 한번에 읽고 씌여지는 단위이다. disk arm은 원하는 track으로 이동할 때 사용하고, 원하는 sector로 이동하기 위해서는 spindle이 flatter를 rotation시켜야 한다. 따라서 disk I/O의 단계는 다음과 같이 정리된다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;disk arm을 해당 track으로 이동 (seek time)&lt;/li&gt;
  &lt;li&gt;해당 sector로 roatation (rotational delay)&lt;/li&gt;
  &lt;li&gt;실제로 data를 read (transfer time)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;disk I/O에 소요되는 시간은 seek time + rotational delay + transfer time이 된다. 이 중 rotational delay, transfer time은 HW의 성능에 의해 좌우되기 때문에 실제로 disk scheduling을 통해 향상시키고자 하는 것은 seek time이다.&lt;/p&gt;

&lt;h1 id=&quot;disk-scheduling&quot;&gt;Disk Scheduling&lt;/h1&gt;

&lt;p&gt;disk의 seek time을 줄일 수 있는 disk scheduling algorithm들에 대해 알아보자. queue에 읽어야 할 track number가 다음과 같은 순서로 들어온다고 가정한다. 또한 disk arm의 시작 위치는 15라고 가정한다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;15, 8, 17, 11, 3, 23, 19, 14, 20&lt;/script&gt;

&lt;h2 id=&quot;fcfsfirst-come-first-service&quot;&gt;FCFS(First Come First Service)&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-12-18-Disk/02.png&quot; alt=&quot;02.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;queue에 들어온 순서대로 track으로 이동하는 것이다. 즉, FIFO와 동일한 방식이다. 모든 track에 대해 공정하게 접근하지만, 비효율적인 이동이 다수 발생할 수 있다는 단점이 있다.&lt;/p&gt;

&lt;h2 id=&quot;sstf-shortest-seek-time-first&quot;&gt;SSTF (Shortest Seek Time First)&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-12-18-Disk/03.png&quot; alt=&quot;03.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;현재 disk arm의 위치에서 가장 적게 움직여 도달할 수 있는 track으로 먼저 이동하는 방식이다. FIFO에 비해 효율적이지만, 양극단(0, 24)에 가까운 track에 대해서는 starvation이 발생할 수 있다. 또한 하나의 track만이 계속 사용될 경우 arm이 움직이지 않고 고정되는 arm stickiness 현상이 발생할 수도 있다.&lt;/p&gt;

&lt;h2 id=&quot;scan&quot;&gt;SCAN&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-12-18-Disk/04.png&quot; alt=&quot;04.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;한 방향으로의 관성을 갖고 움직여 양극단에 도달했을 경우에 반대 방향으로 전환해 이동하는 방식이다. 위의 경우는 track number가 감소하는 방향으로 시작한다고 가정한 것이다. 5시점까지 수행해 track 3으로 이동한 후, 최저점(0)에 도달한 뒤에 방향을 전환하게 된다. 마지막 9시점에서 23까지 이동하고 난 후 최고점(24)에 도달하게 된다. 이후 다시 방향을 전환하게 될 것이다. SSTF와 동일하게 starvation, arm stickiness가 발생할 수 있다.&lt;/p&gt;

&lt;h2 id=&quot;c-scan&quot;&gt;C-SCAN&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-12-18-Disk/05.png&quot; alt=&quot;05.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;SCAN과 유사하지만 한 방향으로만 track을 읽어들이는 정책이다. 1~5시점까지는 track number를 감소시키면서 track을 읽어들이고, 5시점 이후에 최저점(0)에 도달한 뒤 최고점(24)로 이동해 다시 track number를 감소시키며 track을 읽어들인다. 이 역시 starvation, arm stickiness가 발생할 수 있다.&lt;/p&gt;

&lt;h2 id=&quot;c-look&quot;&gt;C-LOOK&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-12-18-Disk/06.png&quot; alt=&quot;06.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;C-SCAN과 유사하지만 양극단(0, 24)까지 이동하지 않고 queue에서 가장 작은 track number(3), 가장 큰 track number(23)까지만 이동한다. 여전히 starvation, arm stickiness가 발생할 수 있다.&lt;/p&gt;

&lt;h2 id=&quot;n-step-scan--fscan&quot;&gt;N-step-SCAN / FSCAN&lt;/h2&gt;

&lt;p&gt;queue를 길이가 N인 하위 queue로 분할한 뒤, 각 queue에 대해 개별적으로 SCAN 정책을 사용하는 방식이다. 하나의 queue가 처리되는 동안 들어오는 신규 요청들은 모두 다른 queue에 추가된다. 따라서 starvation, arm stickiness가 발생하지 않는다. N의 값에 따라 성능이 달라지는데, N의 값이 커질 경우 SCAN과 유사한 성능을 보이고, N=1일 경우에는 FCFS와 완전히 동일한 정책이다. 이 중 FSCAN은 N=2인 경우를 지칭한다.&lt;/p&gt;

&lt;h1 id=&quot;raid&quot;&gt;RAID&lt;/h1&gt;

&lt;p&gt;disk의 성능 지표로는 총 3가지가 있다. 성능(performance), 용량(capacity), 신뢰성(reliability)이다. performance는 얼마나 I/O time이 짧은가에 대한 지표이고, capacity는 얼마나 많은 data를 저장할 수 있는가에 대한 지표,reliability는 data가 얼마나 안전하게 저장되는가에 대한 지표이다. RAID는 여러 disk가 있을 때 사용하는 disk 가상화 기술로, 위의 지표 중 capacity를 다소 희생해 reliability를 향상시키는 정책이다.&lt;/p&gt;

&lt;h2 id=&quot;raid-0&quot;&gt;RAID 0&lt;/h2&gt;

&lt;p&gt;RAID 0은 capacity를 희생시키지 않고 performance를 향상시키는 정책이다. 큰 size의 data를 disk에 저장해야 할 때 여러 disk block을 사용해야 할 것이다. 이 때 대개 하나의 disk에 존재하는 block을 사용하게 되는데, RAID 0은 여러 disk의 block을 균등하게 사용하는 것이다. 이를 striping이라고 한다. 이로 인해 얻을 수 있는 이점은 performance인데, 하나의 disk에서 disk arm을 여러 번 이동시켜 여러 track을 읽어들일 필요 없이, 병렬적으로 (동시에) 각 disk에서 disk arm을 이동시켜 전체 seek time을 줄일 수 있다.&lt;/p&gt;

&lt;p&gt;RAID 0은 capacity를 희생시키지 않기 때문에 N개의 disk가 있을 경우 N개의 disk 전부를 온전히 data 저장 용도로 사용할 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-12-18-Disk/07.png&quot; alt=&quot;07.png&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;raid-1&quot;&gt;RAID 1&lt;/h2&gt;

&lt;p&gt;RAID 1은 mirroring 기법을 사용한다. mirroring이란 같은 disk를 그대로 또다른 disk에 복사해 저장하는 것이다. 이를 통해 둘 중 하나의 disk가 crash된다고 하더라도 다른 mirroring disk의 data를 통해 복구할 수 있다.&lt;/p&gt;

&lt;p&gt;RAID 1은 N개의 disk가 있을 경우 N/2개의 disk의 용량만큼 data를 저장할 수 있다. 대신 fault tolerance가 증가하게 된다. 즉, capacity를 희생해 reliability를 증가시킨 것이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-12-18-Disk/08.png&quot; alt=&quot;08.png&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;raid-10--raid-01&quot;&gt;RAID 10 / RAID 01&lt;/h2&gt;

&lt;p&gt;RAID 1에 대해 RAID 0을 수행하는 정책이다. capacity는 N/2가 된다. reliability와 performance가 모두 향상된다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-12-18-Disk/09.png&quot; alt=&quot;09.png&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;raid-01&quot;&gt;RAID 01&lt;/h2&gt;

&lt;p&gt;RAID 0에 대해 RAID 1을 수행하는 정책이다. capacity는 N/2가 된다. reliability와 performance가 모두 향상된다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-12-18-Disk/10.png&quot; alt=&quot;10.png&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;raid-234&quot;&gt;RAID 2/3/4&lt;/h2&gt;

&lt;p&gt;RAID 2/3/4는 parity disk를 사용하는 방식이다. 이중 RAID 2는 다수의 parity disk를 사용하는 것이고, RAID 3,4는 하나의 parity disk를 사용한다. parity disk란 data 복구를 위해 존재하는 disk로 다른 disk들의 data를 검사할 수 있는 값을 저장하게 된다. 이 때 사용하는 단위는 RAID 3은 bit 단위, RAID 4는 block 단위이다. RAID 3/4는 최대 1개의 disk에 대한 crash를 복원할 수 있는데, 만약 data 저장 disk가 crash됐을 경우 parity disk의 값을 사용해 복원할 수 있고, 만약 parity disk가 crash됐을 경우 다른 disk들의 값을 사용해 parity data를 다시 생성한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-12-18-Disk/11.png&quot; alt=&quot;11.png&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;raid-5&quot;&gt;RAID 5&lt;/h2&gt;

&lt;p&gt;RAID 4에서 parity disk를 별도로 운용하지 않고 여러 disk에 나눠 저장하는 것이다. 이는 parity disk의 I/O 횟수가 일반 data 저장 disk에 비해 많아 parity disk의 수명이 짧아지는 단점을 보완한 것이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-12-18-Disk/12.png&quot; alt=&quot;12.png&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;raid-6&quot;&gt;RAID 6&lt;/h2&gt;

&lt;p&gt;서로 다른 parity 연산을 수행해 2개의 disk를 추가로 사용하는 방식이다. 최대 2개의 disk의 crash까지 복구 가능하다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-12-18-Disk/13.png&quot; alt=&quot;13.png&quot; /&gt;&lt;/p&gt;</content><category term="Operating System" /><summary type="html"></summary></entry><entry><title type="html">[NLP 논문 리뷰] Efficient Estimation Of Word Representations In Vector Space (Word2Vec)</title><link href="https://cpm0722.github.io/Efficient-Estimation-of-Word-Representations-in-Vector-Space/" rel="alternate" type="text/html" title="[NLP 논문 리뷰] Efficient Estimation Of Word Representations In Vector Space (Word2Vec)" /><published>2020-12-11T18:00:00-06:00</published><updated>2020-12-11T18:00:00-06:00</updated><id>https://cpm0722.github.io/Efficient-Estimation-of-Word-Representations-in-Vector-Space</id><content type="html" xml:base="https://cpm0722.github.io/Efficient-Estimation-of-Word-Representations-in-Vector-Space/">&lt;h2 id=&quot;paper-info&quot;&gt;Paper Info&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1301.3781&quot;&gt;Archive Link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1301.3781.pdf&quot;&gt;Paper Link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Submit Date: Jan 16, 2013&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;one-hot encoding 방식은 word를 단순하게 표현하는 방법이다. word 자체가 갖는 정보를 담고 있지 않고 단순하게 index만을 담고 있는데, index 역시 word에 내재된 어떤 정보와도 관련이 없다. 본 논문에서는 word vector에 word 자체가 담고 있는 의미를 확실하게 담아내고자 했다. 구체적으로, 단순하게 유사한 단어들이 vector 공간에서 가까운 거리를 갖는 것에 그치지 않고 syntax, semantic 관점에서의 다양한 similarity를 반영하고자 했다. 동시에 one-hot encoding의 단점인 sparse vector problem을 해결해 dimension이 작으면서도 distribute한 word vector를 생성해냈다. 그 결과 sentence에서 단어가 등장하는 위치가 비슷한 word들의 vector가 가깝게 위치하는 것은 물론(syntax), “King” - “Man” + “Woman” = “Queen” 과 같은 vector 연산까지 정확하게 수행해낼 수 있었다(semantic).&lt;/p&gt;

&lt;h1 id=&quot;model-architectures&quot;&gt;Model Architectures&lt;/h1&gt;

&lt;p&gt;여러 model들을 비교하기 위해서 우선 computational complexity를 정의한다. model의 computational complexity는 #parameters로 정의한다. training complexity는 &lt;script type=&quot;math/tex&quot;&gt;E \times T \times Q&lt;/script&gt;로 정의하는데, &lt;script type=&quot;math/tex&quot;&gt;E&lt;/script&gt;는 #epochs이고, &lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt;는 len(training dataset), &lt;script type=&quot;math/tex&quot;&gt;Q&lt;/script&gt;는 model specific하게 정의된 value이다.&lt;/p&gt;

&lt;h2 id=&quot;feedforward-neural-net-language-model-nnlm&quot;&gt;Feedforward Neural Net Language Model (NNLM)&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;example: “what will the fat cat sit on”, &lt;script type=&quot;math/tex&quot;&gt;N=4&lt;/script&gt;&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/assets/images/2020-12-12-Efficient-Estimation-of-Word-Representations-in-Vector-Space/01.jpg&quot; alt=&quot;01.jpg&quot; /&gt;&lt;/p&gt;

    &lt;p&gt;출처: &lt;a href=&quot;https://wikidocs.net/45609&quot;&gt;https://wikidocs.net/45609&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;N&lt;/script&gt;: input word 개수&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt;: vocabulary size&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;D&lt;/script&gt;: word representation dimenstion&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;H&lt;/script&gt;: hidden layer size&lt;/p&gt;

&lt;p&gt;일반적인 feed forward neural network를 사용한 language model이다.&lt;/p&gt;

&lt;p&gt;이전의 단어들 중 &lt;script type=&quot;math/tex&quot;&gt;N&lt;/script&gt;개의 word에 대한 one-hot encoding을 input으로 받는다. 이 때 &lt;script type=&quot;math/tex&quot;&gt;N&lt;/script&gt;은 n-gram에서의 &lt;script type=&quot;math/tex&quot;&gt;N&lt;/script&gt;과 유사한 의미이며, hyper-parameter이다.&lt;/p&gt;

&lt;p&gt;각각의 input word에 대해 weight matrix &lt;script type=&quot;math/tex&quot;&gt;W_P&lt;/script&gt;(&lt;script type=&quot;math/tex&quot;&gt;V \times D&lt;/script&gt;)를 곱한다. one-hot encoding과 &lt;script type=&quot;math/tex&quot;&gt;W_P&lt;/script&gt;를 곱하는 것은 one-hot encoding에서 1인 index를 사용해 &lt;script type=&quot;math/tex&quot;&gt;W_P&lt;/script&gt;에서 해당 row만 뽑아내는 것과 동일하다. 즉, &lt;script type=&quot;math/tex&quot;&gt;W_P&lt;/script&gt;를 lookup table로 사용하는 것이다.&lt;/p&gt;

&lt;p&gt;이렇게 &lt;script type=&quot;math/tex&quot;&gt;D&lt;/script&gt; 차원의 word vector를 얻어내는데, input에 대한 word vector들을 모두 concatenate해 projection layer &lt;script type=&quot;math/tex&quot;&gt;P&lt;/script&gt;(&lt;script type=&quot;math/tex&quot;&gt;N \times D)&lt;/script&gt;를 만들어낸다. 이는 여러 word에 대한 input을 하나의 matrix로 표현한 &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt;(N \times V&lt;script type=&quot;math/tex&quot;&gt;)와&lt;/script&gt;W_P$$를 곱하는 연산과 동일하다. 이러한 연산의 의미를 직관적으로 이해해보자면 one-hot word vector가 아닌 embedding word vector를 얻는 과정이라고 볼 수 있다.&lt;/p&gt;

&lt;p&gt;이후 projection layer &lt;script type=&quot;math/tex&quot;&gt;P&lt;/script&gt;에 새로운 weight matrix &lt;script type=&quot;math/tex&quot;&gt;W_H&lt;/script&gt;(&lt;script type=&quot;math/tex&quot;&gt;D \times H&lt;/script&gt;)를 곱한 뒤 activation function에 넣어 hidden layer를 생성해낸다. 이는 여러 word embedding vector를 하나의 vector로 축약시키는 과정이다.&lt;/p&gt;

&lt;p&gt;hidden layer에서는 softmax와 cross entropy loss를 사용해 output에 대한 one-hot vector를 만들어낸다.&lt;/p&gt;

&lt;p&gt;전체 과정의 computational complexity는 다음과 같다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Q = N \times D + N \times D \times H + H \times V&lt;/script&gt;

&lt;p&gt;위 수식에서 가장 부하가 큰 연산은 hidden layer에서 output을 만들어내는 연산인 &lt;script type=&quot;math/tex&quot;&gt;H \times V&lt;/script&gt;이지만 hierarchical softmax를 사용하게 되면 &lt;script type=&quot;math/tex&quot;&gt;H \times \log_2{V}&lt;/script&gt;로 연산량을 줄일 수 있다. 이 경우에는 가장 부하가 큰 연산은 projection layer에서 hidden layer를 만들어내는 연산인 &lt;script type=&quot;math/tex&quot;&gt;N \times D \times H&lt;/script&gt;가 된다.&lt;/p&gt;

&lt;h2 id=&quot;recurrent-neural-net-language-model-rnnlm&quot;&gt;Recurrent Neural Net Language Model (RNNLM)&lt;/h2&gt;

&lt;p&gt;RNN을 사용한 language Model이다. RNN의 경우에는 projection layer를 제거한다. 또한 고정된 개수의 word만을 input으로 받는 NNLM과 달리 이전의 모든 word들에 대한 정보를 recurrent하게 hidden layer에 담게 된다.&lt;/p&gt;

&lt;p&gt;전체 과정의 computational complexitiy는 다음과 같다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Q=H \times H + H \times V&lt;/script&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;D&lt;/script&gt;를 &lt;script type=&quot;math/tex&quot;&gt;H&lt;/script&gt;와 동일하게 만들었기 떄문에 위와 같은 수식이 된다. 역시나 동일하게 hierarchical softmax를 사용하면 &lt;script type=&quot;math/tex&quot;&gt;H \times V&lt;/script&gt;를 &lt;script type=&quot;math/tex&quot;&gt;H \times \log_2{V}&lt;/script&gt;로 줄일 수 있다. 이 경우네는 가장 부하가 큰 연산은 &lt;script type=&quot;math/tex&quot;&gt;H \times H&lt;/script&gt;가 된다.&lt;/p&gt;

&lt;h1 id=&quot;new-log-linear-models&quot;&gt;New Log-linear Models&lt;/h1&gt;

&lt;p&gt;computational complexity를 줄이기 위해 2가지 단계를 제안한다.  continuous bag-of-words model(CBOW)을 사용하는 단계와 continuous skip-gram model(Skip-gram)을 사용하는 단계이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-12-12-Efficient-Estimation-of-Word-Representations-in-Vector-Space/02.jpg&quot; alt=&quot;02.jpg&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;continuous-bag-of-words-model&quot;&gt;Continuous Bag-of-Words Model&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-12-12-Efficient-Estimation-of-Word-Representations-in-Vector-Space/03.jpg&quot; alt=&quot;03.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;출처: &lt;a href=&quot;https://wikidocs.net/22660&quot;&gt;https://wikidocs.net/22660&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;NNLM에서 hidden layer를 제거한것이다. CBOW의 목표는 NNLM과 동일하게 word vector 1개를 예측하는 model이다. 다만 NNLM에서는 이전 word들만을 사용해 다음 word를 예측했다면, CBOW에서는 양방향(이전/이후)의 word 총 &lt;script type=&quot;math/tex&quot;&gt;N&lt;/script&gt;개를 사용해 예측을 진행한다.&lt;/p&gt;

&lt;p&gt;CBOW는 NNLM에 비해 computational complexity를 감소시켰는데, NNLM에서의 projection layer는 activation function을 사용하지 않는 linear layer였다. 반면 hidden layer는 activation function을 사용하는 non-linear layer였다. hierarichial softmax를 사용한다는 가정 하에 가장 연산량이 많이 소요되는 layer가 hidden layer였으므로 이를 제거해 전체 연산량을 줄인 것이다. NNLM에서 hidden layer의 존재 의미는 여러 word embedding vector를 하나의 vector로 압축하는 것이었다면, CBOW에서는 이를 non-linear layer를 거치지 않고 단순하게 평균을 내게 된다. 따라서 CBOW의 projection layer는 word embedding vector(&lt;script type=&quot;math/tex&quot;&gt;W_p&lt;/script&gt;에서의 row)들의 평균이다.&lt;/p&gt;

&lt;p&gt;computational complexity는 다음과 같다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Q = N \times D + D \times \log_2{V}&lt;/script&gt;

&lt;h2 id=&quot;continuous-skip-gram-model&quot;&gt;Continuous Skip-gram Model&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-12-12-Efficient-Estimation-of-Word-Representations-in-Vector-Space/04.jpg&quot; alt=&quot;04.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;출처: &lt;a href=&quot;https://wikidocs.net/22660&quot;&gt;https://wikidocs.net/22660&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;CBOW와 유사하지만 input/output이 서로 뒤바뀐경우이다. 현재 word를 통해 이전, 이후의 word를 예측하는 model이다. 여러 word에 대해 prediction을 수행하기 때문에 당연하게도 연산량은 CBOW에 비해 많다. 하지만 skip-gram은 input word vector를 평균내지 않고 온전히 사용하기 때문에 등장 빈도가 낮은 word들에 대해 CBOW 대비 train 효과가 크다는 장점이 있다. CBOW에서는 각 word vector들을 평균내서 사용하기 때문에 등장 빈도가 낮은 word들은 제대로 된 학습을 기대하기 힘들다.&lt;/p&gt;

&lt;p&gt;computational complexity는 다음과 같다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Q = C \times ( D + D \times \log_2{V})&lt;/script&gt;

&lt;p&gt;새로운 variable &lt;script type=&quot;math/tex&quot;&gt;C&lt;/script&gt;가 등장하는데 &lt;script type=&quot;math/tex&quot;&gt;C&lt;/script&gt;는 predict할 word의 개수와 관련된 값이다. 구체적으로, &lt;script type=&quot;math/tex&quot;&gt;C&lt;/script&gt;는 predict할 word와 현재 word의 maximum distance이다. &lt;script type=&quot;math/tex&quot;&gt;[1,C)&lt;/script&gt;의 범위에서 random하게 value &lt;script type=&quot;math/tex&quot;&gt;R&lt;/script&gt;을 뽑고, 현재 word 이전 &lt;script type=&quot;math/tex&quot;&gt;R&lt;/script&gt;개, 이후 &lt;script type=&quot;math/tex&quot;&gt;R&lt;/script&gt;개의 word에 대해서 predict를 수행한다. &lt;script type=&quot;math/tex&quot;&gt;R&lt;/script&gt;의 기댓값은 &lt;script type=&quot;math/tex&quot;&gt;\frac{1+(C-1)}{2}=\frac{C}{2}&lt;/script&gt;이고, predict 수행 횟수는 &lt;script type=&quot;math/tex&quot;&gt;2R=2\times\frac{C}{2}=C&lt;/script&gt;이므로 전체 computational complexity는 1회 수행할 때의 값에 &lt;script type=&quot;math/tex&quot;&gt;C&lt;/script&gt;를 곱한 것이다.&lt;/p&gt;

&lt;h1 id=&quot;results&quot;&gt;Results&lt;/h1&gt;

&lt;p&gt;word embedding의 성능을 측정하던 기존의 방식들은 유사한 word를 찾아내는 것이 대부분이었다. 예를 들면 “France”와 “Italy”가 유사한지를 측정하는 것 등이다. 본 논문에서는 이러한 방식에서 한 발 더 나아가 word 사이의 상관 관계를 뽑아내 다른 word에 적용시키는 방식을 도입했다. 예를 들면 “big”-“biggest”와 유사한 상관 관계를 갖는 word를 “small”에 대해 찾아 “smallest”를 맞추는 것이다. 본 논문에서 제시한 model로 학습한 word vector는 단순한 algebraic operation으로 이러한 task를 해결할 수 있다. X = vector(“biggest”) - vector(“big”) + vector(“small”)을 수행하면 X=vector(“smallest”)가 나올 것이다.&lt;/p&gt;

&lt;p&gt;이와 같은 semantic한 의미까지 제대로 담은 word vector를 활용할 경우 수많은 NLP task에서 뛰어난 성능 향상을 보이게 된다.&lt;/p&gt;

&lt;h2 id=&quot;task-description&quot;&gt;Task Description&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-12-12-Efficient-Estimation-of-Word-Representations-in-Vector-Space/05.jpg&quot; alt=&quot;05.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위와 같은 semantic, syntax 관계들을 목록화시켰다. 각 관계들에 대해 직접 word pair들을 수집하고, 각 word pair를 모두 섞어 random한 pair들을 만들어낸다. 이렇게 생성해낸 dataset으로 test를 수행하는 것이다. 이 때 정답과 완전히 동일한 word를 예측한 경우에만 정답으로 간주한다. 동의어나 유사어에 대해서도 오답 처리를 하기 때문에 사실상 100% accuracy는 불가능한 task이다.&lt;/p&gt;

&lt;h2 id=&quot;maximization-of-accuracy&quot;&gt;Maximization of Accuracy&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-12-12-Efficient-Estimation-of-Word-Representations-in-Vector-Space/06.jpg&quot; alt=&quot;06.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;word vector의 dimension과 #training words를 통해 실험을 진행했다. dimensionality나 #training words 중 하나를 고정시킨 뒤 다른 하나만을 증가시킬 경우 일정한 수준 이상으로 accuracy가 증가하지 않는 현상을 보였다. 기존의 많은 연구에서 단순히 training dataset의 크기만을 늘려가며 성능을 높이려 했지만, 많은 word가 train된다면 이에 대한 정보들을 담을 수 있는 충분한 dimension이 확보되어야 한다는 사실을 알 수 있다.&lt;/p&gt;

&lt;h2 id=&quot;comparison-of-model-architectures&quot;&gt;Comparison of Model Architectures&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-12-12-Efficient-Estimation-of-Word-Representations-in-Vector-Space/07.jpg&quot; alt=&quot;07.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;RNNLM이 가장 낮은 성능을 보였다. CBOW와 Skip-gram은 semantic, syntactic, relatedness에서 모두 NNLM을 능가했다. 특히나 Skip-gram은 Semantic Accuracy에서 다른 model들을 압도했다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-12-12-Efficient-Estimation-of-Word-Representations-in-Vector-Space/08.jpg&quot; alt=&quot;08.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;다른 여러 NNLM과 비교했을 때에도 CBOW와 skip-gram은 훨씬 더 좋은 성능을 보여준다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-12-12-Efficient-Estimation-of-Word-Representations-in-Vector-Space/09.jpg&quot; alt=&quot;09.jpg&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;microsoft-research-sentence-completion-challenge&quot;&gt;Microsoft Research Sentence Completion Challenge&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-12-12-Efficient-Estimation-of-Word-Representations-in-Vector-Space/10.jpg&quot; alt=&quot;10.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Microsoft Research Sentence Completion Challenge는 1040개의 sentence가 주어지는게, 각 sentence는 1개의 word가 빠져 있다. 각 sentence에서 빠진 word를 predict하는 task이다. 이 task에서 skip-gram 단독으로는 기존의 model들에 비해 다소 낮은 수치를 보였지만, RNNLM과 결합한 뒤에는 SOTA를 달성했다.&lt;/p&gt;

&lt;h1 id=&quot;examples-of-the-learned-relationships&quot;&gt;Examples of the Learned Relationships&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-12-12-Efficient-Estimation-of-Word-Representations-in-Vector-Space/11.jpg&quot; alt=&quot;11.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;단어 사이의 상관관계를 분석해 다른 단어에 대해 유사한 관계를 갖는 단어를 예측하는 task에서 본 논문의 model은 60%의 정확도를 달성했다. 더 높은 정확도를 달성하기 위해서는 더 많은 dataset을 사용하고, 또 각 단어 사이의 상관관계 vector를 여러 단어쌍 사이의 subtract vector의 평균으로 만들어내면 될 것이다.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;CBOW와 skip-gram이라는 새로운 word embedding 학습 방법을 제안했다. 기존의 여러 model들에 비해 연산량이 현저히 적고, 간단한 model임에도 매우 높은 성능을 보였다. 또한 word embedding vector의 syntax, semantic 성능을 측정할 수 있는 새로운 dataset을 제시했다.&lt;/p&gt;</content><category term="NLP" /><summary type="html">Paper Info</summary></entry><entry><title type="html">[운영체제] File System</title><link href="https://cpm0722.github.io/File-System/" rel="alternate" type="text/html" title="[운영체제] File System" /><published>2020-11-30T18:00:00-06:00</published><updated>2020-11-30T18:00:00-06:00</updated><id>https://cpm0722.github.io/File-System</id><content type="html" xml:base="https://cpm0722.github.io/File-System/">&lt;hr /&gt;

&lt;p&gt;숭실대학교 컴퓨터학부 홍지만 교수님의 2020-2학기 운영체제 강의를 정리 및 재구성했다.&lt;/p&gt;

&lt;h2 id=&quot;block&quot;&gt;Block&lt;/h2&gt;

&lt;p&gt;OS는 disk를 일정한 크기의 block으로 나누어 저장한다. 대개 block의 크기는 4KB이다. 각 block은 그 목적에 따라 아래의 4가지로 구분지을 수 있다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Super block&lt;/p&gt;

    &lt;p&gt;file system의 global한 정보들을 담는 block으로 하나의 file system에 1개만 존재한다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Allocation structure block&lt;/p&gt;

    &lt;p&gt;bitmap, linked list 등의 방법으로 inode struct와 data에 대해 used/unused 정보가 저장된다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Key meta data block&lt;/p&gt;

    &lt;p&gt;inode struct의 table이 저장된다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;User data block&lt;/p&gt;

    &lt;p&gt;실제 data들이 저장된다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;아래는 2개의 allocation structure block이 각각 inode bitmap, data bitmap으로 운용되고, 최대 80개의 inode struct가 5개의 key meta data block에 저장되는 경우의 전체 disk 구조이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-12-01-File-System/01.png&quot; alt=&quot;01.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-12-01-File-System/02.png&quot; alt=&quot;02.png&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;inode-struct&quot;&gt;inode struct&lt;/h2&gt;

&lt;p&gt;inode struct에는 file에 대한 meta data가 저장된다. 각 file마다 하나의 inode struct가 부여된다. file의 size, mode, permission, 소유자, 각종 시각 등이 저장된다. unix의 ls 명령어로 출력되는 정보들,  unistd.h의 stat() 함수에서 얻을 수 있는 struct stat 에 담긴 정보들은 모두 inode struct에서 가져온 것이다.&lt;/p&gt;

&lt;p&gt;inode struct에서 가장 중요한 정보는 실제 data가 저장된 user data block의 pointer이다. file의 크기가 block의 size보다 클 경우에는 여러 block을 사용해야 하기 때문에 data block을 가리키는 여러 pointer 변수들이 inode struct에 존재하게 된다.&lt;/p&gt;

&lt;p&gt;inode struct에서 pointer로 data block을 가리키는 방법에는 총 2가지가 있는데, direct pointer로 data block을 직접 가리키는 방법, indirect pointer로 disk block을 가리키는 pointer들이 저장된 block을 가리키는 방법이다. direct pointer만을 사용할 경우 한 file이 가질 수 있는 최대 size에 제약이 생기게 된다. 예를 들어 block size가 4KB이고, inode struct가 direct pointer 10개를 운용한다고 할 경우에는 한 file의 최대 size는 10&lt;em&gt;4KB=40KB가 된다. 반면 single indirect pointer 10개를 운용한다 할 경우 각 indirect pointer가 최대 저장할 수 있는 pointer의 개수는 block size / sizeof(pointer)이므로 4KB/4B = 1K이다. 각 pointer는 data block을 가리킬 수 있으므로 10&lt;/em&gt;1K*4KB = 40MB가 된다.&lt;/p&gt;

&lt;h2 id=&quot;directory&quot;&gt;directory&lt;/h2&gt;

&lt;p&gt;directory는 file의 한 종류이다. 그렇다면 directory의 inode struct는 어떻게 구성되어 있을까? inode struct의 일반적인 구성과 동일하다. directory의 data block에서의 data가 다른데, directory 하위 항목들에 대한 linked list를 저장된다. linked list의 각 node는 inode number와 name을 구성 요소로 갖는다. 이 때 inode struct pointer를 직접 저장하지 않고 단순 index 번호만 저장함으로써 공간을 절약한다. directory마다 단순 선형 linked list를 운용하게 될 경우 깊은 계층 구조를 갖는 directory에서 성능이 많이 하락하기 때문에 B-tree와 같은 자료구조를 사용해 성능을 향상시키기도 한다.&lt;/p&gt;

&lt;h2 id=&quot;file-descriptor--inode&quot;&gt;file descriptor &amp;amp; inode&lt;/h2&gt;

&lt;p&gt;각 process는 고유한 file descriptor table을 운용한다. 그 중 0번은 stdin, 1번은 stdout, 2번은 stderr file로 미리 예약되어 있다. file descriptor란 해당 process가 어떤 file을 open했을 때 return되는 값인데, 한 process가 한 file을 여러 번 open할 수도 있다. 이 때마다 file descriptor는 새로 할당되게 된다. 즉, 같은 file에 대해 다른 file descriptor를 동시에 가질 수도 있는 것이다. 각 file descriptor는 open file table을 가리킨다. open file table의 각 항목은 status(read/write 등), offset, inode sturct pointer 등을 저장한다. open file table에서 inode struct를 가리키고, inode struct의 block pointer가 실제로 data가 저장된 data block을 가리키는 것이다.&lt;/p&gt;

&lt;p&gt;정리하자면, file descriptor table은 process마다 별개로 부여되는 local 구조이고, open file table, inode table은 전체 file system에서 하나를 운용하는 global 구조이다. 각 항목이 가리키는 방향은 file descriptor table → open file table → inode table → data block이다.&lt;/p&gt;

&lt;h2 id=&quot;reading-a-file-from-disk&quot;&gt;Reading a File from Disk&lt;/h2&gt;

&lt;p&gt;disk에서 실제 file을 읽어들이는 과정을 따라가보자. super block만이 memory에 올라와 있고, bitmap이 담긴 allocation structure block은 disk에 남아있는 상태라고 가정해보자. 다음의 순서를 따른다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-12-01-File-System/03.png&quot; alt=&quot;03.png&quot; /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;root directory (“ / “) read
    &lt;ol&gt;
      &lt;li&gt;root의 inode struct read&lt;/li&gt;
      &lt;li&gt;root의 block pointer 획득&lt;/li&gt;
      &lt;li&gt;root의 data block read&lt;/li&gt;
      &lt;li&gt;root의 하위 항목들에 대한 linked list 획득&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;foo directory (“ /foo “) read
    &lt;ol&gt;
      &lt;li&gt;root의 하위 항목들에 대한 linked list에서 이름이 “foo”인 항목의 inode number 획득&lt;/li&gt;
      &lt;li&gt;inode number를 통해 inode table에서의 주소 계산&lt;/li&gt;
      &lt;li&gt;foo의 inode sturct read&lt;/li&gt;
      &lt;li&gt;foo의 block pointer 획득&lt;/li&gt;
      &lt;li&gt;foo의 하위 항목들에 대한 linked list 획득&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;bar file (“ /foo/bar “) read
    &lt;ol&gt;
      &lt;li&gt;bar의 하위 항목들에 대한 linked list에서 이름이 “bar”인 항목의 inode number 획득&lt;/li&gt;
      &lt;li&gt;inode number를 통해 inode table에서의 주소 계산&lt;/li&gt;
      &lt;li&gt;bar의 inode struct read&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;bar[0] read
    &lt;ol&gt;
      &lt;li&gt;bar의 inode struct에서 첫번째 data block pointer 획득&lt;/li&gt;
      &lt;li&gt;data block read&lt;/li&gt;
      &lt;li&gt;bar inode struct write(access time 등 갱신 위함)&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;bar[1] read
    &lt;ol&gt;
      &lt;li&gt;bar의 inode struct에서 두번째 data block pointer 획득&lt;/li&gt;
      &lt;li&gt;data block read&lt;/li&gt;
      &lt;li&gt;bar inode struct write(access time 등 갱신 위함)&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;bar[2] read
    &lt;ol&gt;
      &lt;li&gt;bar의 inode struct에서 세번째 data block pointer 획득&lt;/li&gt;
      &lt;li&gt;data block read&lt;/li&gt;
      &lt;li&gt;bar inode struct write(access time 등 갱신 위함)&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;creating--writing-a-file-from-disk&quot;&gt;Creating &amp;amp; Writing a File from Disk&lt;/h2&gt;

&lt;p&gt;disk에서 실제 file을 생성하고 write하는 과정을 따라가보자. 역시나 super block만이 memory에 올라와 있고, bitmap이 담긴 allocation structure block은 disk에 남아있는 상태라고 가정한다. 다음의 순서를 따른다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-12-01-File-System/04.png&quot; alt=&quot;04.png&quot; /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;root directory (“ / “) read
    &lt;ol&gt;
      &lt;li&gt;root의 inode struct read&lt;/li&gt;
      &lt;li&gt;root의 block pointer 획득&lt;/li&gt;
      &lt;li&gt;root의 data block read&lt;/li&gt;
      &lt;li&gt;root의 하위 항목들에 대한 linked list 획득&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;foo directory (“ /foo “) read
    &lt;ol&gt;
      &lt;li&gt;root의 하위 항목들에 대한 linked list에서 이름이 “foo”인 항목의 inode number 획득&lt;/li&gt;
      &lt;li&gt;inode number를 통해 inode table에서의 주소 계산&lt;/li&gt;
      &lt;li&gt;foo의 inode sturct read&lt;/li&gt;
      &lt;li&gt;foo의 block pointer 획득&lt;/li&gt;
      &lt;li&gt;foo의 하위 항목들에 대한 linked list 획득&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;bar file (“ /foo/bar “) create
    &lt;ol&gt;
      &lt;li&gt;현재 사용 중인 inode number을 확인하기 위해 inode bitmap read&lt;/li&gt;
      &lt;li&gt;미사용 중인 inode number 선택 후 사용 중으로 변경하기 위해 inode bitmap write&lt;/li&gt;
      &lt;li&gt;foo의 하위 항목들에 대한 linked list에 획득한 inode number와 “bar” 명칭으로 항목 추가하기 위해 bar data block write&lt;/li&gt;
      &lt;li&gt;bar inode struct read (inode struct 초기화 위함)&lt;/li&gt;
      &lt;li&gt;bar inode struct write (inode struct 초기화 위함)&lt;/li&gt;
      &lt;li&gt;foo inode struct write (access time 등 갱신 위함)&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;bar file (“ /foo/bar “) write
    &lt;ol&gt;
      &lt;li&gt;write 가능한 여유 있는 data block 존재 여부 확인하기 위해 bar inode struct read&lt;/li&gt;
      &lt;li&gt;현재 사용 중인 data block number 확인하기 위해 data bitmap read&lt;/li&gt;
      &lt;li&gt;미사용 중인 data block number 선택 후 사용 중으로 변경하기 위해 data bitmap write&lt;/li&gt;
      &lt;li&gt;bar data block write&lt;/li&gt;
      &lt;li&gt;bar inode write (access time 등 갱신 위함)&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;ffs-fast-file-system&quot;&gt;FFS (Fast File System)&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-12-01-File-System/05.png&quot; alt=&quot;05.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;fast file system은 기존의 unix file system에서 성능을 더 향상시킨 file system이다. 기존의 file system은 disk 전체에 super block, inode bitmap, data bitmap이 disk에 오직 1개만 존재했다. 또한 inode 역시 disk의 한 영역에 몰려서 저장되어 있어 실제 data block들과의 disk상에서의 물리적 거리가 멀 수 밖에 없었다. FFS는 이러한 단점을 해결하고자 disk 전체를 여러 group으로 나누고, 각 group마다 super block, bitmaps, inodes, data block들을 부여한다. 이를 통해 inode에서 참조하는 data block과 실제 inode가 저장된 block 사이의 물리적 거리가 줄어들어 seek time이 감소한다.&lt;/p&gt;

&lt;p&gt;또한 FFS는 directory 구조 역시 개선했는데, 기존의 file system은 단순한 계층 구조였기에 하위 file들의 data block이 부모 directory의 data block과 멀리 떨어져 있을 가능성이 농후했다. FFS는 동일한 directory에 있는 file에 접근할 확률이 40%나 된다는 통계에 기반해 (Name-based Locality) directory와 그 하위 file들을 disk 내에서 같은 group 안에 배치하도록 했다. 이를 통해 seek time을 감소시킬 수 있었다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-12-01-File-System/06.png&quot; alt=&quot;06.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;마지막으로, FFS는 disk layout에 대해서 최적화를 수행했다. 초기 hard disk는 HW의 성능이 떨어져 rotation 속도가 많이 느렸다. 하지만 점차 HW가 발전함에 따라 rotation 속도가 비약적으로 상승했고, 연속된 sector를 읽어들이기에는 이미 head가 다음 sector를 지나쳐버리는 증상이 발생하게 되었다. 이를 해결하기 위해 FFS는 다음 sector를 연속적으로 배치하지 않고, 1칸 뒤에 배치하는 식으로 sector 배치를 변경했다.&lt;/p&gt;

&lt;p&gt;이 외에도 FFS는 block의 크기를 줄여 내부 단편화 현상을 감소시키고, symbolic link를 도입하는 등의 여러 변화를 채택했다.&lt;/p&gt;

&lt;h1 id=&quot;crash-consistency&quot;&gt;Crash Consistency&lt;/h1&gt;

&lt;h2 id=&quot;disk-crash-scenario&quot;&gt;Disk Crash Scenario&lt;/h2&gt;

&lt;p&gt;disk I/O 과정에서 crash가 발생하는 경우에 대해서 살펴보자. 이미 존재하는 file에 대해 새로운 data를 append하는 경우에는 data bitmap, inodes, data block을 갱신해야 한다. 위의 3가지 block들에 대한 갱신은 atomic하게 이루어져야 한다. 이 과정에서 발생할 수 있는 crash scenario는 다음의 6가지이다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;data block만 정상 갱신, data bitmap, inodes는 crash되는 경우&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/assets/images/2020-12-01-File-System/07.png&quot; alt=&quot;07.png&quot; /&gt;&lt;/p&gt;

    &lt;p&gt;data bitmap과 inodes가 모두 crash되었기 때문에 bitmap과 inodes 사이의 불일치(inconsistent)는 없다. 따라서 consistent한 상황이다. 대신 data block은 갱신이 되었는데, 해당 block은 data bitmap에서도 unused로 표시가 되어 있고, inodes에도 data block 포인터가 연결이 되어 있지 않기 떄문에 garbage data이다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;inodes만 정상 갱신, data bitmap, data block은 crash되는 경우&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/assets/images/2020-12-01-File-System/08.png&quot; alt=&quot;08.png&quot; /&gt;&lt;/p&gt;

    &lt;p&gt;data bitmap은 crash, inodes는 정상 갱신되었기 때문에 inconsistent한 상황이다. inodes는 이미 data block을 가리키는데 해당 data block에는 data가 쓰여 있지 않고, data bitmap에서도 해당 data block은 unused로 표시가 되어 있다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;data bitmap만 정상 갱신, inodes, data block은 crash되는 경우&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/assets/images/2020-12-01-File-System/09.png&quot; alt=&quot;09.png&quot; /&gt;&lt;/p&gt;

    &lt;p&gt;data bitmap은 정상 갱신되었지만, inodes는 crash되었기 때문에 inconsistent한 상황이다. data block에는 write가 되지 않았고, inodes에서도 해당 data block을 가리키지 않는데 datat bitmap에서는 used로 표시가 되어있는 경우이다. 이후 해당 data block은 사용되지 못하고 낭비될 것이다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;data bitmap, inodes는 정상 갱신, data block만 crash되는 경우&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/assets/images/2020-12-01-File-System/10.png&quot; alt=&quot;10.png&quot; /&gt;&lt;/p&gt;

    &lt;p&gt;data bitmap과 inodes가 모두 정상갱신 되었기 때문에 consistent한 상황이다. data block에 write만 되지 않은 것이기 때문에 garbage data가 저장된 상태이다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;inodes, data block은 정상 갱신, data bitmap만 crash되는 경우&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/assets/images/2020-12-01-File-System/11.png&quot; alt=&quot;11.png&quot; /&gt;&lt;/p&gt;

    &lt;p&gt;data bitmap은 crash되고, inodes는 정상 갱신되었기 때문에 inconsistent한 상황이다. 이 경우 inode에서 data block을 가리키고, 해당 data block에는 정상적인 data가 쓰여져 있음에도 data bitmap에서 unused로 표시가 되어 있기 때문에 언제든 덮어씌워질 수 있고, 다른 inode가 동일한 data block을 가리킬 수도 있다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;data bitmap, data block은 정상 갱신, inodes만 crash되는 경우&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/assets/images/2020-12-01-File-System/12.png&quot; alt=&quot;12.png&quot; /&gt;&lt;/p&gt;

    &lt;p&gt;data bitmap은 정상 갱신, inodes는 crash되었기 때문에 inconsistent한 상황이다. 이 경우 data block에도 정상적인 data가 쓰여져 있고 data bitmap에서도 used로 표시가 되었지만 inodes에서 해당 data block을 가리키지 않기 때문에 해당 data block은 어떤 file에도 연결되지 못한다. 이를 orphan data block이라고 한다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;fsck&quot;&gt;FSCK&lt;/h2&gt;

&lt;p&gt;FSCK는 unix에서 file system에서의 crash inconsistency를 찾아내 해결하는 도구이다. 초기의 file system은 crash inconsistency를 발견하더라도 무시했다가 rebooting 과정에서 이를 해결했다. FSCK는 rebooting 없이도 이를 해결하고자 개발되었다. FSCK 동작 중에는 file system이 어떠한 다른 동작도 수행하지 않는다고 가정한다.&lt;/p&gt;

&lt;p&gt;FSCK는 inodes 정보를 기반으로 bitmap을 재갱신한다. 개념적으로 간단하고 별도의 write overhead가 없다는 장점이 있지만, 과도하게 많은 연산을 수행하고, consistent한 경우에 대해서는 해결이 불가능하다는 단점이 있다.&lt;/p&gt;

&lt;h2 id=&quot;journaling-wal-write-ahead-logging&quot;&gt;Journaling (WAL: Write-Ahead Logging)&lt;/h2&gt;

&lt;p&gt;Journaling은 disk I/O, 특히 write 연산 시에 log를 기록해 저장했다가 이를 crash inconsistency를 해결하는데 사용하는 방식이다. write 요청이 올 경우 disk는 이를 즉시 갱신하지 않고 write 작업 중 수행할 연산들에 대한  log를 미리 작성한다. 이후 만약 write 도중 disk crash가 발생할 경우, 해당 log를 확인해 다시 write를 수행한다. 이러한 log를 저장하는 용도로 disk에 journal block을 새로 추가한다. log의 저장 단위는 transaction이다.&lt;/p&gt;</content><category term="Operating System" /><summary type="html"></summary></entry><entry><title type="html">[운영체제] Semaphore</title><link href="https://cpm0722.github.io/Semaphore/" rel="alternate" type="text/html" title="[운영체제] Semaphore" /><published>2020-11-23T18:00:00-06:00</published><updated>2020-11-23T18:00:00-06:00</updated><id>https://cpm0722.github.io/Semaphore</id><content type="html" xml:base="https://cpm0722.github.io/Semaphore/">&lt;hr /&gt;

&lt;p&gt;숭실대학교 컴퓨터학부 홍지만 교수님의 2020-2학기 운영체제 강의를 정리 및 재구성했다.&lt;/p&gt;

&lt;h1 id=&quot;semaphore&quot;&gt;Semaphore&lt;/h1&gt;

&lt;p&gt;semaphore는 다수의 thread 사이의 병행성 유지를 위해 OS 단위에서 제공되는 기법이다. 기본적인 작동 원리는 특정 thread가 특정 signal을 수신할 때까지 정해진 위치에서 wait하도록 강제하는 것이다.&lt;/p&gt;

&lt;h2 id=&quot;counting-semaphore&quot;&gt;counting semaphore&lt;/h2&gt;

&lt;p&gt;counting semahpore는 정수값을 갖는 counting 변수와 3가지 연산으로 구성된다. 범용 semaphore라고도 불리운다. 3가지 연산은 아래와 같다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;semInit(): semaphore 변수를 음이 아닌 값(대개 1)으로 초기화를 수행한다.&lt;/li&gt;
  &lt;li&gt;semWait(): semaphore 변수 값을 감소시킨다. 만약 값이 음수가 되면 semWait()을 호출한 thread는 block된다. 그 외에는 해당 thread는 정상적으로 계속 수행한다.&lt;/li&gt;
  &lt;li&gt;semSignal(): semaphore 변수 값을 증가시킨다. 만약 값이 양수가 아니면 semWait()에 의해 block된 thread 중 하나를 깨운다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;기본적인 pseudo code는 다음과 같다.&lt;/p&gt;

&lt;div class=&quot;language-c highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;typedef&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;struct&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
	&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;count&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;queue&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;waitQueue&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;semaphore&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;semInit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;semaphore&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;count&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;semWait&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;semaphore&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;count&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;count&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
		&lt;span class=&quot;c1&quot;&gt;//요청한 thread를 s.waitQueue에 push
&lt;/span&gt;		&lt;span class=&quot;c1&quot;&gt;//요청한 thread의 상태를 block으로 변경
&lt;/span&gt;	&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;semSignal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;semaphore&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;count&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;count&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
		&lt;span class=&quot;c1&quot;&gt;//s.waitQueue에서 thread 1개를 pop
&lt;/span&gt;		&lt;span class=&quot;c1&quot;&gt;//pop한 thread의 상태를 runnable로 변경 후 OS의 readyQueue에 push
&lt;/span&gt;	&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;count 변수는 값이 음수인 경우에 그 절대값은 대기 queue의 길이를 의미한다.&lt;/p&gt;

&lt;h2 id=&quot;binary-semaphore-mutex&quot;&gt;binary semaphore (mutex)&lt;/h2&gt;

&lt;p&gt;mutex는 semaphore 변수가 0 또는 1의 binary 값만 갖는 semaphore를 뜻한다. 동일하게 3가지 연산으로 구성된다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;semInitB(): semaphore 변수를 0 또는 1로 초기화한다.&lt;/li&gt;
  &lt;li&gt;semWaitB(): semaphore 변수 값을 확인해 0일 경우 semWaitB()를 호출한 thread는 block되고, 1일 경우 값을 0으로 변경시킨 뒤 thread는 계속 수행한다.&lt;/li&gt;
  &lt;li&gt;semSignalB(): block된 thread가 있는지 확인한 후, 만약 있을 경우 해당 thread들 중 하나를 깨우고, 없을 경우 semaphore 변수 값을 1로 설정한다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;pseudo code는 다음과 같다.&lt;/p&gt;

&lt;div class=&quot;language-c highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;typedef&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;struct&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
	&lt;span class=&quot;kt&quot;&gt;_Bool&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;queue&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;waitQueue&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;binary_semaphore&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;semInitB&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;binary_semaphore&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;semWaitB&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;binary_semaphore&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
		&lt;span class=&quot;c1&quot;&gt;//요청한 thread를 s.waitQueue에 push
&lt;/span&gt;		&lt;span class=&quot;c1&quot;&gt;//요청한 thread의 상태를 block으로 변경
&lt;/span&gt;	&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;semSignalB&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;binary_semaphore&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;waitQueue&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;empty&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
		&lt;span class=&quot;c1&quot;&gt;//s.waitQueue에서 thread 1개를 pop
&lt;/span&gt;		&lt;span class=&quot;c1&quot;&gt;//pop한 thread의 상태를 runnable로 변경 후 OS의 readyQueue에 push
&lt;/span&gt;	&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;binary semahpore는 일반 범용 semaphore에 비해 구현이 간단하다는 장점이 있다. 둘 모두 waitQueue를 운용한다는 점에서 공통적이다.&lt;/p&gt;

&lt;h2 id=&quot;strong--weak-semaphore&quot;&gt;strong / weak semaphore&lt;/h2&gt;

&lt;p&gt;queue에서 FIFO 방식을 사용하는 semaphore를 강성(strong) semaphore라고 하고, 특별히 queue의 순서를 명시하지 않은 semaphore를 약성(weak) semaphore라고 한다. 하지만 실제로 대부분의 OS에서는 강성 semaphore를 사용한다. starvation이 없고, 직관적이며 구현하기도 용이하기 때문이다.&lt;/p&gt;

&lt;p&gt;아래는 강성 semaphore의 예시이다. D thread는 생산자,와 A, B, C thread는 소비자인 문제이다. 초기 semaphore 변수 s가 값이 1로 시작된다.  s의 값이 음수일 때에는 그 절댓값이 기다리는 thread의 개수(waitQueue 내 thread의 개수)를 뜻하고, s의 값이 음수가 아닐 때에는 생산자가 생성한 자원의 여분 개수를 뜻한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-11-24-Semaphore/01.png&quot; alt=&quot;01.png&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;mutual-exclusion-problem&quot;&gt;mutual exclusion problem&lt;/h2&gt;

&lt;p&gt;범용 semaphore를 사용해 상호 배제 문제를 해결해보자. 상호 배제 문제란 동일한 자원에 접근하려는 n개의 thread의 병행성을 처리하는 문제이다. semInit()에서 count 변수를 0이 아닌 변수로 초기화한다. count 변수의 초기값은 자원의 개수를 의미한다. 따라서 count 변수는 0으로 초기화 되어서는 안된다. 모든 thread가 무한히 block될 것이기 때문이다. 각 thread에서 critical section(임계 영역)을 생성하게 되는데, critical section이란 한 번에 최대 1개의 thread만이 접근할 수 있는 영역이다. semWait()~semSignal() 사이의 영역이 된다. pseudo code는 다음과 같다.&lt;/p&gt;

&lt;div class=&quot;language-c highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;semaphore&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;thread_execute&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;thread_no&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;semWait&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
		&lt;span class=&quot;c1&quot;&gt;//임계 영역
&lt;/span&gt;		&lt;span class=&quot;n&quot;&gt;semSignal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
		&lt;span class=&quot;c1&quot;&gt;//임계 영역 이후
&lt;/span&gt;	&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;semInit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_of_threads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;thread_start&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;thread가 1, 2, 3 순서대로 실행된다고 가정했을 때 각 thread는 아래와 같은 형태로 실행된다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-11-24-Semaphore/02.png&quot; alt=&quot;02.png&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;producer-consumer-problem&quot;&gt;producer-consumer problem&lt;/h2&gt;

&lt;p&gt;생산자-소비자 문제를 mutex를 이용해 해결해보자. 생산자-소비자 문제는 다수의 생산자 thread가 각자 자원을 생성해 공용 buffer에 저장하고, 다수의 소비자 thread가 공용 buffer에서 자원을 1개씩 소비하는 상황의 병행성을 처리하는 문제이다. 공용 buffer에는 한 번에 1개의 thread만 접근 가능하다(critical section)는 조건이 있다. 우선 공용 buffer가 무한한 크기를 갖는다고 가정한다. 이 때 in과 out이라는 pointer 변수를 사용하는데, in은 다음에 생산자가 생성한 자원이 저장될 buffer에서의 위치이며, out은 다음에 소비자가 소비할 자원이 저장된 buffer에서의 위치이다. 따라서 out&amp;lt;in인 경우에만 소비자가 소비할 자원이 있는 것이다. 전체 pseudo code는 다음과 같다.&lt;/p&gt;

&lt;div class=&quot;language-c highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;                     &lt;span class=&quot;c1&quot;&gt;//in-out의 값
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;binary_semaphore&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;        &lt;span class=&quot;c1&quot;&gt;//buffer의 접근을 제어하는 mutex
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;binary_semaphore&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;delay&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;//buffer가 비었는지를 확인해 소비를 제어하는 mutex
&lt;/span&gt;
&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;producer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;produce&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;       &lt;span class=&quot;c1&quot;&gt;//자원 생산
&lt;/span&gt;		&lt;span class=&quot;n&quot;&gt;semWaitB&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
		&lt;span class=&quot;cm&quot;&gt;/*
			                     critical section start
		*/&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;           &lt;span class=&quot;c1&quot;&gt;//buffer에 push
&lt;/span&gt;		&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;            &lt;span class=&quot;c1&quot;&gt;//buffer.empty()==false가 된 상황
&lt;/span&gt;			&lt;span class=&quot;n&quot;&gt;semSignalB&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;delay&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;   &lt;span class=&quot;c1&quot;&gt;//consumer 중 1개 block 해제
&lt;/span&gt;		&lt;span class=&quot;cm&quot;&gt;/*
		                       critical section end
		*/&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;semSignalB&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
		&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
	&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;consumer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
	&lt;span class=&quot;c1&quot;&gt;//consumer가 producer보다 먼저 실행되는 상황(buffer.empty()==true)를 막기 위해 block
&lt;/span&gt;	&lt;span class=&quot;n&quot;&gt;semWaitB&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;delay&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;semWaitB&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
		&lt;span class=&quot;cm&quot;&gt;/*
		                     critical section start
		*/&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;take&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;        &lt;span class=&quot;c1&quot;&gt;//buffer에서 pop
&lt;/span&gt;		&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
		&lt;span class=&quot;cm&quot;&gt;/*
			                	critical section end
		*/&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;semSignalB&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;consume&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;       &lt;span class=&quot;c1&quot;&gt;//자원 소비
&lt;/span&gt;		&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;          &lt;span class=&quot;c1&quot;&gt;//buffer.empty()==true가 된 상황
&lt;/span&gt;			&lt;span class=&quot;n&quot;&gt;semWaitB&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;delay&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;//thread block
&lt;/span&gt;	&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;semInitB&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;semInitB&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;delay&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_of_producers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;thread_start&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_of_consumers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;thread_start&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;위의 code는 producer와 consumer 내의 while문이 매 번 atomic하게 전체가 함께 실행되면 정상적으로 작동할 것이다. 하지만 while loop가 1번 도는 사이에 scheduling이 발생하지 않을 것이라는 보장이 없다. 만약 consumer에서 semSignalB(s)와 if(n==0) 사이에서 scheduling이 발생해 producer가 실행된다면 n은 0에서 1로 변경될 것이고, 그렇다면 다시 scheduling이 되어 consumer로 돌아왔을 때 if(n==0)을 만족하지 못하므로 semWaitB(delay)가 실행되지 않을 것이다. 이는 소비자가 한 개의 thread라면 큰 문제가 되지 않지만, 다수의 thread일 경우에는 문제 상황이 된다. empty임에도 여러 소비자 thread 모두 block되지 않을 수 있기 때문이다.&lt;/p&gt;

&lt;h3 id=&quot;solution-1-보조-변수-사용&quot;&gt;solution 1: 보조 변수 사용&lt;/h3&gt;

&lt;div class=&quot;language-c highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;                     &lt;span class=&quot;c1&quot;&gt;//in-out의 값
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;binary_semaphore&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;        &lt;span class=&quot;c1&quot;&gt;//buffer의 접근을 제어하는 mutex
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;binary_semaphore&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;delay&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;//buffer가 비었는지를 확인해 소비를 제어하는 mutex
&lt;/span&gt;
&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;producer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;produce&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;       &lt;span class=&quot;c1&quot;&gt;//자원 생산
&lt;/span&gt;		&lt;span class=&quot;n&quot;&gt;semWaitB&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
		&lt;span class=&quot;cm&quot;&gt;/*
			                     critical section start
		*/&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;           &lt;span class=&quot;c1&quot;&gt;//buffer에 push
&lt;/span&gt;		&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;            &lt;span class=&quot;c1&quot;&gt;//buffer.empty()==false가 된 상황
&lt;/span&gt;			&lt;span class=&quot;n&quot;&gt;semSignalB&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;delay&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;   &lt;span class=&quot;c1&quot;&gt;//consumer 중 1개 block 해제
&lt;/span&gt;		&lt;span class=&quot;cm&quot;&gt;/*
		                       critical section end
		*/&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;semSignalB&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
		&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
	&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;consumer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
	&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;
	&lt;span class=&quot;c1&quot;&gt;//consumer가 producer보다 먼저 실행되는 상황(buffer.empty()==true)를 막기 위해 block
&lt;/span&gt;	&lt;span class=&quot;n&quot;&gt;semWaitB&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;delay&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;semWaitB&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
		&lt;span class=&quot;cm&quot;&gt;/*
		                     critical section start
		*/&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;take&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;        &lt;span class=&quot;c1&quot;&gt;//buffer에서 pop
&lt;/span&gt;		&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
		&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;
		&lt;span class=&quot;cm&quot;&gt;/*
			                	critical section end
		*/&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;semSignalB&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;consume&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;       &lt;span class=&quot;c1&quot;&gt;//자원 소비
&lt;/span&gt;		&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;          &lt;span class=&quot;c1&quot;&gt;//buffer.empty()==true가 된 상황
&lt;/span&gt;			&lt;span class=&quot;n&quot;&gt;semWaitB&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;delay&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;//thread block
&lt;/span&gt;	&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;semInitB&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;semInitB&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;delay&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_of_producers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;thread_start&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_of_consumers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;thread_start&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;n의 값이 변경되는 것을 막기 위해 critical section 내에서 보조 변수 m에 현재 n의 값을 임시로 저장한다. 이후 critical section 밖의 if문에서 n 대신 m이 0인지를 확인하게 된다.&lt;/p&gt;

&lt;h3 id=&quot;solution-2-범용-semaphore-사용&quot;&gt;solution 2: 범용 semaphore 사용&lt;/h3&gt;

&lt;p&gt;binary semaphore가 아닌 범용 semaphore를 사용하면 애초에 위의 문제 상황이 발생하지 않는다.&lt;/p&gt;

&lt;div class=&quot;language-c highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;semaphore&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;               &lt;span class=&quot;c1&quot;&gt;//buffer의 접근을 제어하는 semaphore
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;semaphore&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;               &lt;span class=&quot;c1&quot;&gt;//buffer에 들어있는 자원의 개수를 제어하는 semaphore
&lt;/span&gt;
&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;producer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;produce&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;       &lt;span class=&quot;c1&quot;&gt;//자원 생산
&lt;/span&gt;		&lt;span class=&quot;n&quot;&gt;semWait&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
		&lt;span class=&quot;cm&quot;&gt;/*
			                     critical section start
		*/&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;           &lt;span class=&quot;c1&quot;&gt;//buffer에 push
&lt;/span&gt;		&lt;span class=&quot;n&quot;&gt;semSignal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;          &lt;span class=&quot;c1&quot;&gt;//consumer 중 1개 block 해제
&lt;/span&gt;		&lt;span class=&quot;cm&quot;&gt;/*
		                       critical section end
		*/&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;semSignal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
		&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
	&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;consumer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;semWait&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;          &lt;span class=&quot;c1&quot;&gt;//buffer.empty()==true일 때 실행되는 것을 방지하기 위해 block
&lt;/span&gt;		&lt;span class=&quot;n&quot;&gt;semWait&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
		&lt;span class=&quot;cm&quot;&gt;/*
		                     critical section start
		*/&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;take&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;        &lt;span class=&quot;c1&quot;&gt;//buffer에서 pop
&lt;/span&gt;		&lt;span class=&quot;cm&quot;&gt;/*
			                	critical section end
		*/&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;semSignal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;consume&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;       &lt;span class=&quot;c1&quot;&gt;//자원 소비
&lt;/span&gt;	&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;semInit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;semInit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_of_producers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;thread_start&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_of_consumers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;thread_start&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;공용 변수 n과 delay를 통합해 하나의 범용 semaphore n으로 운용한다. n의 값에 따라 delay를 wait 또는 signal하지 않고 무조건적으로 producer에서는 semSignal(n), consumer에서는 semWait(n)하게 된다. n은 buffer에 들어가 있는 자원의 개수(음수일 경우 그 절댓값은 waitQueue에 들어있는 thread의 수)임과 동시에 thread의 실행 순서를 제어하는 역할을 하게 된다. consumer thread들은 매 번 실행될 때마다 semWait(n)을 하게 된다. 따라서 producer에서 semSignal(n)과 semSignal(s)가 서로 순서가 바뀌어 semSignal(n)이 critical section 밖에서 수행된다고 하더라도 동일하게 실행된다. 왜냐하면 어차피 consumer들은 semWait(n)을 통해 block된 상태이기에 semSignal(n)이 호출되어야 수행될 수 있기 때문이다.&lt;/p&gt;

&lt;h3 id=&quot;유한-buffer-사용&quot;&gt;유한 buffer 사용&lt;/h3&gt;

&lt;p&gt;위의 모든 solution은 무한한 buffer를 사용한다는 가정 하에서 이루어졌다. 하지만 실제로 무한한 buffer는 존재하지 않으므로 유한한 buffer를 사용하게 된다. 대개 circular queue를 사용하게 된다. pseudo code는 다음과 같다.&lt;/p&gt;

&lt;div class=&quot;language-c highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;n&quot;&gt;semaphore&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;               &lt;span class=&quot;c1&quot;&gt;//buffer의 접근을 제어하는 semaphore
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;semaphore&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;               &lt;span class=&quot;c1&quot;&gt;//buffer에 들어있는 자원의 개수를 제어하는 semaphore
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;semaphore&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;               &lt;span class=&quot;c1&quot;&gt;//유한 buffer를 관리하는 semaphore
&lt;/span&gt;
&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;producer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
	&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;produce&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;       &lt;span class=&quot;c1&quot;&gt;//자원 생산
&lt;/span&gt;		&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;semWait&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;            &lt;span class=&quot;c1&quot;&gt;//buffer.full()==true일 경우 block
&lt;/span&gt;		&lt;span class=&quot;n&quot;&gt;semWait&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
		&lt;span class=&quot;cm&quot;&gt;/*
			                     critical section start
		*/&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;           &lt;span class=&quot;c1&quot;&gt;//buffer에 push
&lt;/span&gt;		&lt;span class=&quot;n&quot;&gt;semSignal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;          &lt;span class=&quot;c1&quot;&gt;//consumer 중 1개 block 해제
&lt;/span&gt;		&lt;span class=&quot;cm&quot;&gt;/*
		                       critical section end
		*/&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;semSignal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
		&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
	&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;consumer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
	&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;semWait&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;          &lt;span class=&quot;c1&quot;&gt;//buffer.empty()==true일 때 실행되는 것을 방지하기 위해 block
&lt;/span&gt;		&lt;span class=&quot;n&quot;&gt;semWait&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
		&lt;span class=&quot;cm&quot;&gt;/*
		                     critical section start
		*/&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;take&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;        &lt;span class=&quot;c1&quot;&gt;//buffer에서 pop
&lt;/span&gt;		&lt;span class=&quot;cm&quot;&gt;/*
			                	critical section end
		*/&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;semSignal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
		&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;semSignal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;       &lt;span class=&quot;c1&quot;&gt;//buffer.full()==false이므로 block된 producer 중 1개 unblock
&lt;/span&gt;		&lt;span class=&quot;n&quot;&gt;consume&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;       &lt;span class=&quot;c1&quot;&gt;//자원 소비
&lt;/span&gt;	&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;semInit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;semInit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;semInit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BUFFER_SIZE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_of_producers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;thread_start&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_of_consumers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;thread_start&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;</content><category term="Operating System" /><summary type="html"></summary></entry><entry><title type="html">[운영체제] Concurrency</title><link href="https://cpm0722.github.io/Concurrency/" rel="alternate" type="text/html" title="[운영체제] Concurrency" /><published>2020-11-16T18:00:00-06:00</published><updated>2020-11-16T18:00:00-06:00</updated><id>https://cpm0722.github.io/Concurrency</id><content type="html" xml:base="https://cpm0722.github.io/Concurrency/">&lt;hr /&gt;

&lt;p&gt;숭실대학교 컴퓨터학부 홍지만 교수님의 2020-2학기 운영체제 강의를 정리 및 재구성했다.&lt;/p&gt;

&lt;h1 id=&quot;thread&quot;&gt;Thread&lt;/h1&gt;

&lt;h2 id=&quot;process&quot;&gt;Process&lt;/h2&gt;

&lt;p&gt;OS에서 process는 역할을 정리해보자. 우선 process는 자원 소유의 단위이다. 자원이라는 것은 main memory, I/O device, file system 등을 의미한다. 대표적인 예시로 process별로 main memory에 서로 다른 공간을 할당하는 것이 있다. 두번째로 process는 scheduling의 단위이다. context switching은 process 사이에 발생하면서 다음 실행될 process를 선택한다. 이러한 process의 2가지 역할은 서로 독립적이다. 따라서 os는 두 가지 역할을 모두 process라는 하나의 개념으로 수행하지 않고, 별개의 단위를 만들어냈다. 우선 자원 관리 역할은 process가 그대로 수행한다. 이 때의 process를 task라고 명명하기도 한다. 반면 scheduling의 단위는 thread 또는 경량(lightweight) process라고 새로 정의한다.&lt;/p&gt;

&lt;h2 id=&quot;multi-thread&quot;&gt;Multi-thread&lt;/h2&gt;

&lt;p&gt;os가 하나의 process 내에 여러 thread를 지원하는 것을 다중 쓰레딩(kernel-level multi thread)라고 한다. MS-DOS와 같은 단일 사용자 process의 경우에는 오직 하나의 process만 동시에 실행될 수 있으며, 해당 process 내에 하나의 thread만이 존재한다. 즉, thread라는 개념이 없는 것과 마찬가지이다. 초기의 UNIX와 같은 다중 사용자 process는 여러 process가 동시에 실행될 수 있지만, 각 process 내에 하나의 thread만이 존재한다. Windows, Mac OS, BSD와 같은 비교적 최신 운영체제는 모두 multi thread를 채택하고 있다. 여러 process가 동시에 실행될 수 있으면서, 각 process 내에 여러 thread가 함께 존재하는 것이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-11-17-Concurrency/01.png&quot; alt=&quot;01.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;다증 쓰레딩 환경에서 process는 자원 할당의 단위, 보호의 단위로써의 의미를 갖는다. process 별로 자원을 할당하고, 다른 process가 자신의 자원에 접근하지 못하도록 보호하는 것이다. 한편 dispatching(scheduling)의 단위는 process가 아닌 thread가 수행하게 된다. 각 thread는 context switching 수행을 위해 별개의 독립된 program counter를 보유한다. 또한 별개의 독립된 stack을 각자 보유한다. 반면 heap, data, bss, text와 같은 memory 영역은 process 내의 다른 thread들과 공유한다. 즉, process에게 할당된 stack memory 영역을 여러 thread들이 나누어 사용하고, 나머지 memory 영역은 process 단위로 공유하는 것이다. 따라서 기본적으로 memory 등의 모든 자원은 process 내의 모든 thread들이 공유한다고 볼 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-11-17-Concurrency/02.png&quot; alt=&quot;02.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그렇다면 multi-thread를 사용함으로써 얻는 이점은 어떤 것이 있을까? 우선 성능 향상에 큰 도움이 된다. process를 여러 thread 별로 할당해 I/O를 많이 하는 thread, CPU 연산을 많이 하는 thread를 분리한다면 I/O 때문에 대기하는 시간을 단일 process 방식보다 훨씬 줄일 수 있을 것이다. 또한 process를 생성하는 것에 비해 이미 존재하는 process 내에서 새로운 thread를 생성하는데 드는 비용이 더 적다는 장점도 있다. 이에 더해 context switching도 thread 간의 전환이 process 단위보다 더 빠르다. 마지막으로, process 간에는 자원을 공유할 수 없기 때문에 서로 통신하기 위해서는 kernel이 개입해야 하지만, thread는 kernel 호출 없이도 서로 원활하게 통신할 수 있다.&lt;/p&gt;

&lt;h2 id=&quot;processthread&quot;&gt;Process/Thread&lt;/h2&gt;

&lt;p&gt;process와 thread의 유사점/차이점을 정리해보자. 우선 context switching의 단위라는 점이 공통적이다. 또한 program counter를 보유한다는 점도 공통적이다. 반면 process는 PCB(process control block)으로 관리하지만, thread는 TCB(thread control block)으로 관리한다는 점이 차이가 있다. 또한 process는 다른 process의 자원(memory 공간 등)에 접근할 수 없는 반면, thread는 다른 thread의 자원에 접근 가능하다는 차이점이 있다. 마지막으로, process 단위의 context switching 후에는 memory 주소 공간이 달라지지만, thread 단위의 context switching 후에는 memory 주소 공간의 변화가 없다는 차이점이 있다.&lt;/p&gt;

&lt;h2 id=&quot;states-of-threads&quot;&gt;States of Threads&lt;/h2&gt;

&lt;p&gt;thread의 상태는 process의 상태와는 별개이다. process의 상태를 떠올려보면, suspend 상태가 존재했다. thread 상태에서는 suspend가 존재하지 않는다. suspend라는 것은 main memory가 아닌 disk의 swap 영역에 위치하는 상태인데, 이는 process 전체가 swap 영역으로 옮겨지는 것이기에 thread 단위의 작업이 아니다. 즉, process가 swap-out된다면 해당 process에 속한 모든 thread가 함께 swap-out되는 것이다. 즉, suspend는 thread의 state와는 아무런 연관이 없다. thread의 상태는 크게 다음의 4가지가 있다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;생성(spawn)&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;블록(block)&lt;/p&gt;

    &lt;p&gt;thread가 어떠한 사건을 기다리는 상태이다. 자신의 register, program counter, stack pointer를 저장한다. dispatcher는 같은 process나 다른 process 내의 다른 thread를 수행한다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;비블록(unblock)&lt;/p&gt;

    &lt;p&gt;사건이 발생해 thread가 준비 queue에 push되는 상태이다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;종료(finish)&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;ultuser-level-thread--kltkernel-level-thread&quot;&gt;ULT(User-level Thread) / KLT(Kernel-level Thread)&lt;/h2&gt;

&lt;p&gt;사용자는 kernel-level thread를 직접 제어하지 못한다. KLT는 오직 kernel만이 제어할 수 있는 thread이다. single-thread 운영체제일 경우에는 KLT가 구현되어 있지 않다. 사용자는 현재 환경이 KLT가 구현되어 있는지도 알지 못하고, KLT를 제어할 수도 없기 때문에 user-level thread를 사용하게 되는데, 대개 pthread와 같은 thread library를 활용한다. thread library는 실행 운영체제가 single-thread일 경우 여러 ULT를 하나의 process로 보내게 된다. 만약 KLT가 구현되어 있는 multi-thread 운영체제라면 알맞게 KLT와 mapping을 시켜 여러 process로 보내게 된다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-11-17-Concurrency/03.png&quot; alt=&quot;03.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ULT와 KLT를 비교해보자. 한 process 내의 thread 사이 dispatch를 수행할 때를 생각해보자. 우선 ULT는 각 thread가 모두 사용자 주소 공간에 위치하기 때문에 dispatch를 할 때에 kernel mode로 변경될 필요 없이 user mode에서 모두 수행 가능하다. 반면 KLT는 dispatch를 할 때마다 kernel mode로 변경되어야만 한다. 즉, mode 전환 여부에 있어서는 ULT가 KLT보다 유리하다. 또한, ULT는 운영 체제에 종속적이지 않고 어떠한 kernel의 변경 없이도 원활히 수행될 수 있는 반면, KLT는 운영 체제에 따라 존재하지 않을 수도 있다는 차이점도 있다. 한편, ULT의 경우에는 하나의 thread에서 system call을 호출할 경우 같은 process 내의 모든 thread들이 함께 block이 된다는 치명적인 단점이 있다. 즉, 순수한 ULT만으로는 다중 처리의 장점을 살리지 못하게 된다. 반면 KLT의 경우에는 한 thread가 block된다고 하더라도 다른 thread들은 자유롭다. 즉, 여러 dispatcher에 하나의 process에 속한 여러 thread를 동시에 scheduling이 가능하다. 진정한 의미의 다중 처리가 가능한 것이다.&lt;/p&gt;

&lt;h1 id=&quot;lock&quot;&gt;Lock&lt;/h1&gt;

&lt;p&gt;thread는 서로 memory를 공유한다. 따라서 공통으로 사용하는 공유 변수가 있다. 여러 thread가 모두 변수의 값을 read만 하는 경우에는 문제가 발생하지 않지만, 만약 특정 thread가 변수의 값을 write하게 된다면 동기화 문제가 발생한다. 이를 해결하기 위해서는 한 thread가 변수를 write할 때에는 다른 thread가 해당 변수에 접근할 수 없도록 lock을 걸어주는 과정이 필요하다. lock~unlock의 구간을 critical section(임계 영역)이라고 한다. critical section은 해당 영역 내의 다수의 명령어를 atomic(원자적)하게 실행되도록 보장한다. lock에 대한 정책은 모든 thread에게 공정해야만 한다. 구체적으로, 우선 모든 thread가 적절한 시간 내에 critical section에 들어갈 수 있어야만 하고, critical section에 들어가기 위해 대기 중인 모든 thread들의 요청은 언젠가는 허가가 되어야 한다. 마지막으로 starvation이 발생하지 않아야 한다. lock은 적용 범위에 따라 사용하는 일부분만 lock을 하는 coarse-grained lock, 사용하는 모든 영역을 lock하는 fine-grained lock으로 구분되기도 한다.&lt;/p&gt;

&lt;p&gt;lock에서 사용되는 용어들을 정리해보자.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;mutual exclusion (상호 배제)&lt;/p&gt;

    &lt;p&gt;critical section에는 어느 시점에서든 반드시 단 1개의 thread만 접근 가능해야 한다는 개념이다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;race condition (경쟁 조건)&lt;/p&gt;

    &lt;p&gt;다수의 thread가 공유 data를 read/write하는 상황이다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;busy waiting&lt;/p&gt;

    &lt;p&gt;모든 thread가 critical section 접근 조건을 만족하지 못해, 반복적으로 접근 조건만을 검사하며 함께 대기하는 상황이다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;deadlock (교착 상태)&lt;/p&gt;

    &lt;p&gt;다수의 thread가 다른 thread가 어떠한 일을 해 줄 때까지 대기하는 상태로, 모든 thread가 다른 thread의 변화를 기다리며 대기하는 상황이다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;livelock&lt;/p&gt;

    &lt;p&gt;다수의 thread가 단순히 자신의 상태를 변화시키는 작업만 반복적으로 수행하면서 대기하는 상황이다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;mutual-exclusion&quot;&gt;mutual exclusion&lt;/h2&gt;

&lt;p&gt;mutual exclusion을 위한 세부 요구 조건이 있는데, 우선 mutual exclusion은 선택 사항이 아니라 필수 사항이라는 점이다. 즉, mutual exclusion은 강제되어야만 한다. 두번째로, critical section의 밖에 있는 어떤 thread도 critical section 내의 thread에게 간섭해서는 안된다. 세번째로, deadlock 및 starvation이 발생하지 않아야 한다. 네번째로, critical section에 아무도 접근하지 않을 때에는 대기하던 thread 중 하나가 즉시 critical section에 접근할 수 있어야 한다. 마지막으로, 어떠한 thread도 critical section을 무한히 점유할 수는 없다.&lt;/p&gt;

&lt;p&gt;mutual exclusion을 구현하기 위한 여러 방법을 살펴보자. 단순화를 위해 binary mutual exclusion으로 가정한다. 즉, 2개의 process만이 존재하는 상황이다.&lt;/p&gt;

&lt;h3 id=&quot;turn-variable&quot;&gt;turn variable&lt;/h3&gt;

&lt;div class=&quot;language-c highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;turn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;// thread 0
&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;thread0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;turn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
		&lt;span class=&quot;cm&quot;&gt;/*
			critical section...
		*/&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;turn&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
	&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;// thread 1
&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;thread1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;!&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;turn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
		&lt;span class=&quot;cm&quot;&gt;/*
			critical section...
		*/&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;turn&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
	&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;turn이라는 boolean 변수를 활용해 turn==0일 때에는 thread0을, turn==1일 때에는 thread1을 실행한다.&lt;/p&gt;

&lt;p&gt;하지만 이러한 정책은 한 thread가 연속적으로 critical section을 점유하지 못한다는 문제점이 있다. 구체적인 예시로, thread0이 critical section을 점유한 뒤 나오게 되면, thread1이 critical section을 점유하기 전까지는 절대 critical section을 점유할 수 없다. 만약 thread1이 critical section에 접근할 필요가 없는 thread라면 thread0은 무한히 대기하게 될 것이다. 즉, deadlock이 발생한 것이다.&lt;/p&gt;

&lt;h3 id=&quot;flag&quot;&gt;flag&lt;/h3&gt;

&lt;div class=&quot;language-c highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;flag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;};&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;// thread 0
&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;thread0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]);&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;flag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
		&lt;span class=&quot;cm&quot;&gt;/*
			critical section...
		*/&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;flag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
	&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;// thread 1
&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;thread1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]);&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;flag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
		&lt;span class=&quot;cm&quot;&gt;/*
			critical section...
		*/&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;flag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
	&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;각 thread마다 하나의 boolean 변수를 사용해 자신이 critical section을 점유하고 있는지를 나타낸다. 따라서 turn variable 정책과는 달리 하나의 thread가 연속적으로 critical section을 점유할 수 있다. 상대방 thread가 critical section을 점유하고 있는지만 확인하기 때문이다. 이는 flag 변수의 개수를 늘리고 while문에서 확인할 flag 개수를 증가만 시킨다면 binary가 아닌 n개의 thread에 대해서도 적용 가능하도록 손쉽게 확장할 수 있다.&lt;/p&gt;

&lt;p&gt;하지만 이러한 정책은 busy waiting을 발생시킨다는 문제점이 있다. 만약 다른 thread의 flag를 검사하는while문과 자신의 flag를 1로 변화시키는 명령어 사이에 context switching이 발생하게 된다면 두 flag가 모두 1이 되어 두 thread 모두 critical section에 접근하게 된다. 이는 critical section의 정의에 부합하지 않는 상황이다. 이는 실제 lock을 수행하는 명령과 lock 수행을 돕는 변수의 값 변경 사이에 context switching이 발생한 것으로 lock step 사이에 scheduling이 발생했다고 볼 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;busy-waiting&quot;&gt;busy waiting&lt;/h3&gt;

&lt;div class=&quot;language-c highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;flag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;};&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;// thread 0
&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;thread0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;flag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]);&lt;/span&gt;
		&lt;span class=&quot;cm&quot;&gt;/*
			critical section...
		*/&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;flag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
	&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;// thread 1
&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;thread1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;flag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]);&lt;/span&gt;
		&lt;span class=&quot;cm&quot;&gt;/*
			critical section...
		*/&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;flag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
	&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;lock step 사이에 context switching이 발생하는 것을 막기 위해서 flag를 먼저 변경시킨 뒤 대기한다. 그러나 이 경우에도 flag의 변경과 while문 사이에 context switching이 발생하게 되면 모든 flag가 1이 되게 된다. 이러한 경우 모든 thread가 flag를 확인하는 while문을 무한히 수행하게 된다. busy waiting이 발생한 것이다.&lt;/p&gt;

&lt;h3 id=&quot;busy-flag-again&quot;&gt;busy flag again&lt;/h3&gt;

&lt;div class=&quot;language-c highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;flag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;};&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;// thread 0
&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;thread0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]){&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;flag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;delay&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;flag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
		&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
		&lt;span class=&quot;cm&quot;&gt;/*
			critical section...
		*/&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;flag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
	&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;// thread 1
&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;thread1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]){&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;flag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;delay&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;flag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
		&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
		&lt;span class=&quot;cm&quot;&gt;/*
			critical section...
		*/&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;flag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
	&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;다른 thread의 flag를 검사하는 while문 내에서 자신의 flag를 일정 시간 간격으로 toggle하는 것이다. 이러한 경우 delay 시각 내에 context switching이 발생하게 되면 다른 thread가 critical section에 접근할 수 있게 된다. 하지만 이 역시 두 thread의 delay가 동시에 발생하는 최악의 경우에는 livelock 상태에 빠지게 된다. 하지만 이는 현실에서는 발생하기 불가능에 가깝기 때문에 무시되고는 한다. SW 상으로 critical section 정책을 구현하는 것에 있어서는 위의 정책이 가장 최선이다.&lt;/p&gt;

&lt;h3 id=&quot;interrupt-disable&quot;&gt;interrupt disable&lt;/h3&gt;

&lt;div class=&quot;language-c highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;// thread 0
&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;thread0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;interrupt_disable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
		&lt;span class=&quot;cm&quot;&gt;/*
			critical section...
		*/&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;interrupt_enable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
	&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;// thread 1
&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;thread1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;interrupt_disable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
		&lt;span class=&quot;cm&quot;&gt;/*
			critical section...
		*/&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;interrupt_enable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
	&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;HW적으로 critical section 접근 이전에 interrupt를 disable한 뒤, critical section 이후에 interrupt를 enable함으로써 context switching이 발생하지 않도록 만드는 것이다. 그러나 real time OS에서 context switching을 금지한다는 것은 있을 수 없는 일이기에 현실성이 없다.&lt;/p&gt;

&lt;h3 id=&quot;atomic-instruction&quot;&gt;atomic instruction&lt;/h3&gt;

&lt;div class=&quot;language-c highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;flag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;};&lt;/span&gt;

&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;thread0_atomic_instruction&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]);&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;flag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;thread1_atomic_instruction&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]);&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;flag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;// thread 0
&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;thread0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;thread0_atomic_instruction&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
		&lt;span class=&quot;cm&quot;&gt;/*
			critical section...
		*/&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;flag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
	&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;// thread 1
&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;thread1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;thread1_atomic_instruction&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
		&lt;span class=&quot;cm&quot;&gt;/*
			critical section...
		*/&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;flag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
	&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;기존의 flag 정책에서 문제가 발생하는 상황은 다른 thread의 flag를 검사하는 while문과 자신 flag의 값을 변경하는 명령어 사이에 context switching이 발생하는 경우였다. 이를 막기 위해 HW 상에서 두 명령어를 묶어 atomic하게 실행되도록 하는 것이다. 이를 위해서 HW는 testset과 exchange라는 명령어를 제공하게 된다.&lt;/p&gt;

&lt;h1 id=&quot;deadlock&quot;&gt;Deadlock&lt;/h1&gt;

&lt;p&gt;deadlock은 어느 경우에 발생하는지, 어떻게 해결할 수 있는지에 대해 알아보자.&lt;/p&gt;

&lt;p&gt;deadlock은 아래의 4가지 조건이 모두 충족되었을 때 발생한다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;mutual exclusion (상호 배제)&lt;/p&gt;

    &lt;p&gt;critical section을 동시에 최대 1개의 thread만 점유할 수 있는 것이다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Hold-and-wait (점유 대기)&lt;/p&gt;

    &lt;p&gt;critical section을 점유할 수 없을 경우 critical section이 비워질 때까지 대기한다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Non preemption (비선점)&lt;/p&gt;

    &lt;p&gt;한 번 점유한 경우 다른 thread에 의해 강제로 점유를 뺏기지 않는다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Circular wait (환형 대기)&lt;/p&gt;

    &lt;p&gt;자원 할당 그래프 (Resource Allocation Graph)에서 cycle이 생성된 경우이다. 즉, 서로 다른 thread의 행동을 기다리면서 무한히 대기하는 상황이다.&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/assets/images/2020-11-17-Concurrency/04.png&quot; alt=&quot;04.png&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;1~3의 조건은 deadlock의 필요 조건이다. 즉, 조건 중 어느 하나라도 충족하지 않으면 deadlock은 발생하지 않는다. 하지만 1~3의 조건이 모두 충족되었다고 해서 무조건 deadlock이 발생하는 것은 아니다. 1~4의 조건이 모두 만족해야만 deadlock이 발생한다. 즉 1~4의 조건은 deadlock의 필요충분 조건이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-11-17-Concurrency/05.png&quot; alt=&quot;05.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위는 Process P와 Q가 자원 A와 B를 경쟁적으로 사용하는 상황에서의 deadlock 발생 가능성을 나타낸 것이다. 총 6개의 시나리오에 대해서 살펴보자.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Q가 B를 획득하고, A를 획득한다. Q는 모든 작업을 수행하고 B와 A를 순서대로 release한다. 이후에 P로 전환되어 자유롭게 실행된다.&lt;/li&gt;
  &lt;li&gt;Q가 B를 획득하고, A를 획득한다. P로 전환되지만 A를 획득할 수 없어 block된다. Q가 마저 실행되고 B와 A를 release한 뒤에 P가 실행된다.&lt;/li&gt;
  &lt;li&gt;Q가 B를 획득한 뒤 P로 전환되어 P가 A를 획득한다. 이후에 Q로 전환될 경우 Q가 A를 획득할 수 없어 block되고, P가 계속 실행될 경우 B를 획득할 수 없어 block된다. &lt;strong&gt;deadlock&lt;/strong&gt;이다.&lt;/li&gt;
  &lt;li&gt;P가 A를 획득한 뒤 Q로 전환되어 Q가 B를 획득한다. 이후에 P로 전환될 경우 P가 B를 획득할 수 없어 block되고, Q가 계속 실행될 경우 A를 획득할 수 없어 block된다. &lt;strong&gt;deadlock&lt;/strong&gt;이다.&lt;/li&gt;
  &lt;li&gt;P가 A를 획득하고, B를 획득한다. Q로 전환되지만 B를 획득할 수 없어 block된다. P가 마저 실행되고 A와 B를 release한 뒤에 Q가 실행된다.&lt;/li&gt;
  &lt;li&gt;P가 A를 획득하고, B를 획득한다. P는 모든 작업을 수행하고 A와 B를 순서대로 release한다. 이후에 Q로 전환되어 자유롭게 실행된다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-11-17-Concurrency/06.png&quot; alt=&quot;06.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위와 같이 한 process가 두 자원은 동시에 점유하지 않을 경우에는 deadlock이 발생하지 않게 된다. 위의 deadlock 발생 상황과의 차이점은 한 process가 동시에 두 자원을 점유하지 않는다는 것이다.&lt;/p&gt;

&lt;h2 id=&quot;deadlock-prevention&quot;&gt;Deadlock Prevention&lt;/h2&gt;

&lt;p&gt;deadlock을 해결하는 방법은 크게 2가지가 있다. deadlock이 발생할 가능성이 생기면 이를 예방하는 것이 그 중 하나이다. deadlock의 필요 조건(상호 배제, 점유 대기, 비선점)은 고려하지 않고 circular wait이 발생하지 않도록만 하는 것이다. 하지만 process가 사용할 모든 자원을 미리 알고 있어야 circular wait이 발생하는지를 예측할 수 있기 때문에 현실적으로 구현이 불가능에 가깝다. deadlock prevention의 방법으로는 process 시작 거부와 자원 할당 거부가 있다.&lt;/p&gt;

&lt;h3 id=&quot;process-시작-거부&quot;&gt;Process 시작 거부&lt;/h3&gt;

&lt;p&gt;자원에 대한 vector와 matrix를 정의해 계산하고, 이를 이용해 deadlock 발생을 예측해 회피한다. OS는 process 수행 이전에 아래의 정보들을 모두 알고 있어야만 한다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;자원: system에 존재하는 자원의 전체 개수&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;R = (R_1,R_2, ...,R_m)&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;가용: system에 존재하는 자원 중 현재 사용 가능한 자원의 개수&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;V=(V_1,V_2,...,V_m)&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;요청: process가 요청하고 있는 자원의 개수&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
C=\begin{pmatrix}C_{11}&amp;...&amp;C_{1m}\\&amp;...&amp;\\C_{n1}&amp;...&amp;C_{nm}\end{pmatrix} %]]&gt;&lt;/script&gt;

    &lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;C_{ij}&lt;/script&gt;: process &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;가 자원 &lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt;를 &lt;script type=&quot;math/tex&quot;&gt;C_{ij}&lt;/script&gt;만큼 요청&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;할당: process가 할당받고 있는 자원의 개수&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
A=\begin{pmatrix}A_{11}&amp;...&amp;A_{1m}\\&amp;...&amp;\\A_{n1}&amp;...&amp;A_{nm}\end{pmatrix} %]]&gt;&lt;/script&gt;

    &lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;A_{ij}&lt;/script&gt;: process &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;가 자원 &lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt;를 &lt;script type=&quot;math/tex&quot;&gt;A_{ij}&lt;/script&gt;만큼 할당받음&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;위의 vector와 matrix는 정의에 따라 아래와 같은 수식들이 성립된다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;전체 자원의 개수는 가용 가능한 자원과 전체 process들에게서 사용중인 자원의 합이다.&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;R_j=V_j+\sum_{j=1}^m{A_{ij}}&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;요청 자원은 전체 자원의 양보다 많을 수 없다.&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;C_{ij}\le R_j&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;할당 자원은 요청 자원보다 많을 수 없다.&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;A_{ij}\le C_{ij}&lt;/script&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;process 시작 거부 방식은 모든 자원들에 대해 아래의 수식을 만족할 때에만 해당 process를 시작한다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;R_j\ge C_{(n+1)j}+\sum_{i=1}^n{C_{ij}}\ \ \ for\ all\ j&lt;/script&gt;

&lt;p&gt;모든 자원들에 대해 process가 요청하는 전체 자원의 합과 새로운 process가 요청하는 자원을 더한 값이 실제 자원의 양보다 작을 때에만 process를 시작하는 것이다. 이는 최악의 경우에도 실행됨을 보장하기 위함이다. 최악의 경우라는 것은 모든 process들이 동시에 자신이 요청할 수 있는 최대 자원량을 한꺼번에 요청하는 상황을 뜻한다. 이러한 보수적인 조건을 만족했을 때에만 process가 실행되는 것이기에 현실에서 사용할 수 없는 방식이다.&lt;/p&gt;

&lt;h3 id=&quot;자원-할당-거부-은행원-algorithm&quot;&gt;자원 할당 거부 (은행원 algorithm)&lt;/h3&gt;

&lt;p&gt;자원 할당 거부를 통한 deadlock prevention는 은행원 algorithm을 사용한다. 은행원 algorithm이란 system의 상태를 safe state와 unsafe state로 구분한다. safe state란 deadlock이 발생하지 않도록 process에게 자원을 할당할 수 있는 경로가 존재하는 상태를 의미하고, unsafe state란 해당 경로가 존재하지 않는 상태를 말한다. 은행원 algorithm은 safe state를 유지할 수 있는 thread의 요청에 대해서만 수락해 자원을 할당해주고, unsafe state가 되는 thread의 요청에 대해서는 계속 거절한다.&lt;/p&gt;

&lt;p&gt;다음은 safe state가 계속되어 정상적으로 모든 process가 실행되는 경우 대한 예시이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-11-17-Concurrency/07.png&quot; alt=&quot;07.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;C-A&lt;/script&gt;는 추가적으로 할당해야 할 자원들의 matrix이다. &lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt;와 비교해 더 작은 값들을 갖는 &lt;script type=&quot;math/tex&quot;&gt;C-A&lt;/script&gt;의 row를 찾은 뒤 해당 process를 실행시키게 된다. 이후 &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt;에서 해당 process의 값들이 &lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt;에 더해지게 된다. 해당 process의 값들은 &lt;script type=&quot;math/tex&quot;&gt;C&lt;/script&gt;와 &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;C-A&lt;/script&gt;에서 모두 0이 된다.&lt;/p&gt;

&lt;p&gt;아래는 unsafe state에 대한 예시이다. 실행할 수 있는 process가 없는 경우이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-11-17-Concurrency/08.png&quot; alt=&quot;08.png&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;deadlock-detection&quot;&gt;Deadlock Detection&lt;/h2&gt;

&lt;p&gt;deadlock detection은 deadlock prevention에 비해 상대적으로 낙관적인 방법이다. process의 시작이나 자원 접근에 대해 제약을 가하지 않고, 요청이 들어오면 항상 할당을 한다. 대신 주기적으로 system에서 deadlock이 발생했는지를 검사하고 발생했을 경우 이를 해결하게 된다.&lt;/p&gt;

&lt;h3 id=&quot;deadlock-detection-1&quot;&gt;Deadlock Detection&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-11-17-Concurrency/09.png&quot; alt=&quot;09.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;deadlock prevention과 비슷하게 동작한다. algorithm은 다음과 같다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;임시 vector &lt;script type=&quot;math/tex&quot;&gt;W&lt;/script&gt;를 생성해 초기 값으로 &lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt;를 복사한다.&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;Q&lt;/script&gt;의 row를 탐색하며 모든 자원이 &lt;script type=&quot;math/tex&quot;&gt;W&lt;/script&gt;보다 작은 process가 있을 경우 해당 process를 mark한다. 그러한 process가 없을 경우 deadlock이 발생한 것이므로 algorithm을 종료한다.&lt;/li&gt;
  &lt;li&gt;process를 찾았을 경우 &lt;script type=&quot;math/tex&quot;&gt;W&lt;/script&gt;에 &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt;에서의 process의 값을 더한다. 2단계로 돌아가 다시 수행한다.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;deadlock-solution&quot;&gt;Deadlock Solution&lt;/h3&gt;

&lt;p&gt;여러 deadlock solution이 있지만, 그 중에서 대표적인 solution들을 살펴본다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;deadlock에 연관된 모든 process 중지&lt;/p&gt;

    &lt;p&gt;실제 많은 OS에서 채택하고 있는 방식이다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;deadlock에 연관된 모든 process roll-back 후 재수행&lt;/p&gt;

    &lt;p&gt;특정 checkpoint까지 roll-back 후 재수행하는 방식이나, 어떤 process가 먼저 수행될 지는 nondeterministic하기 때문에 deadlock이 재발생할 수도 있다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;deadlock이 해소될 때까지 연관된 process를 하나씩 종료&lt;/p&gt;

    &lt;p&gt;비용이 가장 적은 것, 지금까지 사용한 dispatcher 시간이 적은 것, 지금까지 생산한 출력량이 적은 것, 이후 남은 수행 시간이 가장 긴 것, 할당받은 자원이 가장 적은 것, 우선 순위가 낮은 것부터 종료시킨다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;deadlock이 해소될 때까지 연관된 자원들을 하나씩 preemption&lt;/p&gt;

    &lt;p&gt;가장 비용이 적은 자원부터 하나씩 preemption한 후 deadlock detection algorithm을 수행해 deadlock 존재 여부를 파악한다. 자원을 preemption당한 process는 해당 자원을 할당 받기 전으로 roll-back된다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;dinning-philosopher-problem&quot;&gt;Dinning Philosopher Problem&lt;/h2&gt;

&lt;p&gt;식사하는 철학자 problem은 deadlock의 대표적인 예시이다. 여러 철학자가 원탁 테이블에 앉아 식사를 하는데, 철학자가 왼쪽의 포크를 먼저 집은 뒤, 오른쪽에 있는 포크를 집어 식사를 한다. 식사를 마치면 두 포크를 테이블에 내려놓는다. 철학자들은 포크 2개를 모두 가진 상태에서만 식사를 할 수 있다. 이 때 포크가 철학자의 인원수와 동일하게 배치가 되어있다고 하면 deadlock이 발생할 것이다. pseudo code는 다음과 같다.&lt;/p&gt;

&lt;div class=&quot;language-c highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;cp&quot;&gt;#define N 5       //number of philosopher
&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;semaphore&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fork&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;

&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;philosopher&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;think&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;semWait&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fork&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]);&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;semWait&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fork&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mod&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;eat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;semSignal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fork&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mod&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]);&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;semSignal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fork&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]);&lt;/span&gt;
	&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;//semaphore 모두 로 초기화
&lt;/span&gt;		&lt;span class=&quot;n&quot;&gt;semInit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fork&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

	&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;philosopher&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

	&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;solution은 동시에 테이블에 앉을 수 있는 최대 인원수를 N-1로 제한하는 것이다. 이 때 semaphore를 사용한다.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://cpm0722.github.io/operating%20system/Semaphore/&quot;&gt;Semaphore&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;pseudo code는 다음과 같다.&lt;/p&gt;

&lt;div class=&quot;language-c highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;cp&quot;&gt;#define N 5       //number of philosopher
&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;semaphore&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fork&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;semaphore&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;table&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;philosopher&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;think&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
		&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;semWait&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;table&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;semWait&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fork&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]);&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;semWait&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fork&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mod&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;eat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;semSignal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fork&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mod&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]);&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;semSignal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fork&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]);&lt;/span&gt;
		&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;semSignal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;table&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;
	&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;//semaphore 모두 로 초기화
&lt;/span&gt;		&lt;span class=&quot;n&quot;&gt;semInit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fork&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

	&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;semInit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;table&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;

	&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;philosopher&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

	&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;</content><category term="Operating System" /><summary type="html"></summary></entry><entry><title type="html">[NLP 논문 리뷰] KR-BERT: A Small Scale Korean Specific Language Model</title><link href="https://cpm0722.github.io/KR-BERT-A-Small-Scale-Korean-Specific-Language-Model/" rel="alternate" type="text/html" title="[NLP 논문 리뷰] KR-BERT: A Small Scale Korean Specific Language Model" /><published>2020-11-12T18:00:00-06:00</published><updated>2020-11-12T18:00:00-06:00</updated><id>https://cpm0722.github.io/KR-BERT-A-Small-Scale-Korean-Specific-Language-Model</id><content type="html" xml:base="https://cpm0722.github.io/KR-BERT-A-Small-Scale-Korean-Specific-Language-Model/">&lt;h2 id=&quot;paper-info&quot;&gt;Paper Info&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2008.03979&quot;&gt;Archive Link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2008.03979.pdf&quot;&gt;Paper Link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Submit Date: Aug 10, 2020&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;기존의 BERT model은 104개의 language의 Wikipedia dataset으로 학습된 model이다. 범용적으로 사용될 수 있다는 장점에도 불구하고, model의 크기가 과도하게 크다는 단점이 존재한다. 또한 non-English downstream task에서 좋은 성능을 보여주지 못하는 경우가 많다는 한계도 명확하다. 특히나 Korean과 같은 언어에서는 한계가 두드러진다.&lt;/p&gt;

&lt;p&gt;Korean NLP task를 해결하기 위한 BERT model은 다음과 같은 이유들로 인해 많은 어려움이 있다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Korean이 교착어라는 특성으로 인해 과도하게 많은 형태소&lt;/li&gt;
  &lt;li&gt;Hangul의 과도하게 많은 character (10,000개 이상)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;본 논문에서는 위와 같은 Korean의 한계점에도 불구하고 Korean-specific한 BERT model을 고안해냈다. 우선 Multilingual BERT model에 비해 model의 size를 과감히 줄이고, sub-characters BPE를 사용했다. 또한 Bidirectional WordPiece Tokenizer를 사용해 Korean의 linguistic한 특성을 반영하고자 했다. KR-BERT model은 다른 Multilingual BERT Model의 성능을 모든 task에서 능가했고, 이에 더해 KorBERT나 KoBERT와 같은 기존의 Korean-specific model과도 동등하거나 더 좋은 성능을 보였다. 이는 KR-BERT의 작은 model 크기를 고려하면 매우 유의미한 결과이다.&lt;/p&gt;

&lt;h1 id=&quot;related-work&quot;&gt;Related Work&lt;/h1&gt;

&lt;h2 id=&quot;models-after-bert&quot;&gt;Models after BERT&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-11-13-KR-BERT-A-Small-Scale-Korean-Specific-Language-Model/01.jpg&quot; alt=&quot;01.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;BERT 이후로 XLNet과 RoBERTa와 같은 대규모 dataset을 사용한 model들이 많이 등장했다. 그에 비해 DistilBERT나 ALBERT와 같이 #parameters를 줄이고, dataset도 늘리지 않은 small model들도 등장했다.&lt;/p&gt;

&lt;h2 id=&quot;recent-korean-bert-models&quot;&gt;Recent Korean BERT models&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-11-13-KR-BERT-A-Small-Scale-Korean-Specific-Language-Model/02.jpg&quot; alt=&quot;02.jpg&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;the-need-for-a-small-scale-language-specific-model&quot;&gt;The Need for a Small-scale Language-specific Model&lt;/h1&gt;

&lt;p&gt;Korean NLP task에서 multilingual BERT model은 아래와 같은 한계를 지닌다.&lt;/p&gt;

&lt;h2 id=&quot;limit-of-corpus-domain&quot;&gt;Limit of Corpus Domain&lt;/h2&gt;

&lt;p&gt;multilingual BERT는 104개의 language로 구성된 Wikipedia data로 pretrain된 model이다. German이나 French와 같은 data가 풍부한 language에 대해서는 Wikipedia에 더해 추가적인 dataset을 활용해 pretrain을 수행했다. 그러나 그 외 대부분의 language는 Wikipedia dataset만으로 pretrain되었다. Wikipedia dataset은 다양한 corpus를 포함하고 있지 않기에 제대로 된 학습을 기대하기 어렵다.&lt;/p&gt;

&lt;h2 id=&quot;considering-language-specific-properties&quot;&gt;Considering Language-specific Properties&lt;/h2&gt;

&lt;h3 id=&quot;rare-character-problem&quot;&gt;Rare “Character” Problem&lt;/h3&gt;

&lt;p&gt;English와 같은 Alphabet을 사용하는 language는 OOV가 적을 수 밖에 없다. 전체 character가 26개에 불과하기 때문이다. 반면 Korean은 syllable 기반이기 때문에 무려 11,172개의 character가 존재한다. 그러나 multilingual BERT에서는 이 중 오직 1,187개의 character만이 포함되었다. 나머지 character에 대해서는 제대로 학습이 되었다고 볼 수 없는 것이다.&lt;/p&gt;

&lt;h3 id=&quot;inadequacy-for-morphologically-rich-languages&quot;&gt;Inadequacy for Morphologically Rich Languages&lt;/h3&gt;

&lt;p&gt;Korean은 교착어이다. 때문에 English와 같은 language보다 훨씬 많은 형태소를 가짐은 물론 French나 German과 같은 굴절어 보다도 더 많은 형태소를 갖는다. 대표적인 교착어인 Japanese나 Korean은 동사의 활용형만 하더라도 수많은 다른 형태를 갖는다.&lt;/p&gt;

&lt;h3 id=&quot;lack-of-meaningful-tokens&quot;&gt;Lack of Meaningful Tokens&lt;/h3&gt;

&lt;p&gt;character-level의 Korean은 음절 단위인데, 각 음절의 구분은 발음에서의 가치만 있을 뿐 의미론적으로 큰 가치가 없는 구분이다. 오히려 자음/모음 (문자소) 단위가 의미를 갖는 경우가 더 많다. multilingual BERT는 모든 language에 universal하게 적용되는 model을 위해 character-level로 설계가 되었기 때문에 Korean NLP task에 적합하지 않다.&lt;/p&gt;

&lt;h2 id=&quot;large-scale-of-the-model&quot;&gt;Large Scale of the Model&lt;/h2&gt;

&lt;p&gt;XLNet이나 RoBERTa와 같은 대규모 model은 매우 많은 parameters와 큰 dataset, 큰 vocabulary를 사용했다. 그러나 이러한 대규모 model은 자원의 제약이 너무 많이 가해지기 때문에 작은 vocabulary, 적은 parameters, 적은 training dataset으로도 좋은 성능을 보이는 것을 목표로 했다.&lt;/p&gt;

&lt;h1 id=&quot;models&quot;&gt;Models&lt;/h1&gt;

&lt;p&gt;총 4가지 version의 KR-BERT에 대해 제시하고 비교한다. 우선 가장 작은 의미의 단위를 character-level(음절 단위)과 sub-character-level(자음/모음 단위)로 구분한다. 각각의 경우에 대해 BERT의  Original Tokenizer(WordPiece)를 사용한 것과 Bidirectional WordPiece Tokenizer를 사용한 것을 비교한다.&lt;/p&gt;

&lt;h2 id=&quot;subcharacter-text-representation&quot;&gt;Subcharacter Text Representation&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-11-13-KR-BERT-A-Small-Scale-Korean-Specific-Language-Model/03.jpg&quot; alt=&quot;03.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;자음/모음 단위 구분을 통해 얻을 수 있는 이점은 동사나 형용사에 붙는 활용형을 정확하게 잡아낼 수 있다는 것이다. Table 3의 “갔”, “감”, “간”, “갈”은 모두 “가다”의 “가”에 여러 활용형이 붙은 경우이다. 하지만 이를 character-level로 분석하게 되면 모두 별개의 token이 된다. sub-character level로 분석을 함으로써 실제 “가다”의 의미를 파악해 낼 수 있는 것이다.&lt;/p&gt;

&lt;h2 id=&quot;subword-vocabulary&quot;&gt;Subword Vocabulary&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-11-13-KR-BERT-A-Small-Scale-Korean-Specific-Language-Model/04.jpg&quot; alt=&quot;04.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;BPE의 성능은 vocabulary size에 따라 결정된다. 이는 heuristic하게 결정해야 하는데, 8000~20000 사이의 vocabulary size에 대해 test를 진행한 뒤 100,000 step에서의 Masked LM Accuracy를 비교한 결과 vocabulary size가 10,000일 때에 가장 성능이 좋다는 결론을 도출해냈다.&lt;/p&gt;

&lt;p&gt;이후 Korean text에서 빈번하게 사용되는 외국어(Alphabet, 한자, 일본어 등)에 대해 heuristic하게 token을 추가했다.&lt;/p&gt;

&lt;p&gt;[Table 4]에서 볼 수 있듯이 KR-BERT는 character-level과 sub-character-level 모두에 있어서 Multilingual BERT나 KorBERT보다 훨씬 작은 크기의 vocabulary를 사용했다.&lt;/p&gt;

&lt;h3 id=&quot;subword-tokenization&quot;&gt;Subword Tokenization&lt;/h3&gt;

&lt;p&gt;기존의 WordPiece Tokenization과 본 논문에서 새로 제안한 Bidirectional WordPiece Tokenization을 모두 사용해 둘을 비교한다.&lt;/p&gt;

&lt;h3 id=&quot;baselines&quot;&gt;Baselines&lt;/h3&gt;

&lt;p&gt;Multilingual BERT나 KorBERT는 BPE를 사용한 WordPiece Tokenization를 채택했다. 반면 KoBERT는 Unigram LM을 사용한 SentencePiece Tokenization을 채택했다.&lt;/p&gt;

&lt;h3 id=&quot;bidirectional-wordpiece-tokenizer&quot;&gt;Bidirectional WordPiece Tokenizer&lt;/h3&gt;

&lt;p&gt;BPE를 forward로만 진행하지 않고, backward로도 동시에 진행하는 것이다. forward와 backward 각각의 pair를 생성한 뒤, 두 후보 중 더 등장 빈도가 높은 쪽을 선택하게 된다. 이는 한국어의 문법적 특성에 따라 고안된 방식이다. 한국어의 명사는 상대적으로 긴 어근을 갖고 주로 짧은 접두사들이 앞에 붙게 된다. 반면 동사의 경우에는 짧은 어근을 갖고 주로 짧은 접미사들이 뒤에 붙게 된다. Bidirectional BPE는 이러한 경우들에 대해 적절한 tokenizing을 수행할 수 있도록 돕는다.&lt;/p&gt;

&lt;h2 id=&quot;comparison-with-other-korean-models&quot;&gt;Comparison with Other Korean Models&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-11-13-KR-BERT-A-Small-Scale-Korean-Specific-Language-Model/05.jpg&quot; alt=&quot;05.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-11-13-KR-BERT-A-Small-Scale-Korean-Specific-Language-Model/06.jpg&quot; alt=&quot;06.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;[Table 4]를 보면 KR-BERT는 Multilingual BERT, KorBERT에 비해 더 적은 vocabulary, 더 적은 parameter, 더 적은 data size를 갖는다는 것을 확인할 수 있다. 반면 KoBERT에 비해서는 더 많은 vocabulary, 더 많은 parameter를 갖지만 dataset은 더 적다.&lt;/p&gt;

&lt;p&gt;[Table 5]는 각 model들의 vocabulary이 어떤 비율로 구성되어 있는지를 보여준다. Korean Specific한 model들이 Multilingual BERT보다 Korean words와 Korean subwords의 비율이 압도적으로 높다는 것을 확인할 수 있다.&lt;/p&gt;

&lt;p&gt;[Table 6]은 실제로 Tokenization이 어떻게 이루어지는지 구체적인 단어 예시를 통해 보여준다. “냉장고”는 Multilingual BERT와 KorBERT, KoBERT에서 모두  “냉”, “장”, “고”로 tokenizing된다. 반면 KR-BERT에서는 token level과 tokenizer에 관계없이 모든 model에 있어서 “냉장고”라는 하나의 token으로 분류한다. “냉장고”를 각 character 별로 단순하게 tokenizing한 것에 비해 의미론적으로 더 알맞게 tokenization이 된  것이다.&lt;/p&gt;

&lt;p&gt;“춥다”는 Multilingual BERT에서는 아예 OOV로 판별이 된다. KorBERT와 KoBERT에서는 모두 “춥”, “다”로 tokenizing하게 된다. 그러나 KR-BERT에서는 character level은 “춥”, “다”로 다른 Korean Specific Model과 동일하게 tokenizing을 하지만, sub-character level에서는 “추”, “ㅂ다”로 tokenizing을 한다. sub-character level의 tokenizing이 더 적절한 결과를 도출해낸다는 것을 확인할 수 있다.&lt;/p&gt;

&lt;p&gt;“뱃사람”은 Multilingual BERT에서는 OOV이고, KorBERT와 KoBERT에서는 “뱃”, “사람”으로 tokenizing된다. character level의 KR-BERT에서도 마찬가지의 결과를 보여준다. 반면 sub-character level KR-BERT는 “배”, “ㅅ”, “사람”으로 tokenizing을 한다. Korean의 문법적 특성인 ‘사이시옷’까지 잡아낸 것이다.&lt;/p&gt;

&lt;p&gt;“마이크”는 Multilingual BERT와 KoBERT에서는 “마”, “이”, “크”로, KorBERT에서는 “마이”, “크”로 tokenizing된다. 반면 KR-BERT에서는 모든 model에서 동일하게 “마이크”로 tokenizing한다. 외래어 표기에 있어서 기존 model에 비해 더 강력한 성능을 보여주는 것이다.&lt;/p&gt;

&lt;h1 id=&quot;experiments-and-results&quot;&gt;Experiments and Results&lt;/h1&gt;

&lt;p&gt;여러 Korean NLP downstream task에 대해서 Multilingual BERT와 기존의 Korean Specific Model, KR-BERT를 비교한다. sentiment classification, question answering, named entity recognition, paraphrase detection에 대해서 실험을 진행했다.&lt;/p&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;h3 id=&quot;masked-lm-accuracy&quot;&gt;Masked LM Accuracy&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-11-13-KR-BERT-A-Small-Scale-Korean-Specific-Language-Model/07.jpg&quot; alt=&quot;07.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;KR-BERT의 모든 model이 KoBERT보다 더 좋은 MLM Accuracy를 보여준다. 또한 KR-BERT 내에서 Bidirectional WordPiece를 사용한 model이 조금 더 나은 결과를 보여준다.&lt;/p&gt;

&lt;h3 id=&quot;downstream-tasks&quot;&gt;Downstream tasks&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-11-13-KR-BERT-A-Small-Scale-Korean-Specific-Language-Model/08.jpg&quot; alt=&quot;08.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;sentiment classification은 Naver Sentiment Movie Corpus Dataset을, question answering은 KorQuAd Dataset을, named entity recognition는 KorNER Dataset을, paraphrase detection은 Korean Paired Question Dataset을 사용했다.&lt;/p&gt;

&lt;p&gt;모든 경우에 있어서 Multilingual BERT는 Korean Specific Model의 최고 성능을 능가하지 못했다. KR-BERT는 KorQuAD와 KorNER에서 가장 좋은 성능을 보여준다. 반면 NSMC와 Paraphrase Detection에 있어서는 KorBERT가 근소하게 더 높은 수치를 보여준다. 하지만 그럼에도 불구하고 KorQuAD와 KorNER에서의 KorBERT와 KR-BERT의 차이는 7%로 매우 높다는 점, KorBERT의 model size와 풍부한 dataset을 고려한다면 매우 유의미한 결과이다.&lt;/p&gt;

&lt;h2 id=&quot;analysis-of-downstream-tasks&quot;&gt;Analysis of Downstream Tasks&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-11-13-KR-BERT-A-Small-Scale-Korean-Specific-Language-Model/09.jpg&quot; alt=&quot;09.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-11-13-KR-BERT-A-Small-Scale-Korean-Specific-Language-Model/10.jpg&quot; alt=&quot;10.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;사실 KR-BERT model 중에서 sub-character Bidirectional WordPiece model이 일관되게 최고의 성능을 보여주지는 못한다. 하지만 그럼에도 다른 model들에 비해 일관되게 좋은 성능을 유지한다는 점에서 긍정적이다.&lt;/p&gt;

&lt;p&gt;NSMC의 경우에는 웹사이트 사용자들의 data라는 점에서 noise나 문법적 오류가 상대적으로 많고, unformal한 data이다. NER은 task의 특성 상 당연하게도 고유 명사가 많으므로 OOV의 비율이 높을 것이다. 또한 KorQuAD와 Paraphrase Detection은 상대적으로 formal한 data일 것이다.&lt;/p&gt;

&lt;p&gt;[Table 9]를 보면 bidirectional 방식과 sub-character level이 문법적 오류를 더 정확하게 잡아낸다는 점을 확인할 수 있다. “이영화”는 사실 “이”, “영화”의 두 단어로 구분되어야 하지만 중간의 공백이 삽입되지 않은 경우이다. 이에 대해 Bidirectional WordPiece KR-BERT만이 “이”, “영화”로 정확하게 tokenizing을 수행한다. Bidirectional이 아닌 KR-BERT는 “이영”, “화”로 잘못된 tokenizing을 수행했다.&lt;/p&gt;

&lt;p&gt;“재밌는뎅”의 경우에는 “재밌는데”에 “ㅇ”라는 nosie가 추가된 경우이다. 이는 sub-character level KR-BERT가 정확하게 잡아내는데, “재미”, “ㅆ”, “는데”, “ㅇ”로 tokenizing을 수행한다. 반면 character-level KR-BERT는 “재”, “밌”, “는”, “뎅”으로 잘못된 tokenizing을 수행한다.&lt;/p&gt;

&lt;p&gt;NER과 같은 OOV 비율이 높은 task에 대해서는 sub-character level이 더 좋은 성능을 보여준다. 이는 [Table 10]에서 OOV rate를 확인했을 때 sub-character level이 character level 대비 OOV가 훨씬 낮다는 점을 보면 당연한 결과이다.&lt;/p&gt;

&lt;p&gt;KorQuAD나 Paraphrase Detection과 같은 formal data의 경우에는 WordPiece가 Bidirectional WordPiece보다 더 좋은 성능을 보여준다.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;Korean-specific BERT model인 KR-BERT model을 제안했다. 기존의 Korean-specific model에 비해 더 작은 규모에서 더 적은 dataset으로 동등하거나 더 좋은 성능을 보여줬다. 이 과정에서 sub-character level tokenizing, Bidirectional BPE를 사용해 Korean의 문법적 특성을 잡아냈다.&lt;/p&gt;</content><category term="NLP" /><category term="Korean" /><summary type="html">Paper Info</summary></entry><entry><title type="html">[운영체제] Paging Mechanism</title><link href="https://cpm0722.github.io/Paging-Mechanism/" rel="alternate" type="text/html" title="[운영체제] Paging Mechanism" /><published>2020-10-19T19:00:00-05:00</published><updated>2020-10-19T19:00:00-05:00</updated><id>https://cpm0722.github.io/Paging-Mechanism</id><content type="html" xml:base="https://cpm0722.github.io/Paging-Mechanism/">&lt;hr /&gt;

&lt;p&gt;숭실대학교 컴퓨터학부 홍지만 교수님의 2020-2학기 운영체제 강의를 정리 및 재구성했다.&lt;/p&gt;

&lt;h1 id=&quot;paging-mechanism&quot;&gt;Paging Mechanism&lt;/h1&gt;

&lt;p&gt;paging 기법에 대해 자세히 알아보자. 위에서 살펴본 고정 분할 및 가변 분할 기법은 각각 내부 단편화, 외부 단편화의 문제점이 존재했다. paging은 이러한 단점들을 해결하기 위해 고안된 방식이다. paging을 사용하면 결론적으로 외부 단편화는 발생하지 않으며, 내부 단편화는 아주 적은 횟수 (대개 process 당 1회) 발생하게 된다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-10-20-Paging-Mechanism/01.png&quot; alt=&quot;01.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;paging이란 memory 가상화에서 가상 주소와 물리 주소를 mapping시킬 때에 page frame을 단위로 하는 방식이다. page table에는 가상 주소, 물리 주소 뿐만 아니라 P, M, U bit 등의 control bit도 담겨져 있다. P(present) bit는 해당 page가 memory에 위치하는가에 대한 bit로, 1일 경우 가리키는 물리 주소가 물리 memory 영역이라는 의미이고 0일 경우에는 가리키는 물리 주소가 memory가 아닌 disk의 swap 영역이라는 뜻이다. 즉, P bit가 0일 경우에는 swap 영역에 있는 page를 memory로 불러와야 한다. 이러한 과정을 &lt;strong&gt;page fault&lt;/strong&gt;라고 한다. page fault는 결국 disk I/O를 호출하는 것이기에 schedule() 함수를 호출한다. 한편 M(modify) bit는 해당 page가 수정된 적이 있는지에 대한 bit이고, W(write) bit, D(dirty) bit라고도 불린다. U(used) bit는 해당 page를 read한 적이 있는지에 대한 bit로, R(read) bit라고도 불린다. page table은 OS가 각각의 process에게 개별적으로 부여하게 되며,  task_struct와 같은 PCB들은 멤버 변수로 page table을 가리키는 포인터 값을 저장한다. 한편, 대부분의 가상 memory 기법은 page table을 실제 memory가 아닌 가상 memory에 저장하게 된다. process가 running 상태라면, 최소한 해당 process의 page table 중 일부분은 memory에 존재해야 하고, 전체 page table이 memory에 존재하는 것이 가장 바람직할 것이다.&lt;/p&gt;

&lt;h1 id=&quot;virtual-address&quot;&gt;Virtual Address&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-10-20-Paging-Mechanism/02.png&quot; alt=&quot;02.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;page table을 통해 사용되는 가상 주소와 물리 주소는 모두 number + offset의 구조를 갖는다. page number를 통해 page table의 몇 번째 row에 접근할 지를 파악하고, register에 저장된 page table의 포인터 값과 page number를 더해 해당 page table의 row에 접근한다. 이후 얻은 frame number를 통해 실제 물리 memory에 접근하게 된다. 하지만 frame number는 결국 물리 memory에서의 시작 주소를 뜻하는 값이기 때문에 얼마나 data를 읽어들일지에 대한 정보는 알지 못한다. 이 때 사용하는 것이 offset이다. 가상 주소에서의 offset을 그대로 물리 주소에서 사용하게 된다. 이러한 모든 작업은 대개 HW(CPU의 Memory Management Unit)가 수행하게 된다. 과거에는 OS에서 SW를 통해 구현해 사용하기도 했으나 속도가 HW를 이용하는 것에 비해 많이 느리다.&lt;/p&gt;

&lt;p&gt;가상 주소의 bit 사용량을 통해 역으로 OS의 각종 변수 값을 유추할 수도 있다. 가상 주소에서 offset이 차지하는 bit수가 $o$라면, 해당 OS의 page frame size는 $2^o$가 된다. 한편, 가상 주소에서 page number가 사용하는 bit 수가 $p$라면, 해당 OS의 page table의 최대 크기(가질 수 있는 최대 항목 수)는 $2^p$가 된다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-10-20-Paging-Mechanism/03.png&quot; alt=&quot;03.png&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;과도한-크기의-page-table-문제-해결&quot;&gt;과도한 크기의 Page Table 문제 해결&lt;/h1&gt;

&lt;h2 id=&quot;계층-구조-page-table-사용&quot;&gt;계층 구조 Page Table 사용&lt;/h2&gt;

&lt;p&gt;page table의 크기는 page table entry의 size * page table가 가질 수 있는 최대 항목 수로 계산할 수 있다. 즉, page table이 가질 수 있는 최대 항목 수가 클 수록 page table의 크기도 커진다는 것이다. 너무 큰 page table을 운용하게 되면 memory 낭비가 심해진다. 각 process마다 page table 운용을 위해 여러 page frame을 사용하지만 그 중 실제로 page table의 극히 일부만 사용하는 상황이 대표적인 예시이다. 이를 해결하기 위한 대표적인 방법이 계층 구조 page table이다. 주로 2단계 계층 구조, 3단계 계층 구조 등이 있다. 우선 page directory가 있어 각각의 항목이 page table을 가리키도록 한다. page directory가 가리키는 page table이 꽉 찼을 경우에만 page directory의 다음 항목에서 새로운 page table을 가리키도록 동적으로 운용하는 방식이다. 아래는 2단계 계층 구조 page table의 예시이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-10-20-Paging-Mechanism/04.png&quot; alt=&quot;04.png&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;inverted-page-table-사용&quot;&gt;Inverted Page Table 사용&lt;/h2&gt;

&lt;p&gt;Page Number를 그대로 사용하지 않고 hash function을 이용해 얻은 hash value로 사용하게 된다. hash value는 hash table에서의 인덱스이다. hash table의 항목 수는 물리 memory의 page frame의 개수와 동일하다. 즉, hash table은 모든 process가 공용으로 사용하는 것이다. 따라서 hash table entry에는 page number뿐만 아니라 pid까지 함께 담겨져 있다. hash table에서의 collision을 해결하기 위해 linked list로 다음 entry를 연결하게 된다. 이렇게 찾은 hash table entry의 hash table에서의 인덱스를 이용해 page frame을 찾아가게 된다. hash table에서의 인덱스가 $i$라면, mapping된 page frame도 실제 물리 memory에서 $i$번째 page frame이 된다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-10-20-Paging-Mechanism/05.png&quot; alt=&quot;05.png&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;tlb-translation-look-aside-buffer-사용&quot;&gt;TLB (Translation Look-aside Buffer) 사용&lt;/h2&gt;

&lt;p&gt;translation look-aside buffer란 page table 항목들을 저장하기 위한 특수한 고속 cache 장치이다. 가장 최근에 참조된 n개의 page table entry항목들을 저장하게 된다. page table은 기존과 동일하게 control bits와 frame number를 저장하고 있으며, page number를 page table에서의 인덱스로 사용한다. 하지만 TLB에서는 page number를 인덱스로 사용해 각 항목에 접근할 수 없기 때문에 page table의 항목들이 가진 정보에 더해 page number를 추가적으로 저장해야 한다. 이를 &lt;strong&gt;연관 사상(Assosiative Mapping)&lt;/strong&gt;이라고 한다.&lt;/p&gt;

&lt;p&gt;실제 가상 주소를 물리 주소로 변환하는 과정을 따라가보자. 가상 주소가 주어지면 우선 TLB에서 해당 page number가 있는지 확인한다. page number가 TLB에 있을 경우 TLB Hit으로, 바로 frame number를 얻어 물리 주소를 구해낸다. 만약 TLB에 page number가 없을 경우 TLB Miss로, 기존과 동일하게 page table에서 page number를 통해 frame number를 구해낸다. 이후 해당 page number에 관련된 정보들을 TLB에 추가한다.  만약 TLB에 여유 공간이 없을 경우 가장 오래된 항목을 제거해 공간을 확보한다. 한편, 만약 page table에서 P bit가 0이라면 Page Fault로, secondary memory(swap)에 접근해 해당 page frame을 memory로 load한다. 이후 다시 page table에서 물리 주소를 찾아나선다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-10-20-Paging-Mechanism/06.png&quot; alt=&quot;06.png&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;적절한-page-frame-size&quot;&gt;적절한 Page (Frame) Size&lt;/h1&gt;

&lt;p&gt;page의 크기는 HW 설계에 있어서 매우 중요한 issue 중 하나이다. 여러 관점에서 장단점을 고려해서 신중하게 결정해야만 한다. 만약 page의 크기가 작다면 내부 단편화가 적게 발생할 것이다. 대신 한 process 당 필요한 page의 수가 많아지고, 이는 결국 page table의 size를 늘리는 결과를 낳는다. page table의 size가 커지면 Multi-Programming 환경에서는 여러 활성 process 중 일부의 page table이 물리 memory가 아닌 swap 영역에 있어야 함을 의미한다. 최악의 경우에는 한 번의 memory 참조로 Page Fault가 2번(page table, page frame) 발생할 수 있는 것이다.&lt;/p&gt;

&lt;h1 id=&quot;paging-replacement-policy&quot;&gt;Paging Replacement Policy&lt;/h1&gt;

&lt;p&gt;Memory의 모든 page frame이 사용 중인 상황에서 swap에 위치한 page frame을 참조하는 상황이 발생할 수 있다. 이 때에는 memory의 page frame 중 하나를 swap의 page frame과 교체해야 한다. 이러한 현상을 Page Fault라고 부른다. page fault가 다수 발생하는 현상을 thrashing이라고 한다. thrashing을 방지하기 위해 합리적인 page replacement policy를 채택해야 한다. 만약 자주 호출되는 page frame을 memory에서 빼내어 swap으로 이동시키게 되면, page fault 발생 횟수가 증가해 성능에 악영향을 미칠 것이다. page replacement 정책의 핵심은 page frame이 미래에 얼마나 참조될 지를 예측하는 것이다. 미래의 일을 완전히 예견하는 것은 불가능하나 과거의 경향을 근거로 미흡하게나마 예측할 수는 있다. 따라서 page replacement 정책은 대개 과거의 page frame 이동의 경향을 파악해 미래를 예측하고자 한다. 하지만 너무 정교한 page replacement policy를 적용하게 된다면 오히려 HW와 SW 상의 부담이 더 커지기 때문에 적절한 trade-off가 이루어져야 한다. 아래에서는 6가지 Paging Replacement 정책에 대해 살펴본다.&lt;/p&gt;

&lt;h2 id=&quot;optimal&quot;&gt;Optimal&lt;/h2&gt;

&lt;p&gt;미래에 참조될 page의 순서를 모두 아는 상태에서 앞으로 참조될 때까지의 시간이 가장 긴 page를 교체한다. 당연하게도 현실에서는 구현할 수 없다. optimal 정책은 어디까지나 다른 paging replacement 정책을 평가하는 기준으로써의 가치만 있을 뿐, 구현 대상이 아니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-10-20-Paging-Mechanism/07.png&quot; alt=&quot;07.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;5 time에서 page 5가 삽입되는데, 이전 time 기준으로 main memory에 존재하는 page 2, 3, 1는 각각 1 time, 4 time, $\infin$ time 후에 다시 참조된다. 따라서 이후 참조되기까지의 시간이 가장 많이 남은 page 1이 교체되게 된다.&lt;/p&gt;

&lt;h2 id=&quot;fifo-first-input-first-out&quot;&gt;FIFO (First Input First Out)&lt;/h2&gt;

&lt;p&gt;먼저 들어온 page가 먼저 나가는 단순 Queue 방식이다. scheduling 중 RR과 비슷하다고 볼 수 있다. 가장 오래 전에 반입된 page는 memory에 가장 오래 존재했기 때문에 더이상 사용되지 않을 것이라는 논리 하에서 구현된 정책이다. 구현이 매우 간단하지만 좋은 성능을 보이지 못한다. FIFO 정책 하에서 main memory의 page frame 수를 늘릴 경우에는 page fault가 덜 발생할 것 같지만, 의외로 page fault가 더 자주 발생하기도 한다. 이를 FIFO abnormally(이상 현상)이라고 한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-10-20-Paging-Mechanism/08.png&quot; alt=&quot;08.png&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;lifo-last-input-last-out&quot;&gt;LIFO (Last Input Last Out)&lt;/h2&gt;

&lt;p&gt;가장 최근에 들어온 page가 빠져나가는 Stack 방식이다. 하나의 page frame만이 지속적으로 교체되기 때문에 page fault가 매우 자주 발생할 것이다. 하지만 의외로 평균적인 성능은 FIFO와 비슷하다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-10-20-Paging-Mechanism/09.png&quot; alt=&quot;09.png&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;lru-least-recently-used&quot;&gt;LRU (Least Recently Used)&lt;/h2&gt;

&lt;p&gt;가장 오랫동안 참조되지 않은 page를 교체하는 것이다. LRU는 Optimal과 가장 비슷한 성능을 보이지만 실제로는 구현이 매우 곤란하다는 단점이 있다. 각 page frame마다 가장 최근에 참조된 시각을 기록해야 하는데, 결국 물리 memory 내의 모든 page frame에 대해 int 변수를 추가하고 매 참조마다 갱신하는 형태가 될 수 밖에 없다. 이는 시스템에 큰 부하를 줘 좋은 성능을 내지 못한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-10-20-Paging-Mechanism/10.png&quot; alt=&quot;10.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;5 time에서 page 5가 삽입되는데, 이전 time 기준으로 memory에 존재하는 page 2, 3, 1은 각각 참조 시점이 2 time, 3 time, 1 time 전이다. 따라서 가장 오래 전에 참조된 page 3이 교체된다.&lt;/p&gt;

&lt;h2 id=&quot;lfu-least-frequently-used&quot;&gt;LFU (Least Frequently Used)&lt;/h2&gt;

&lt;p&gt;참조된 빈도가 가장 낮은 page를 교체하는 것이다. LRU와 동일하게 좋은 성능을 보이지만 구현하기 곤란하다. LRU와 마찬가지로 각 page frame마다 새로운 변수를 추가해야 하는데, 이 경우에는 참조 횟수일 것이다. 만약 동일한 참조 횟수를 가진다면 FIFO 정책을 채택해 먼저 들어온 page를 교체한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-10-20-Paging-Mechanism/11.png&quot; alt=&quot;11.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;5 time에서 page 5가 삽입되는데, 이전 time 기준으로 memory에 존재하는 page 2, 3, 1은 각각 참조 횟수 2, 1, 1을 갖는다. 따라서 page 3과 1 중 선택을 해야 하는데, FIFO 정책을 채택해 더 먼저 들어온 page 3을 교체한다.&lt;/p&gt;

&lt;h2 id=&quot;clock--nur-not-used-recently&quot;&gt;Clock = NUR (Not Used Recently)&lt;/h2&gt;

&lt;p&gt;Clock 정책은 현대 OS에서 채택하고 있는 page replacement 정책이다. LRU나 LFU와 같이 추가적인 변수를 생성하지 않고, 기존에 page table에 이미 존재하던 R, W bit를 활용하게 된다. 사용하는 bit 수가 더 많아질수록 더 좋은 성능을 보인다. 아래에서는 2 bit를 사용하는 two handed clock이 아닌 one handed clock의 예시이다. 교체하는 우선 순위는 다음과 같다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;참조되지 않았으며, 수정되지 않음 (R = 0, W = 0)&lt;/li&gt;
  &lt;li&gt;참조되었으며, 수정되지 않음 (R = 1, W = 0)&lt;/li&gt;
  &lt;li&gt;참조되었으며, 수정됨 (R = 1, W = 1)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;우선 순위 1을 먼저 찾고, 우선 순위 1이 없을 경우 우선 순위 2를 찾아나가되 그 과정에서 지나치는 모든 page frame의 R bit를 0으로 설정한다. 만약 우선 순위 2도 없을 경우 모든 page frame을 탐색하면서 R bit를 0으로 만들었을 것이다. 그 상태에서 다시 우선 순위 1을 찾는 반복을 수행한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-10-20-Paging-Mechanism/12.png&quot; alt=&quot;12.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;새로운 symbol이 추가되는데, $\rightarrow$는 memory를 가리키는 pointer이다. clock 정책에서 다음에 삽입할 page가 어디인지를 가리킨다. * symbol은 참조 여부이다. *가 있을 경우 참조된 frame (R bit = 1), *가 없을 경우 참조되지 않은 frame (R bit = 0)이다.&lt;/p&gt;

&lt;p&gt;5 time에서 page 5가 삽입되는데, 현재 pointer가 가리키는 page frame은 2이다. page 2는 이미 참조가 된 상태이므로 우선 순위 1에 해당되지 않는다. 따라서 우선 순위 2를 찾아 나선다. 그 과정에서 page 2, 3, 1의 R bit를 0으로 수정한다. 우선 순위 2를 찾기 실패했기 때문에 처음으로 되돌아온다. page 2가 R bit=0이 되었기 때문에 우선 순위 1에 해당한다. 따라서 page 2를 교체한다.&lt;/p&gt;

&lt;p&gt;6 time에서 page 2가 삽입되는데, 현재 pointer가 가리키는 page frame은 3이다. page 3은 R bit=0이므로 우선 순위 1에 해당되기 때문에 3 page를 교체한다.&lt;/p&gt;

&lt;p&gt;7 time에서 page 4가 삽입되는데, 현재 pointer가 가리키는 page frame은 1이다. page 1은 R bit=0이므로 우선 순위 1에 해당되기 때문에 1 page를 교체한다.&lt;/p&gt;

&lt;p&gt;8 time에서 page 5가 삽입되는데, 이미 main memory에 존재하므로 fault가 발생하지 않는다. 따라서 pointer도 이동하지 않는다.&lt;/p&gt;

&lt;p&gt;9 time에서 page 3이 삽입되는데, 현재 pointer가 가리키는 page frame은 5이다. page 5는 이미 참조가 된 상태이므로 우선 순위 1에 해당되지 않는다. 따라서 우선 순위 2를 찾아 나선다. 그 과정에서 page 5, page 2, page 4의 R bit를 0으로 수정한다. 우선 순위 2를 찾기 실패했기 때문에 처음으로 되돌아온다. page 5가 R bit=0이 되었기 때문에 우선 순위 1에 해당한다. 따라서 page 5를 교체한다.&lt;/p&gt;

&lt;p&gt;10 time에서 page 2가 삽입되는데, 이미 main memory에 존재하므로  fault가 발생하지 않는다. 그런데 R bit=0이므로 R bit=1로 변경한다.&lt;/p&gt;

&lt;p&gt;11 time에서 page 5가 삽입되는데, 현재 pointer가 가리키는 page frame은 2이다. page 2는 이미 참조가 된 상태이므로 우선 순위 1에 해당되지 않는다. 따라서 우선 순위 2를 찾아 나선다. 우선 순위 1에 해당하는 page 4를 찾았고, 그 과정에서 page 2의 R bit를 0으로 변경했다. page 4를 교체한다.&lt;/p&gt;

&lt;p&gt;12 time에서 page 2가 삽입되는데, 이미 main memory에 존재하므로 fault가 발생하지 않는다. 그런데 page 2의 R bit=0이므로 R bit를 1로 변경한다.&lt;/p&gt;</content><category term="Operating System" /><summary type="html"></summary></entry><entry><title type="html">[NLP 논문 리뷰] An Empirical Study of Tokenization Strategies for Various Korean NLP Tasks</title><link href="https://cpm0722.github.io/An-Empirical-Study-of-Tokenization-Strategies-for-Various-Korean-NLP-Tasks/" rel="alternate" type="text/html" title="[NLP 논문 리뷰] An Empirical Study of Tokenization Strategies for Various Korean NLP Tasks" /><published>2020-10-09T19:00:00-05:00</published><updated>2020-10-09T19:00:00-05:00</updated><id>https://cpm0722.github.io/An-Empirical-Study-of-Tokenization-Strategies-for-Various-Korean-NLP-Tasks</id><content type="html" xml:base="https://cpm0722.github.io/An-Empirical-Study-of-Tokenization-Strategies-for-Various-Korean-NLP-Tasks/">&lt;h2 id=&quot;paper-info&quot;&gt;Paper Info&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2010.02534&quot;&gt;Archive Link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2010.02534.pdf&quot;&gt;Paper Link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Submit Date: Oct 6, 2020&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;NLP에서 Tokenization은 전처리 과정에서 가장 중요한 issue 중 하나이다. 가장 적절한 Tokenization 전략을 찾기 위한 연구는 수도 없이 이루어져 왔다. 그 중 가장 대표적인 방식이 BPE이다. BPE는 많은 연구를 통해 보편적으로 가장 효율적인 Tokenization 기법으로 알려졌지만, 아직 language나 task에 구애받지 않고 가장 효율적인가에 대해서는 명확하지 않다. 본 논문에서는 English에 비해 언어 형태론적으로 더 난해한 언어인 Korean에 적합한 tokenization 기법을 찾아내고자 한다. BPE는 가장 보편적인 language인 English를 기준으로 연구된 방식이기에 Korean에 적합하지 않을 수 있다는 생각에서 시작된 연구이다. 본 논문에서는 Korean-English translation, natural language understanding, machine reading comprehension, natural language inference, semantic textual similarity, sentiment analysis, paraphrase identification 등 많은 task에서 실험을 진행했다.&lt;/p&gt;

&lt;h1 id=&quot;background&quot;&gt;Background&lt;/h1&gt;

&lt;h2 id=&quot;mecab-ko-a-korean-morphological-analyzer&quot;&gt;MeCab-ko: A Korean Morphological Analyzer&lt;/h2&gt;

&lt;p&gt;MeCab은 Conditional Random Fields(CRFs)를 기반으로 하는 Japanese 형태소 번역기이다. Japanese와 Korean의 형태론, 문법 상의 유사성에서 착안해 Korean에 적용시킨 것이 MeCab-ko이다. MeCab-ko는 Sejong Corpus를 통해 학습되었으며, 많은 Korean NLP task에서 사용되어 왔고 매우 좋은 성능을 보였다.&lt;/p&gt;

&lt;h2 id=&quot;byte-pair-encoding&quot;&gt;Byte Pair Encoding&lt;/h2&gt;

&lt;p&gt;BPE는 data에서의 등장 빈도를 기반으로 묶는 data-driven statistical alogirhtm이다.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://cpm0722.github.io/paper%20review/Neural-Machine-Translation-of-Rare-Words-with-Subword-Units/&quot;&gt;Neural Machine Translation of Rare Words with Subword Units&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;related-work&quot;&gt;Related Work&lt;/h1&gt;

&lt;p&gt;몇몇 연구에서는 단순 BPE보다 해당 language의 구문에 대한 정보를 기반으로 한 segmentation 기법과 BPE를 혼합해 적용하는 것이 더 좋은 성능을 보인다고 주장해왔다. 특히 non-English language, 그 중 형태론적으로 unique한 특성을 갖는 language에 대해서 더욱 두드러진다. Hindi/Bengali, Arabic, Latvian 등에 대해서 BPE와 함께 unique한 segmentation 기법을 혼용한 연구가 진행되었으며, Korean에 있어서도 동일한 연구가 진행되었다. 하지만 Tokenization이 아닌 NMT task에 있어서 사용되는 parallel corpus filtering 전처리에 관한 연구였다는 점에서 본 논문과는 목적이 다르다.&lt;/p&gt;

&lt;h1 id=&quot;tokenization-strategies&quot;&gt;Tokenization Strategies&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-10-10-An-Empirical-Study-of-Tokenization-Strategies-for-Various-Korean-NLP-Tasks/01.jpg&quot; alt=&quot;01.jpg&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;consonant-and-vowel-cv&quot;&gt;Consonant and Vowel (CV)&lt;/h2&gt;

&lt;p&gt;자모 단위로 tokenizing을 하는 기법이다. 공백에 대해서 special token &lt;script type=&quot;math/tex&quot;&gt;\star&lt;/script&gt;를 추가했다.&lt;/p&gt;

&lt;h2 id=&quot;syllable&quot;&gt;Syllable&lt;/h2&gt;

&lt;p&gt;음절 단위로 tokenizing을 하는 기법이다. 역시나 공백에 대한 special token &lt;script type=&quot;math/tex&quot;&gt;\star&lt;/script&gt;를 사용한다.&lt;/p&gt;

&lt;h2 id=&quot;morpheme&quot;&gt;Morpheme&lt;/h2&gt;

&lt;p&gt;MeCab-ko의 형태소 단위 tokenizer를 사용한다. 하지만 이를 사용하면 original input에서의 공백이 제거가 되고, 따라서 original sentence로의 복원이 불가능해진다. 이를 해결하기 위해 공백 special token &lt;script type=&quot;math/tex&quot;&gt;\star&lt;/script&gt;를 추가했다.&lt;/p&gt;

&lt;h2 id=&quot;subword&quot;&gt;Subword&lt;/h2&gt;

&lt;p&gt;SentencePiece를 사용한 BPE를 적용했다. original sentence의 단어 단위를 구분하기 위해서 original sentence의 공백에 대응하는 token &lt;script type=&quot;math/tex&quot;&gt;\_&lt;/script&gt;를 매 단어의 시작에 삽입했다.&lt;/p&gt;

&lt;h2 id=&quot;morpheme-aware-subword&quot;&gt;Morpheme-aware Subword&lt;/h2&gt;

&lt;p&gt;위의 Subword 방식에서 한 발 더 나아가 언어론적 특징을 기반으로 한 segmentation 전략을 BPE와 결합한 방식이다. Morpheme 방식을 먼저 적용한 뒤, 형태소의 list에 대해서 BPE를 적용하게 된다. Morpheme를 적용한 후에 BPE를 사용하기 때문에 형태소 경계를 뛰어넘는 BPE는 발생하지 않는다. (“나랑 쇼핑하자.”에서 ‘쇼핑’, ‘하’는 각각이 별개의 형태소이기 때문에 (‘핑’,’하’)가 Pair로 묶일 수는 없다.)&lt;/p&gt;

&lt;h2 id=&quot;word&quot;&gt;Word&lt;/h2&gt;

&lt;p&gt;original input에서 공백을 기준으로 단어 단위로 tokenizing을 수행하는 가장 단순한 방식이다.&lt;/p&gt;

&lt;h1 id=&quot;experiments&quot;&gt;Experiments&lt;/h1&gt;

&lt;h2 id=&quot;korean-tofrom-english-machine-translation&quot;&gt;Korean to/from English Machine Translation&lt;/h2&gt;

&lt;h3 id=&quot;dataset&quot;&gt;Dataset&lt;/h3&gt;

&lt;p&gt;AI Hub에서 제공하는 Korean-English parallel corpus를 사용했다. 800K sentences pairs의 news data를 포함하고, 784K의 train data, 8K의 dev data, 8K의 test data로 구분했다.&lt;/p&gt;

&lt;h3 id=&quot;bpe-modeling&quot;&gt;BPE Modeling&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-10-10-An-Empirical-Study-of-Tokenization-Strategies-for-Various-Korean-NLP-Tasks/02.jpg&quot; alt=&quot;02.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;BPE training에서 AI Hub의 data를 사용할지, Wiki의 data를 사용할지 결정하기 위해 실험을 진행한다. AI Hub의 data는 실제 task에서의 dataset과 동일하기 때문에 corpus set이 동일하다는 장점이 있는 반면, dataset의 크기가 작다. Wiki는 dataset의 크기가 크지만, news에서 사용되는 corpus set과는 차이가 있다는 단점이 있다. Korean-English Translation, English-Korean Translation으로 성능을 비교해보는데, English BPE는 동일하게 Wiki의 English data를 사용한 32K BPE model을 사용했다. 그 결과, AI Hub의 data보다 Wiki의 data가 더 좋은 성능을 보였다. 따라서 본 논문의 이후에서는 Korean BPE training을 위해 Wiki dataset을 사용한다.&lt;/p&gt;

&lt;h3 id=&quot;training&quot;&gt;Training&lt;/h3&gt;

&lt;p&gt;다양한 vocabulary size의 BPE model로 AI Hub news dataset에 대해서 tokenization 기법을 테스트한다. 우선 NMT task에서 SOTA를 달성한 Transformer model을 사용한다. 가장 보편적으로 사용되는 hyperparameter 값을 채택했다. FAIRSEQ를 사용해 실험을 진행했다. 50 epochs마다 checkpoint를 저장했다.&lt;/p&gt;

&lt;h3 id=&quot;results&quot;&gt;Results&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-10-10-An-Empirical-Study-of-Tokenization-Strategies-for-Various-Korean-NLP-Tasks/03.jpg&quot; alt=&quot;03.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Ko-En, En-Ko task에서 모두 Subword와 Syllable가 Morpheme이나 Word보다 더 좋은 성능을 보였다. 이는 OOV Rate와 큰 관련이 있다. 한국어의 형태론은 너무 복잡한 규칙을 가져 수많은 형태소가 있기 때문에 64K 이하의 vocabulary size로는 OOV가 많이 발생할 수 밖에 없다. 하지만 Subword나 Syllable은 모두 음절 단위의 model이기 때문에 OOV가 훨씬 더 적게 발생하게 된다.&lt;/p&gt;

&lt;p&gt;한편 CV의 OOV Rate는 당연히 가장 적은 수치를 보여주는데, Syllable나 Subword에 비해서는 더 낮은 성능을 보여준다. 이를 통해 자모 단위는 문맥 정보를 담기에는 너무 작은 단위라는 것을 알 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-10-10-An-Empirical-Study-of-Tokenization-Strategies-for-Various-Korean-NLP-Tasks/04.jpg&quot; alt=&quot;04.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Morpheme-aware Subword가 가장 높은 BLEU Scores를 보여준다. Subword와 Morpheme-aware Subword의 차이점은 BPE 이전에 Morpheme의 수행 여부인데, 이는 결국 형태소 경계를 넘어서는 BPE가 발생하는가(Token Spanning Morpheme Boundaries)에서 차이를 보인다. 위의 Table은 Subword에서 각 vocabulary size마다 발생하는 Tokens Spanning Morpheme Boundaries의 횟수를 보여준다. 6~37%의 수치를 보여준다. 이를 통해 형태소 단위의 구분은 tokenization에서 성능에 큰 영향을 미치며, 따라서 형태소 구분을 무시한 단순 BPE는 Korean Tokenizing에 적합하지 않다는 것을 알 수 있다.&lt;/p&gt;

&lt;h2 id=&quot;korean-natural-language-understanding-tasks&quot;&gt;Korean Natural Language Understanding Tasks&lt;/h2&gt;

&lt;p&gt;BERT model을 사용했다. KorQuAD, KorNLI, KorSTS, NSMC, PAWS의 5개 NLU downstream task에 대해서 테스트를 진행했다.&lt;/p&gt;

&lt;h3 id=&quot;downstream-tasks&quot;&gt;Downstream Tasks&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Machine Reading Comprehension: KorQuAD 1.0 Dataset&lt;/p&gt;

    &lt;p&gt;SQuAD를 Korean에 맞게 적용한 dataset이다. 10,645개의 지문과 66,181개의 질문이 포함되고, 각 지문에 대해 주어진 여러 질문 중 가장 적합한 질문을 선택하는 task이다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Natural Language Inference: KorNLI Dataset&lt;/p&gt;

    &lt;p&gt;950,354개의 sentence pair(전제, 추론)이 있고 각 pair에 대해 두 sentence 사이의 관계가 entailment, contradiction, neutral인지 classification을 수행하는 task이다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Semantic Textual Similarity: KorSTS Dataset&lt;/p&gt;

    &lt;p&gt;8628개의 sentence pair가 있고, 각 pair에 대해 0~5 사이의 semantic similarity를 도출해내는 task이다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Sentiment Analysis: NSMC Dataset&lt;/p&gt;

    &lt;p&gt;네이버 영화 review에서 추출한 400K의 sentence에 대해 0(negative)~1(positive) 사이의 sentiment Analysis를 도출해내는 task이다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Paraphrase Identification: PAWS-X Dataset&lt;/p&gt;

    &lt;p&gt;paraphrase identification dataset인 PAWS-X에서 Korean dataset만 추출해낸 53,338 sentence pairs에 대해 0(negative)~1(positive)의 paraphrase identification을 도출해내는 task이다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;training-1&quot;&gt;Training&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-10-10-An-Empirical-Study-of-Tokenization-Strategies-for-Various-Korean-NLP-Tasks/05.jpg&quot; alt=&quot;05.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;large corpus로 pre-train된 BERT-Base model을 각 5개의 NLU task에 대해 별개로 fine-tuning시켜 실험을 진행했다. Korean Wiki Corpus(640MB)는 pre-train을 진행할 만큼 충분한 크기가 되지 못해 Namu-wiki에서 5.5GB의 corpus를 추출해내 Wiki Corpus와 함께 사용했다. hyperparameter는 batch size=1024, max sequence length=128, optimizer=AdamW, lr=5e-5, warm up steps=10K를 사용했다. pre-trained된 BERT Model을 Tensorflow에서 Pytorch로 convert한 뒤, HuggingFace Transformers를 사용해 fine-tuning을 진행했다. fine-tuning에서의 hyperparameter는 위 Table의 값을 사용했다.&lt;/p&gt;

&lt;h3 id=&quot;results-1&quot;&gt;Results&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-10-10-An-Empirical-Study-of-Tokenization-Strategies-for-Various-Korean-NLP-Tasks/06.jpg&quot; alt=&quot;06.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 5개의 NLU task에 대해 6개의 tokenizing 기법을 사용해 각각 dev set, test set에서의 성능을 측정했다. 예외적으로 KorQuAD의 경우에는 test set이 부족해 dev set만 사용했다.&lt;/p&gt;

&lt;p&gt;KorQuAD task에서는 Subword 64K model이 가장 좋은 성능을 보였다. Morpheme와 Subword에서는 vocabulary size와 Score가 비례 관계이다. 하지만 Morpheme-aware Subword에서는 32K model이 제일 높은 수치를 달성했다. 결론적으로, Morpheme-aware Subword model에서는 성능과 vocabulary size 사이의 유의미한 상관관계를 찾을 수 없었다.&lt;/p&gt;

&lt;p&gt;나머지 다른 4개의 task에 대해서는 모두 Morpheme-aware Subword의 64K model이 가장 좋은 성능을 달성했다. tokenization 방식에 관계 없이 모두 다 vocabulary size와 score가 대체로 비례 관계를 보였다. 그러나 위에서 진행했던 NMT task에 있어서는 Morpheme-aware Subword에서의 높은 vocabulary size가 좋은 성능을 보장하지는 않았는데, 다소 배치되는 결과이다.&lt;/p&gt;

&lt;h1 id=&quot;discussion&quot;&gt;Discussion&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-10-10-An-Empirical-Study-of-Tokenization-Strategies-for-Various-Korean-NLP-Tasks/07.jpg&quot; alt=&quot;07.jpg&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;token-length&quot;&gt;Token Length&lt;/h2&gt;

&lt;p&gt;token의 길이가 성능에 얼마나 영향을 미치는지 알아본다. token length는 한 token에 포함된 음절 개수의 평균으로 정의한다. CV의 경우에는 자모 단위이기 때문에 평균 token length는 0.33~0.5 사이의 값이다. 한 음절은 2~3개의 자모로 구성되어 있기 때문이다. Syllable은 음절 단위로 tokenizing을 한 것이기 때문에 평균 token length는 1의 고정 값을 갖는다. Morpheme는 형태소 단위로 tokenizing을 수행한 것이기 때문에 평균 token length가 vocabulary size에 따라 변하지 않고 일정하다. Subword나 Morpheme-aware-Subword는 모두 BPE를 사용하는 방식이기 때문에 vocabulary size가 증가할수록 token length도 증가하게 된다. 통계적인 빈도를 기반으로 vocabulary size에 따라 상위 N개를 pair로 묶기 때문이다. 위의 figure에는 word model이 누락됐는데, word model은 Ko-En과 En-Ko에서 각각 7.07, 18.42로 매우 낮은 Score를 보여줘 공간상의 제약으로 figure에서 제외했다.&lt;/p&gt;

&lt;p&gt;Figure 1을 분석해보자. 자모 단위로 tokenizing을 수행한 CV의 성능이 기준점이다. 대부분의 model은 평균 token length가 1.0~1.5인 구간에서 가장 좋은 성능을 보여준다. 평균 token length가 1.5를 넘어가기 시작하면서 점차 감소하는 경향을 보인다. 특히 평균 token length가 2.5에 달하는 word model의 경우에는 최악의 성능을 보여줬다.&lt;/p&gt;

&lt;h2 id=&quot;linguistic-awareness&quot;&gt;Linguistic Awareness&lt;/h2&gt;

&lt;p&gt;Figure 1에서 8K Subword model과 16K Morpheme-aware Subword model을 비교해보자. figure에서 파란 색 배경으로 강조 표시가 된 부분이다. 두 model은 평균 token length가 동일한 값이다. 두 model의 차이는 언어론적 지식을 사용했는가(형태소 경계를 넘어서는 pair를 생성했는가)에 있다. Ko-En과 En-Ko 두 task에서 모두 Morpheme-aware Subword model이 더 좋은 성능을 보여줬다는 것은 token length뿐만 아니라 linguistic awareness도 tokenization 전략 수립에 매우 중요한 factor라는 것을 보여준다.&lt;/p&gt;

&lt;h2 id=&quot;under-trained-tokens&quot;&gt;Under-trained Tokens&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-10-10-An-Empirical-Study-of-Tokenization-Strategies-for-Various-Korean-NLP-Tasks/08.jpg&quot; alt=&quot;08.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 1에서 Morpheme model의 경우에만 예외적으로 CV보다 훨씬 못한 성능을 보여준다. 이러한 결과는 Morpheme model의 높은 OOV rate에서 비롯된다. 위의 Experiments에서 살펴본 NMT task에서의 result table을 확인해보면 Morpheme model의 OOV rate가 압도적으로 높다는 것을 확인할 수 있다(본 논의에서는 모든 task에서 최악의 성능을 보여줬던  Word model은 배제한다). OOV는 정의하자면 test set에서만 등장하고, train set에서는 등장하지 않았던 token을 의미한다. 즉, OOV rate가 높다는 것은 model 입장에서는 처음 보는 token이 test set에서 등장하는 비율을 의미한다. 완전히 처음 마주하는 token이 아닌 적게 마주한 token들의 비율에 대해서도 확인을 해보자. OOV가 아니라 하더라도 등장 빈도가 확연히 적은 token들에 대해서는 model이 under-train했을 가능성이 농후하기 때문이다. Figure 2에서는 실제로 등장 빈도가 낮은 token의 비중이 얼마나 되는지를 시각화 한 graph이다.  예상했던 바와 같이 OOV rate가 높은 Morpheme model이 훨씬 더 높은 수치를 보여준다는 것을 확인할 수 있다. 이는 결국 Morpheme model이 under-trained된 token의 비중이 높다는 것을 의미한다. 이러한 이유로 Morpheme model이 타 model 대비 확연히 낮은 성능을 보이는 것이다.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;여러 Korean NLP task에 대해서 다양한 tokenization 전략을 사용한 model들의 성능을 비교했다. Korean-English NMT task에서는 BPE에 언어론적 특성(형태소)를 더한 Morpheme-aware Subword Model이 가장 높은 성능을 보여줬다. NLU task의 KorQuAD를 제외한 모든 task에서 역시 Morpheme-aware Subword Model이 가장 좋은 수치를 달성했다. 이를 통해 각 language의 unique한 linguistic awareness가 model 성능 향상에 매우 큰 영향을 미친다는 사실을 도출해냈다.&lt;/p&gt;</content><category term="NLP" /><category term="Korean" /><summary type="html">Paper Info</summary></entry></feed>

<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Hansu Kim's Dev Blog</title>
    <description>Hansu Kim's Dev Blog</description>
    <link>http://0.0.0.0:4000/</link>
    <atom:link href="http://0.0.0.0:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Tue, 19 Jan 2021 00:29:14 -0600</pubDate>
    <lastBuildDate>Tue, 19 Jan 2021 00:29:14 -0600</lastBuildDate>
    <generator>Jekyll v4.2.0</generator>
    
      <item>
        <title>Neural Machine Translation of Rare Words with Subword Units</title>
        <description>&lt;p&gt;Archive Link: https://arxiv.org/abs/1508.07909
Created: Sep 21, 2020 3:12 PM
Field: NLP
Paper Link: https://arxiv.org/pdf/1508.07909.pdf
Status: completed
Submit Date: Aug 15, 2015&lt;/p&gt;

&lt;h1 id=&quot;backgrounds&quot;&gt;Backgrounds&lt;/h1&gt;

&lt;h2 id=&quot;bleu-score-bilingual-evaluation-understudy-score&quot;&gt;BLEU Score (Bilingual Evaluation Understudy) score&lt;/h2&gt;

\[BLEU=min\left(1,\frac{\text{output length}}{\text{reference_length}}\right)\left(\prod_{i=1}^4precision_i\right)^{\frac{1}{4}}\]

&lt;p&gt;reference sentence와 output sentence의 일치율을 나타내는 score이다. 3단계 절차를 거쳐 최종 BLEU Score를 도출해낸다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;n-gram에서 순서쌍의 겹치는 정도 (Precision)
    &lt;ul&gt;
      &lt;li&gt;Example
        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;output sentence&lt;/p&gt;

            &lt;p&gt;&lt;strong&gt;빛이 쐬는&lt;/strong&gt; 노인은 &lt;strong&gt;완벽한&lt;/strong&gt; 어두운 곳에서 &lt;strong&gt;잠든 사람과 비교할 때&lt;/strong&gt; 강박증이 &lt;strong&gt;심해질&lt;/strong&gt; 기회가 &lt;strong&gt;훨씬 높았다&lt;/strong&gt;&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;true sentence&lt;/p&gt;

            &lt;p&gt;&lt;strong&gt;빛이 쐬는&lt;/strong&gt; 사람은 &lt;strong&gt;완벽한&lt;/strong&gt; 어둠에서 &lt;strong&gt;잠든 사람과 비교할 때&lt;/strong&gt; 우울증이 &lt;strong&gt;심해질&lt;/strong&gt; 가능성이 &lt;strong&gt;훨씬 높았다&lt;/strong&gt;&lt;/p&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;1-gram precision&lt;/p&gt;

\[\frac{\text{\# of correct 1-gram in output sentence}}{\text{all 1-gram pair in output sentence}}=\frac{10}{14}\]
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;2-gram precision&lt;/p&gt;

\[\frac{\text{\# of correct 2-gram in output sentence}}{\text{all 2-gram pair in output sentence}}=\frac{5}{13}\]
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;3-gram precision&lt;/p&gt;

\[\frac{\text{\# of correct 3-gram in output sentence}}{\text{all 3-gram pair in output sentence}}=\frac{2}{12}\]
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;4-gram precision&lt;/p&gt;

\[\frac{\text{\# of correct 4-gram in output sentence}}{\text{all 4-gram pair in output sentence}}=\frac{1}{11}\]
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;같은 단어에 대한 보정 (Clipping)
    &lt;ul&gt;
      &lt;li&gt;Example
        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;output sentence&lt;/p&gt;

            &lt;p&gt;&lt;strong&gt;The more&lt;/strong&gt; decomposition &lt;strong&gt;the more&lt;/strong&gt; flavor &lt;strong&gt;the&lt;/strong&gt; food has&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;true sentence&lt;/p&gt;

            &lt;p&gt;&lt;strong&gt;The more the&lt;/strong&gt; merrier I always say&lt;/p&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;1-gram precision&lt;/p&gt;

\[\frac{\text{\# of 1-gram in output sentence}}{\text{all 1-gram pair in output sentence}}=\frac{5}{9}\]
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Clipping 1-gram precision&lt;/p&gt;

\[\frac{\text{\# of 1-gram in output sentence}}{\text{all 1-gram pair in output sentence}}=\frac{3}{9}\]
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;문장 길이에 대한 보정 (Brevity Penalty)
    &lt;ul&gt;
      &lt;li&gt;Example
        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;output sentence&lt;/p&gt;

            &lt;p&gt;&lt;strong&gt;빛이 쐬는&lt;/strong&gt; 노인은 &lt;strong&gt;완벽한&lt;/strong&gt; 어두운 곳에서 잠듬&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;true sentence&lt;/p&gt;

            &lt;p&gt;&lt;strong&gt;빛이 쐬는&lt;/strong&gt; 사람은 &lt;strong&gt;완벽한&lt;/strong&gt; 어둠에서 잠든 사람과 비교할 때 우울증이 심해질 가능성이 훨씬 높았다&lt;/p&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;brevity penalty&lt;/p&gt;

\[min\left(1,\frac{\text{\# of words in output sentence}}{\text{\# of words in true sentence}}\right)=min\left(1,\frac{6}{14}\right)=\frac{3}{7}\]
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;최종 BLEU Score
    &lt;ul&gt;
      &lt;li&gt;Example
        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;output sentence&lt;/p&gt;

            &lt;p&gt;&lt;strong&gt;빛이 쐬는&lt;/strong&gt; 노인은 완벽한 어두운 곳에서 &lt;strong&gt;잠든 사람과 비교할 때&lt;/strong&gt; 강박증이 &lt;strong&gt;심해질&lt;/strong&gt; 기회가 &lt;strong&gt;훨씬 높았다&lt;/strong&gt;&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;true sentence&lt;/p&gt;

            &lt;p&gt;&lt;strong&gt;빛이 쐬는&lt;/strong&gt; 사람은 &lt;strong&gt;완벽한&lt;/strong&gt; 어둠에서 &lt;strong&gt;잠든 사람과 비교할 때&lt;/strong&gt; 우울증이 &lt;strong&gt;심해질&lt;/strong&gt; 가능성이 &lt;strong&gt;훨씬 높았다&lt;/strong&gt;&lt;/p&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;BLEU Score&lt;/p&gt;

\[BLEU=min\left(1,\frac{\text{output length}}{\text{reference length}}\right)\left(\prod_{i=1}^4precision_i\right)^{\frac{1}{4}}\\=min\left(1,\frac{14}{14}\right)\times\left(\frac{10}{14}\times\frac{5}{13}\times\frac{2}{12}\times\frac{1}{11}\right)^{\frac{1}{4}}\]
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;출처: &lt;a href=&quot;https://donghwa-kim.github.io/BLEU.html&quot;&gt;https://donghwa-kim.github.io/BLEU.html&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;

&lt;p&gt;기존의 NMT (Neural machine translation)는 모두 고정된 개수의 vocabulary 안에서 작업했다. 하지만 translation은 vocabulary 개수의 제한이 없는 open-vocabulary problem이기 OOV(out of vocabulary) word가 많이 발생할 수밖에 없다. 본 논문에서는 이러한 OOV 문제를 subword unit 활용해 해결하고자 했다.&lt;/p&gt;

&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;기존의 NMT Model은 OOV words에 대해 back-off model 사용해왔다. back-off model 대신 본 논문에서 제시할 subword unit을 사용할 경우 OOV 문제를 더 확실히 해결해 open-vocabulary problem에서 성능 향상을 이끌어낼 수 있다.&lt;/p&gt;

&lt;h1 id=&quot;subword-translation&quot;&gt;Subword Translation&lt;/h1&gt;

&lt;p&gt;현재의 language model에서 translatable하지 않더라도, 다른 language의 translation의 sub word를 사용하면 translate이 가능하다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;이름 등의 고유 명사는 음절 별로 대응시킨다.
    &lt;ul&gt;
      &lt;li&gt;Barack Obama (English; German)&lt;/li&gt;
      &lt;li&gt;Барак Обама (Russian)&lt;/li&gt;
      &lt;li&gt;バラク・オバマ (ba-ra-ku o-ba-ma) (Japanese)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;동의어, 외래어 등 같은 origin을 갖는 단어들은 일정한 규칙을 갖고 변형되므로, character-level translation 사용한다.
    &lt;ul&gt;
      &lt;li&gt;claustrophobia (English)&lt;/li&gt;
      &lt;li&gt;Klaustrophobie (German)&lt;/li&gt;
      &lt;li&gt;Клаустрофобия (Klaustrofobiâ) (Russian)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;복합어는 각각의 sub-word를 번역한 후 결합한다.
    &lt;ul&gt;
      &lt;li&gt;solar system (English)&lt;/li&gt;
      &lt;li&gt;Sonnensystem (Sonne + System) (German)&lt;/li&gt;
      &lt;li&gt;Naprendszer (Nap + Rendszer) (Hungarian)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;위와 같은 규칙으로 german training data에서 가장 빈도 낮은 100개의 word를 분석하면 english data를 통해 56개의 복합어, 21개의 고유명사, 6개의 외래어 등을 찾아낼 수 있었다.&lt;/p&gt;

&lt;h2 id=&quot;related-work&quot;&gt;Related Work&lt;/h2&gt;

&lt;p&gt;OOV는 고유명사 (사람 이름, 지역명), 외래어 등에 대해서 자주 발생한다. 이를 해결하기 위해 character level로 word를 분리한 뒤, 각 character들이 일정한 기준을 충족할 경우 하나의 token으로 묶어 표현하는 방식을 채택했다. 이를 통해 text size는 줄어들게 된다. 이 때 단어를 subword로 구분하는 기존의 Segmentation algorithm을 사용하되,  좀 더 aggressive한 기준을 적용하고자 했다. vocabulary size와 text size는 서로 trade-off 관계이므로 vocabulary size가 감소한다면 시간/공간 복잡도는 낮아지겠지만  unknown word의 개수가 증가하게 된다.&lt;/p&gt;

&lt;h2 id=&quot;byte-pair-encoding-bpe&quot;&gt;Byte Pair Encoding (BPE)&lt;/h2&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;re&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;collections&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;get_stats&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vocab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;pairs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;collections&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;defaultdict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;word&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;freq&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vocab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;items&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;symbols&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;word&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;symbols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;pairs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;symbols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;symbols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;freq&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pairs&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;merge_vocab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pair&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;v_out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{}&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;bigram&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;re&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;escape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;' '&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pair&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;re&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;compile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'(?&amp;lt;!\S)'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bigram&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;sa&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'(?!\S)'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;word&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;w_out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sub&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;''&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pair&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;word&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;v_out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w_out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;word&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v_out&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;vocab&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'low&amp;lt;/w&amp;gt;'&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'lower&amp;lt;/w&amp;gt;'&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
				 &lt;span class=&quot;s&quot;&gt;'newest&amp;lt;/w&amp;gt;'&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'widest&amp;lt;/w&amp;gt;'&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;num_merges&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_merges&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;pairs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get_stats&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vocab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;best&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pairs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pairs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;vocab&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;merge_vocab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;best&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vocab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;best&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# r .  -&amp;gt;  r.
# l o  -&amp;gt;  lo
# lo w -&amp;gt;  low
# e r. -&amp;gt;  er.
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;BPE는 가장 빈도가 높은 pair of bytes부터 하나의 single byte로 치환해 저장하는 압축 algorithm이다.&lt;/p&gt;

&lt;p&gt;BPE는 다음과 같은 과정을 따른다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;word를 character의 sequence로 변환 후 end symbol  ·  추가&lt;/li&gt;
  &lt;li&gt;모든 character의 pair를 센 후 가장 빈도가 높은 pair of character (‘A’, ‘B’)를 새로운 symbol ‘AB’ (character n-gram)로 치환&lt;/li&gt;
  &lt;li&gt;2번 단계를 원하는 횟수만큼(vocabulary size만큼 token이 생성될 때까지) 반복&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;BPE의 반복 횟수는 vocabulary size라는 hyperparameter에 따라 결정된다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;예시
    &lt;ol&gt;
      &lt;li&gt;
        &lt;p&gt;train sentences&lt;/p&gt;

        &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; &lt;span class=&quot;n&quot;&gt;sentence&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt; 
 &lt;span class=&quot;s&quot;&gt;'black bug bit a black bear but is the black bear that the big black bug bit'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
 &lt;span class=&quot;s&quot;&gt;'a big bug bit the little beetle but the little beetle bit the big bug back'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;
 &lt;span class=&quot;s&quot;&gt;'the better with the butter is the batter that is better'&lt;/span&gt;
 &lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;        &lt;/div&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;count segments&lt;/p&gt;

        &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; &lt;span class=&quot;p&quot;&gt;[(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'t h e &amp;lt;/w&amp;gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'b l a c k &amp;lt;/w&amp;gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'b i t &amp;lt;/w&amp;gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'i s &amp;lt;/w&amp;gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'b i g &amp;lt;/w&amp;gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'b e a r &amp;lt;/w&amp;gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'b u t &amp;lt;/w&amp;gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'t h a t &amp;lt;/w&amp;gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'l i t t l e &amp;lt;/w&amp;gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'b e e t l e &amp;lt;/w&amp;gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'b e t t e r &amp;lt;/w&amp;gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'b a c k &amp;lt;/w&amp;gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'w i t h &amp;lt;/w&amp;gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'b u t t e r &amp;lt;/w&amp;gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'b a t t e r &amp;lt;/w&amp;gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;        &lt;/div&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;count bi-grams&lt;/p&gt;

        &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; &lt;span class=&quot;p&quot;&gt;[((&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'t'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'h'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;11&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'h'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'e'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'t'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'&amp;lt;/w&amp;gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'g'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'&amp;lt;/w&amp;gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;        &lt;/div&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;add merge-rules&lt;/p&gt;

        &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'t'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'h'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;th&lt;/span&gt;
 &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'h'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'e'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;he&lt;/span&gt;
 &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'t'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'&amp;lt;/w&amp;gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;
 &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'g'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'&amp;lt;/w&amp;gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;        &lt;/div&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;evaluation&quot;&gt;Evaluation&lt;/h1&gt;

&lt;h2 id=&quot;subword-statistics&quot;&gt;Subword statistics&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-Neural-Machine-Translation-of-Rare-Words-with-Subword-Units/05-09-2020-21.10.22.jpg&quot; alt=&quot;Neural%20Machine%20Translation%20of%20Rare%20Words%20with%20Subw%203301351401254a21af391ffcd056405b/05-09-2020-21.10.22.jpg&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;h1 id=&quot;tokens-text-size&quot;&gt;tokens: text size&lt;/h1&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;h1 id=&quot;types-vocabulary-size-token-개수&quot;&gt;types: vocabulary size, token 개수&lt;/h1&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;h1 id=&quot;unk-unknown-word-oov-word의-개수&quot;&gt;UNK: unknown word (OOV word)의 개수&lt;/h1&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;translation-experiments&quot;&gt;Translation experiments&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-Neural-Machine-Translation-of-Rare-Words-with-Subword-Units/05-09-2020-21.22.15.jpg&quot; alt=&quot;Neural%20Machine%20Translation%20of%20Rare%20Words%20with%20Subw%203301351401254a21af391ffcd056405b/05-09-2020-21.22.15.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-Neural-Machine-Translation-of-Rare-Words-with-Subword-Units/05-09-2020-21.24.02.jpg&quot; alt=&quot;Neural%20Machine%20Translation%20of%20Rare%20Words%20with%20Subw%203301351401254a21af391ffcd056405b/05-09-2020-21.24.02.jpg&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;W Unk: back-off dictionary를 사용하지 않은 model이다.&lt;/li&gt;
  &lt;li&gt;W Dict: back-off dictionary를 사용한 model이다.&lt;/li&gt;
  &lt;li&gt;C2-50k: char-bigram을 사용한 model이다.&lt;/li&gt;
  &lt;li&gt;CHR F3: 인간의 판단과 일치율&lt;/li&gt;
  &lt;li&gt;unigram F1: BLEU unigram(brevity penalty 제외)와 Recall의 조합&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;source와 target 각각 따로 BPE를 수행하는 BPE보다 동시에 수행하는 BPE joint가 더 좋은 성능을 보였다.&lt;/p&gt;

&lt;h1 id=&quot;analysis&quot;&gt;Analysis&lt;/h1&gt;

&lt;h2 id=&quot;unigram-accuracy&quot;&gt;Unigram accuracy&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-Neural-Machine-Translation-of-Rare-Words-with-Subword-Units/05-09-2020-22.04.35.jpg&quot; alt=&quot;Neural%20Machine%20Translation%20of%20Rare%20Words%20with%20Subw%203301351401254a21af391ffcd056405b/05-09-2020-22.04.35.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-Neural-Machine-Translation-of-Rare-Words-with-Subword-Units/05-09-2020-23.20.17.jpg&quot; alt=&quot;Neural%20Machine%20Translation%20of%20Rare%20Words%20with%20Subw%203301351401254a21af391ffcd056405b/05-09-2020-23.20.17.jpg&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;manual-analysis&quot;&gt;Manual Analysis&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-Neural-Machine-Translation-of-Rare-Words-with-Subword-Units/12-01-2020-01.37.29.jpg&quot; alt=&quot;Neural%20Machine%20Translation%20of%20Rare%20Words%20with%20Subw%203301351401254a21af391ffcd056405b/12-01-2020-01.37.29.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-Neural-Machine-Translation-of-Rare-Words-with-Subword-Units/12-01-2020-01.37.33.jpg&quot; alt=&quot;Neural%20Machine%20Translation%20of%20Rare%20Words%20with%20Subw%203301351401254a21af391ffcd056405b/12-01-2020-01.37.33.jpg&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;OOV 문제를 해결해 NMT와 같은 open-vocabulary translation에서 좋은 성능을 보였다. 기존에 OOV를 해결하기 위해 사용되던 back-off translation model보다 더 좋은 성능을 보였다.&lt;/p&gt;
</description>
        <pubDate>Tue, 19 Jan 2021 05:58:20 -0600</pubDate>
        <link>http://0.0.0.0:4000/paper%20review/Neural-Machine-Translation-of-Rare-Words-with-Subword-Units/</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/paper%20review/Neural-Machine-Translation-of-Rare-Words-with-Subword-Units/</guid>
        
        <category>NLP</category>
        
        
        <category>Paper Review</category>
        
      </item>
    
      <item>
        <title>test title v4</title>
        <description>\[\sum^N_{i=1}i\]
</description>
        <pubDate>Tue, 19 Jan 2021 00:00:00 -0600</pubDate>
        <link>http://0.0.0.0:4000/test/</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/test/</guid>
        
        
      </item>
    
      <item>
        <title>Xlnet Generalized Autoregressive Pretraining For Language Understanding</title>
        <description>&lt;h1 id=&quot;xlnet-generalized-autoregressive-pretraining-for-language-understanding&quot;&gt;XLNet: Generalized Autoregressive Pretraining for Language Understanding&lt;/h1&gt;
&lt;p&gt;title: XLNet: Generalized Autoregressive Pretraining for Language Understanding
subtitle: XLNet
categories: Paper Review
tags: NLP
date: 2021-01-19 12:59:48 +0000
last_modified_at: 2021-01-19 12:59:48 +0000
—&lt;/p&gt;

&lt;p&gt;Archive Link: https://arxiv.org/abs/1906.08237
Created: Sep 21, 2020 3:18 PM
Field: NLP
Paper Link: https://arxiv.org/pdf/1906.08237.pdf
Status: not checked
Submit Date: Jun 19, 2019&lt;/p&gt;

&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;Unsupervised Learning을 pretraining에 적용시키는 방식은 NLP domain에서 매우 큰 성과를 이뤄냈다. Unsupervised pretraining하는 방법론은 크게 AutoRegressive(AR)과 AutoEncoding(AE)가 있다. AutoRegressive는 순방향 또는 역방향으로 다음의 corpus를 예측하는 방식으로 학습한다. 이는 단방향 context만 학습할 수 있다는 단점이 있다. 하지만 현실의 대부분의 downstream task는 bidirectional context가 필수적이기에 이는 크나큰 한계가 된다.&lt;/p&gt;

&lt;p&gt;반면 AutoEncoding은 변형된 input을 다시 본래의 input으로 재구성하는 방식이다. BERT가 대표적인 예시인데, input data의 일부분을 [MASK] token 등으로 변화를 준 뒤, 원래의 input을 만들어내도록 학습시킨다. 이러한 방법은 bidirectional context를 학습할 수 있다는 점에서 AutoRegressive에 비해 상대적으로 좋은 성능을 보인다.하지만 인위적인 변형을 가해 만들어낸 [MASK] token 등은 pretraining 과정에서만 존재하는 token이고, 이후 downstream task를 학습시키는 fine-tuning 과정에서는 존재하지 않는 token이 된다. 따라서 pre-training과 fine-tuning 사이의 괴리가 발생하게 된다. 또한 각 [MASK] token을 predict하는 과정은 independent하기 때문에 predict된 token들 사이의 dependency는 학습할 수 없다는 한계도 있다.&lt;/p&gt;

&lt;p&gt;본 논문에서 제시하는 XLNet은 이러한 AR과 AE을 모두 사용해 각각의 장점만을 취하도록 했다.&lt;/p&gt;

&lt;h1 id=&quot;proposed-method&quot;&gt;Proposed Method&lt;/h1&gt;

&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;

&lt;h3 id=&quot;ar-autoregressive&quot;&gt;AR (Autoregressive)&lt;/h3&gt;

&lt;p&gt;일반적인 AR의 objective function은 다음과 같다.&lt;/p&gt;

\[\underset{\theta}{max}\ \ log{\ p_\theta(x)} = \sum_{t=1}^Tlog{\ p_\theta\left(x_t|x_{&amp;lt;t}\right)} = \sum_{t=1}^Tlog{\ \frac{exp\left(h_\theta\left(x_{1:t-1}\right)^Te\left(x_t\right)\right)}{\sum_{x'}{exp\left(h_\theta\left(x_{1:t-1}\right)^Te\left(x'\right)\right)}}}\]

&lt;p&gt;\(h_\theta\left(x_{1:t-1}\right)\)는 model의 context representation이고, \(e\left(x'\right)\)는 x의 embedding이다.&lt;/p&gt;

&lt;h3 id=&quot;ae-autoencoding&quot;&gt;AE (Autoencoding)&lt;/h3&gt;

&lt;p&gt;일반적인 AE의 objective function은 다음과 같다.&lt;/p&gt;

\[\underset\theta{max}\ log{\ p_\theta\left(\bar{x}|\hat{x}\right)} \approx \sum_{t=1}^T{m_tlog{\ p_\theta\left(x_t|\hat{x}\right)}} = \sum_{t=1}^T{m_tlog{\ \frac{exp\left(H_\theta\left(\hat{x}\right)_t^Te\left(x_t\right)\right)}{\sum_{x'}{exp\left(H_\theta\left(\hat{x}\right)_t^Te\left(x'\right)\right)}}}}\]

&lt;p&gt;\(\hat{x}\)는 [MASK] token 등이 추가된 변형된 input이고, \(\bar{x}\)는 masked token이다.&lt;/p&gt;

&lt;p&gt;\(m_t=1\)인 경우 \(x_t\)가 masked된 경우를 뜻하고, \(H_\theta\)는 Transformer의 hidden vector를 뜻한다.&lt;/p&gt;

&lt;h3 id=&quot;xlnet&quot;&gt;XLNet&lt;/h3&gt;

&lt;p&gt;XLNet은 AR와 AE를 아래의 3가지 관점에서 비교하며 각각의 장점만 취한다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Independence Assumption&lt;/p&gt;

    &lt;p&gt;AE의 objective function은 조건부확률을 계산하는 것이다. 이 때 \(\approx\)를 사용한다. 이는 모든 \(\bar{x}\)에 대한 reconstruction이 independent하게 이루어진다는 가정 하에 이루어지기 때문이다. 반면 AR의 objective function은 이러한 가정 없이도 성립하기에 \(=\)를 사용한다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Input Noise&lt;/p&gt;

    &lt;p&gt;AE에서는 [MASK] token과 같이 실제 input에 없던 token들이 추가되게 된다. 이는 pretraining 때에만 존재하는 token으로 fine-tuning 과정에서는 존재하지 않는다. 이러한 pretraining과 fine-tuning 사이의 괴리를 해결하기 위해 BERT에서는 masking에 대해 모두 [MASK] token으로 변경하지 않고 일부분은 original token 그대로 두는 등의 기법을 사용했으나, 이는 전체 token에서 극히 일부분에만 적용되기 때문에 (0.15 * 0.1 == 0.015) 의미있는 결과를 도출해내지 못한다. AR에서는 input에 대한 변경이 없기 때문에 이러한 문제가 발생하지 않는다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Context Dependency&lt;/p&gt;

    &lt;p&gt;AE는 bidirectional context를 모두 학습할 수 있지만, AR은 unidirectional context만 학습한다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;objective-permutation-language-modeling&quot;&gt;Objective: Permutation Language Modeling&lt;/h2&gt;

&lt;p&gt;AR의 장점은 모두 취하면서(no Indepence Assumption, no Input Noise) AR의 단점은 해결하는(Bidirectional Context) Objective function을 정의하기로 한다.&lt;/p&gt;

\[\underset{\theta}{max} = E_{z\thicksim Z_T}\left[\sum_{t=1}^T{log{\ p_\theta\left(x_{z_t}|x_{z_{&amp;lt;t}}\right)}}\right]\]

&lt;p&gt;\(Z_T\)는 길이가 \(T\)인 sequence의 모든 순열 집합 을 뜻하고, \(z_t\)는 \(Z_T\)에서 \(t\)번째 element를 뜻한다. \(z_{&amp;lt;t}\)는 \(Z_T\)에서 \(0\) ~ \(t-1\)번째 원소들을 뜻한다.&lt;/p&gt;

&lt;p&gt;위의 Objective Function은 \(x_i\)에 대해 \(x_i\)를 제외한 모든 \(x_t\)를 전체 집합으로 하는 순열에 대해 likelihood를 구하게 된다. AR의 구조를 채택했으나 순열을 사용해 bidirectional context까지 학습하도록 한 것이다.&lt;/p&gt;

&lt;h2 id=&quot;architecture-two-stream-self-attention-for-target-aware-representations&quot;&gt;Architecture: Two-Stream Self-Attention for Target-Aware Representations&lt;/h2&gt;

&lt;p&gt;일반적인 Transformer의 Self-Attention 구조에서는 Query, Key, Value가 모두 같은 값으로 시작하게 된다. 즉, 하나의 hidden state의 값을 공유한다. 그러나 XLNet에서는 구조상 Query의 값과 Key, Value의 값이 분리되어야 한다. 이를 위해 새로운 representation을 추가하게 된다.&lt;/p&gt;

&lt;p&gt;구체적인 예시를 들어보자. \(T = 4\)일 때, 두가지의 순열이 선택되었다고 하자.&lt;/p&gt;

\[Z_1 = [x_2,x_3,x_1,x_4]\]

\[Z_2 = [x_2,x_3,x_4,x_1]\]

&lt;p&gt;\(Z_1\)에서 \(t=3\)에 대한 조건부 확률을 구하는 식은 다음과 같다.&lt;/p&gt;

\[p\left(x_1|x_{z_{&amp;lt;3}}\right) =p\left(x_1|x_2,x_3\right)=\frac{exp\left(e\left(x_1\right)^Th_\theta\left(x_2,x_3\right)\right)}{\sum_{x'}{exp\left(e\left(x'\right)^Th_\theta\left(x_2,x_3\right)\right)}}\]

&lt;p&gt;\(Z_2\)에서 \(t=3\)에 대한 조건부 확률을 구하는 식은 다음과 같다.&lt;/p&gt;

\[p\left(x_4|x_{z_{&amp;lt;3}}\right) =p\left(x_4|x_2,x_3\right)=\frac{exp\left(e\left(x_4\right)^Th_\theta\left(x_2,x_3\right)\right)}{\sum_{x'}{exp\left(e\left(x'\right)^Th_\theta\left(x_2,x_3\right)\right)}}\]

&lt;p&gt;위의 두 조건부확률 식은 분모는 완전히 같은 값이다. 만약 \(x_1\)과 \(x_4\)가 같은 word였다고 한다면 (a, an, the와 같은 관사 등) 완전히 같은 조건부 확률을 계산하는 상황이 발생하게 된다. 직전 시점 \(t-1\)까지의 정보 embedding 정보만을 저장하는 representation \(h_\theta\left(x_{z_{&amp;lt;t}}\right)\)만으로는 이러한 문제를 해결할 수 없다. 따라서 현재 시점의 위치정보 까지 받는 새로운 representation \(g_\theta\left(x_{z_{&amp;lt;t}},z_t\right)\)을 추가한다.  최종적으로 아래의 수식을 정의하게 된다.&lt;/p&gt;

\[p\left(X_{z_t}=x|x_{z_{&amp;lt;t}}\right) =\frac{exp\left(e\left(x\right)^Tg_\theta\left(x_{z&amp;lt;t},z_t\right)\right)}{\sum_{x'}{exp\left(e\left(x'\right)^Tg_\theta\left(x_{z&amp;lt;t},z_t\right)\right)}}\]

&lt;p&gt;두 representation에 대해 자세히 알아보자.&lt;/p&gt;

&lt;h3 id=&quot;content-representation&quot;&gt;Content Representation&lt;/h3&gt;

&lt;p&gt;\(h_\theta\left(x_{z&amp;lt;t}\right)\)는 기존 Transformer의 hidden state와 동일한 구조이다. 현재 시점(\(t\))의 정보까지 포함해 입력으로 받는다. 이를 Content Representation이라고 하고, Key, Value에 사용하게 된다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-XLNet-Generalized-Autoregressive-Pretraining-for-Language-Understanding/08-29-2020-18.49.21.jpg&quot; alt=&quot;XLNet%20Generalized%20Autoregressive%20Pretraining%20for%20L%20833f510b35954da883906c9bc6b15f9d/08-29-2020-18.49.21.jpg&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;query-representation&quot;&gt;Query Representation&lt;/h3&gt;

&lt;p&gt;\(g_\theta\left(x_{z_{&amp;lt;t}},z_t\right)\)는 현재 시점(\(t\))의 정보는 제외하고 입력으로 받는다. 대신 현재 시점(\(t\))의 위치 정보(\(z_t\))는 입력으로 받는다. 이를 Query Representation이라고 하고, Query에 사용하게 된다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-XLNet-Generalized-Autoregressive-Pretraining-for-Language-Understanding/08-29-2020-18.49.27.jpg&quot; alt=&quot;XLNet%20Generalized%20Autoregressive%20Pretraining%20for%20L%20833f510b35954da883906c9bc6b15f9d/08-29-2020-18.49.27.jpg&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;permutation-language-modeling-with-two-stream-attention&quot;&gt;Permutation Language Modeling with Two-Stream Attention&lt;/h3&gt;

&lt;p&gt;전체적인 Two-Stream Attention의 구조는 아래와 같다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-XLNet-Generalized-Autoregressive-Pretraining-for-Language-Understanding/08-29-2020-18.52.01.jpg&quot; alt=&quot;XLNet%20Generalized%20Autoregressive%20Pretraining%20for%20L%20833f510b35954da883906c9bc6b15f9d/08-29-2020-18.52.01.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Query의 초기값은 weight \(w\), Key와 Value의 초기 값은 embedding된 input 값 \(e\left(x_i\right)\)이다. 이후 아래와 같이 갱신된다.&lt;/p&gt;

&lt;p&gt;Query Stream은 현재 시점 \(t\)의 위치 정보(\(z_t\))는 알 수 있지만, 실제 값(\(x_{z_t}\))는 알지 못하는 상태로 구해진다.&lt;/p&gt;

&lt;p&gt;Content Stream은 현재 시점 \(t\)의 위치 정보(\(z_t\))는 물론, 실제 값(\(x_{z_t}\))도 사용해 구해진다.&lt;/p&gt;

\[g_{z_t}^{\left(m\right)} = Attention\left(Q=g_t^{\left(m-1\right)},KV=h_{z_{&amp;lt;t}}^{\left(m-1\right)};\theta\right)\]

\[h_{z_t}^{\left(m\right)}=Attention\left(Q=h_{z_t}^{\left(m-1\right)},KV=h_{z_{z\leq t}}^{\left(m-1\right)};\theta\right)\]

&lt;p&gt;\(m\)은 Multi-head Atention Layer의 현재 Layer Number이다.&lt;/p&gt;

&lt;h2 id=&quot;incorporating-ideas-from-transformer-xl&quot;&gt;Incorporating Ideas from Transformer-XL&lt;/h2&gt;

&lt;h2 id=&quot;modeling-multiple-segments&quot;&gt;Modeling Multiple Segments&lt;/h2&gt;

&lt;p&gt;BERT의 input과 동일한 구조를 채택했다. [CLS, A, SEP, B, SEP]의 구조이다. [CLS], [SEP] token은 BERT와 동일한 역할이고, [A], [B]는 각각 sentence A, sentence B이다. BERT와의 차이점은 NSP (Next Sentence Predict)를 Pretraining에 적용하지 않은 것인데, 유의미한 성능 향상이 없었기 때문이라고 한다.&lt;/p&gt;

&lt;h3 id=&quot;relative-segment-encodings&quot;&gt;Relative Segment Encodings&lt;/h3&gt;

&lt;p&gt;BERT의 segment embedding은 \(S_A\)와 \(S_B\) 등으로 \(A\)문장인지, \(B\)문장인지를 드러냈다. XLNet에서는 Transformer-XL의 relative positional encoding의 idea를 segment에도 적용해 relative한 값으로 표현했다. XLNet의 Segment Encoding은 두 position \(i, j\)가 같은 segment라면 \(s_+\), 다른 segment라면 \(s_-\)로 정의된다. \(s_+\)와 \(s_-\)는 모두 training 과정에서 학습되는 parameters이다. 이러한 relative segment encoding은 재귀적으로 segment encoding을 찾아내면서 generalization된 표현이 가능하다는 점, 두 개 이상의 segment input에 대한 처리 가능성을 열었다는 점에서 의의가 있다.&lt;/p&gt;

&lt;h2 id=&quot;discussion&quot;&gt;Discussion&lt;/h2&gt;

&lt;p&gt;구체적인 예시를 들어 BERT와 비교해보자. BERT와 XLNet이 “New York is a city.”라는 문장을 pretraining하는 상황이다. [New, York]의 두 token을 predict하는 것이 목표이다. BERT의 objective는 다음의 수식이다.&lt;/p&gt;

\[J_{BERT}=log{\ p\left(New\ |\ is\ a\ city\right)} + log{\ p\left(York\ |\ is\ a\ city\right)}\]

&lt;p&gt;XLNet은 순열을 특정해야 objective를 구체화할 수 있다. [is, a, city, New, York]의 순열이라고 가정하자. 다음의 수식이 XLNet의 objective이다.&lt;/p&gt;

\[J_{XLNet}=log{\ p\left(New\ |\ is\ a\ city\right)} + log{\ p\left(York\ |\ \textbf{New}\ is\ a\ city\right)}\]

&lt;p&gt;XLNet은 AutoRegressive Model이기 때문에 input sentence에 변형을 가하지 않고, 따라서 predict target word 사이의 dependency 역시 학습할 수가 있다. 위의 예시에서는 ‘York’를 predict할 때에 ‘New’ token의 정보를 활용했다.&lt;/p&gt;

&lt;h1 id=&quot;experiments&quot;&gt;Experiments&lt;/h1&gt;

&lt;h2 id=&quot;pretraining-and-implementation&quot;&gt;Pretraining and Implementation&lt;/h2&gt;

&lt;p&gt;Pretraining의 Dataset으로 BooksCorpus, Giga5, CLue Web2012-B, Common Crawl dataset의 Dataset을 사용했다. Google SentencePiece Tokenizer를 사용했다. XLNet-Large는 512 TPU v.3를 사용해 2.5일동안 500K step의 학습을 진행했다. 이 때 Adam Optimizer를 사용했다. Dataset의 크기에 비해 학습량이 적어 Unerfitting된 상태이지만, Pretraining을 더 수행한다고 하더라도 실제 downstream task에서 유의미한 성능 향상은 없었다.&lt;/p&gt;

&lt;h2 id=&quot;fair-comparison-with-bert&quot;&gt;Fair Comparison with BERT&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-XLNet-Generalized-Autoregressive-Pretraining-for-Language-Understanding/08-29-2020-22.01.26.jpg&quot; alt=&quot;XLNet%20Generalized%20Autoregressive%20Pretraining%20for%20L%20833f510b35954da883906c9bc6b15f9d/08-29-2020-22.01.26.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;XLNet-Large는 모든 task에서 BERT-Large보다 좋은 성능을 보였다.&lt;/p&gt;

&lt;h2 id=&quot;comparison-with-roberta-scailing-up&quot;&gt;Comparison with RoBERTa: Scailing Up&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-XLNet-Generalized-Autoregressive-Pretraining-for-Language-Understanding/08-29-2020-22.01.38.jpg&quot; alt=&quot;XLNet%20Generalized%20Autoregressive%20Pretraining%20for%20L%20833f510b35954da883906c9bc6b15f9d/08-29-2020-22.01.38.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;XLNet은 RACE task에서도 BERT, GPT, RoBERTa 등의 model들보다 좋은 성능을 보였다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-XLNet-Generalized-Autoregressive-Pretraining-for-Language-Understanding/08-29-2020-22.05.19.jpg&quot; alt=&quot;XLNet%20Generalized%20Autoregressive%20Pretraining%20for%20L%20833f510b35954da883906c9bc6b15f9d/08-29-2020-22.05.19.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;XLNet은 SQuAD2.0 task에서도 BERT, RoBERTa보다 좋은 성능을 보였다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-XLNet-Generalized-Autoregressive-Pretraining-for-Language-Understanding/08-29-2020-22.06.47.jpg&quot; alt=&quot;XLNet%20Generalized%20Autoregressive%20Pretraining%20for%20L%20833f510b35954da883906c9bc6b15f9d/08-29-2020-22.06.47.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;XLNet은 GLUE task에서도 BERT, RoBERTa보다 좋은 성능을 보였다.&lt;/p&gt;

&lt;h2 id=&quot;ablation-study&quot;&gt;Ablation Study&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-XLNet-Generalized-Autoregressive-Pretraining-for-Language-Understanding/08-29-2020-22.12.45.jpg&quot; alt=&quot;XLNet%20Generalized%20Autoregressive%20Pretraining%20for%20L%20833f510b35954da883906c9bc6b15f9d/08-29-2020-22.12.45.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;1~4를 살펴보면 XLNet-Base가 BERT나 Transformer-XL보다 좋은 성능을 보인다. 이를 통해 permutation language modeling objective가 효과적이었다는 것을 알 수 있다.&lt;/p&gt;

&lt;p&gt;1~2를 살펴보면 Transformer-XL이 BERT보다 RACE와 SQuAD2.0 task에서 더 좋은 성능을 보인다. 이를 통해 Transformer-XL 계열의 model이 long sequence modeling에 효과적이라는 것을 알 수 있다.&lt;/p&gt;

&lt;p&gt;4~5행을 살펴보면 memory caching mechanism이 빠진 경우 RACE나 SQuAD2.0과 같은 long sequence task에서 성능 저하가 있었다는 것을 알 수 있다.&lt;/p&gt;

&lt;p&gt;4, 6~7행을 살펴보면 span-based prediction과 bidirectional data가 성능 향상에 기여했다는 것을 알 수 있다.&lt;/p&gt;

&lt;p&gt;마지막으로 4, 8행을 통해 NSP가 RACE task를 제외한 모든 경우에서 오히려 성능을 하락시켰다는 것을 알 수 있다.&lt;/p&gt;

&lt;h1 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h1&gt;

&lt;p&gt;Permutation을 사용한 Autoregressive Pretraining 방식을 개척했다는 점에서 의의가 있다.&lt;/p&gt;
</description>
        <pubDate>Tue, 19 Jan 2021 00:00:00 -0600</pubDate>
        <link>http://0.0.0.0:4000/XLNet-Generalized-Autoregressive-Pretraining-for-Language-Understanding/</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/XLNet-Generalized-Autoregressive-Pretraining-for-Language-Understanding/</guid>
        
        
      </item>
    
      <item>
        <title>Subword Level Word Vector Representation For Korean</title>
        <description>&lt;h1 id=&quot;subword-level-word-vector-representation-for-korean&quot;&gt;Subword-level Word Vector Representation for Korean&lt;/h1&gt;
&lt;p&gt;title: Subword-level Word Vector Representation for Korean
subtitle: Korean BPE
categories: Paper Review
tags: NLP Korean
date: 2021-01-19 13:00:25 +0000
last_modified_at: 2021-01-19 13:00:25 +0000
—&lt;/p&gt;

&lt;p&gt;Archive Link: https://www.aclweb.org/anthology/P18-1226/
Created: Sep 21, 2020 3:20 PM
Field: NLP
Paper Link: https://www.aclweb.org/anthology/P18-1226.pdf
Status: completed
Submit Date: Jul 1, 2018&lt;/p&gt;

&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;

&lt;p&gt;지금까지의 word representation에 관한 연구는 모두 영어에 집중되어 왔다. language-specific knowledge는 언어 학습에 있어서 매우 결정적인 요소인데 영어와 기원이 다른 여러 다양한 언어들에 대해서는 이러한 연구가 부진했던 것이 사실이다. 본 논문에서는 한국어만의 unique한 언어 구조를 분석해 NLP에 적용해보고자 했다. 구체적으로, Korean에만 존재하는 ‘jamo’ 개념을 도입해 character level에서 더 깊이 들어간 ‘jamo’ 단위로 단어를 분해해 사용했다. 동시에 여러 task에 대해 측정  가능한 한국어 test set도 제안했다. 본 논문에서 제안하는 방식은 word2vec이나 character-level Skip-Grams을 semantic, syntatic 모두에서 능가했다.&lt;/p&gt;

&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;기존의 word representation은 모두 영어 위주였기 때문에 다양한 형태를 가진 한국어에 대해서는 제대로 적용할 수 없었다. 단어를 n-gram set으로 분해하는 방식은 여러 language에서 효과적이긴 했으나 해당 언어만의 unique한 linguistic structure를 무시한다는 단점이 있었다. 이를 해결하기 위해 word vector를 학습하는 과정에서부터 language-specific한 구조를 활용하고자 하는 연구는 활발히 진행되어 왔다. 한국어의 경우 character-level로 단어를 분해하는 연구는 있어왔으나 한국어의 character는 자음과 모음으로 분해가 가능하다는 점에서 다소 부족한 부분이 있었다. 본 논문에서는 ‘jamo’ 단위로까지 분해해 word를 subword로 분해하는 방식을 제안하고, 한국어 원어민들에게서 얻어낸 정확도 높은 한국어 test dataset을 제안하고자 한다.&lt;/p&gt;

&lt;h1 id=&quot;related-work&quot;&gt;Related Work&lt;/h1&gt;

&lt;h2 id=&quot;language-specific-features-for-nlp&quot;&gt;Language-specific features for NLP&lt;/h2&gt;

&lt;p&gt;다양한 언어들의 각각의 고유한 특징은 여러 언어들에 대한 universal model을 개발하는데 큰 장애물이다. universal model은 한국어와 같은 교착어(어근+접사의 결합으로 구성되는 언어)에서는 특히나 좋은 성능을 내지 못했다. 언어 자체의 고유한 구조가 문법과 강력하게 연결되어 있기 때문이다. 한국어에 관련된 기존의 NLP 연구들은 교착어로써의 특성을 반영하고자 노력해왔다. word embedding 이후 ‘Josa’에 대해 특별한 labeling을 부여하거나, ‘jamo’ 단위로 word를 분해해 형태소의 변형을 찾아내려는 시도가 있었다.&lt;/p&gt;

&lt;h2 id=&quot;subword-features-for-nlp&quot;&gt;Subword features for NLP&lt;/h2&gt;

&lt;p&gt;character-level의 subword features 방식은 여러 NLP task에서 성능 향상에 많은 기여를 했다. 특히나 character n-gram model은 sparsity 문제에서 상대적으로 자유롭기 때문에 작은 dataset에서 좋은 성능을 보였다.&lt;/p&gt;

&lt;h1 id=&quot;model&quot;&gt;Model&lt;/h1&gt;

&lt;p&gt;우선 word를 ‘jamo’ 단위로 word를 분해한 뒤, 얻은 ‘jamo’ sequence에서 n-gram을 사용해 word vector를 생성하는 방식을 제안한다.&lt;/p&gt;

&lt;h2 id=&quot;decomposition-of-korean-words&quot;&gt;Decomposition of Korean Words&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-Subword-level-Word-Vector-Representation-for-Korean/09-19-2020-19.34.06.jpg&quot; alt=&quot;Subword-level%20Word%20Vector%20Representation%20for%20Korea%2030b57d4d751a4283a16ae362d09b11b1/09-19-2020-19.34.06.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;우선 한국어 word를 ‘jamo’ 단위로 분해하는 것에 대해서 살펴보자. 영어와는 달리 한국어는 자음과 모음의 규칙이 엄격하다. (영어의 straight을 생각해보자. 모음 a 뒤에 모음 i가 연속해서 등장한다.) 한국어의 character는 영어의 음절과 비슷한 개념이며, 이는 3개의 ‘jamo’ {1. 초성(‘chosung’):자음, 2. 중성(‘joongsung’):모음, 3. 종성( ‘jongsung’):자음}로 구성된다.  ‘joongsung’에 대해서는 예외적으로 없을 수도 있는데, 이 때에는 새로운 symbol \(e\)를 사용했다. 아래는 한국어 ‘해’와 ‘달’에 대한 예시이다.&lt;/p&gt;

\['해'=\{ㅎ,ㅐ,e\},\ '달'=\{ㄷ,ㅏ,ㄹ\}\]

&lt;p&gt;이러한 representation을 이용하면 \(N\)개의 한국어 character는 \(3 * N\)개의 ‘jamo’로 구성된다고 보장 가능하다. word에 대해서도 시작과 끝에 symbol \(\lt\)와 \(\gt\)를 추가했다. 따라서 아래는 한국어 ‘강아지’에 대한 예시이다.&lt;/p&gt;

\['강아지'=\{\lt,ㄱ,ㅏ,ㅇ,ㅇ,ㅏ,e,ㅈ,ㅣ,e,\gt\}\]

&lt;h2 id=&quot;extracting-n-grams-from-jamo-sequence&quot;&gt;Extracting N-grams from jamo Sequence&lt;/h2&gt;

&lt;h3 id=&quot;character-level-n-grams&quot;&gt;Character-level n-grams&lt;/h3&gt;

&lt;p&gt;한국어의 ‘먹었다’ 라는 word를 예시로 n-gram을 추출해보자. 우선 character-level에서는 다음과 같은 3개의 unigram을 얻을 수 있다.&lt;/p&gt;

\[\{ㅁ,ㅓ,ㄱ\},\{ㅇ,ㅓ,ㅆ\},\{ㄷ,ㅏ,e\}\]

&lt;p&gt;2개의 bigram을 얻을 수 있다.&lt;/p&gt;

\[\{ㅁ,ㅓ,ㄱ,ㅇ,ㅓ,ㅆ\},\{ㅇ,ㅓ,ㅆ,ㄷ,ㅏ,e\}\]

&lt;p&gt;1개의 trigram을 얻을 수 있다.&lt;/p&gt;

\[\{ㅁ,ㅓ,ㄱ,ㅇ,ㅓ,ㅆ,ㄷ,ㅏ,e\}\]

&lt;h3 id=&quot;inter-character-jamo-level-n-grams&quot;&gt;Inter-character jamo-level n-grams&lt;/h3&gt;

&lt;p&gt;한국어의 character는 character마다 독립적이지 않고, 이전 character의 영향을 강하게 받는다. 대표적인 예로 조사 ‘이’, ‘가’를 들 수 있다. 두 조사는 semantic에서는 완전히 동일하지만, 직전 character가 종성이 있을 경우 ‘이’를, 종성이 없을 경우 ‘가’를 사용해야만 한다. 이러한 제약을 반영하기 위해 ‘jamo’-level의 n-gram은 인접한 character들까지 통합하도록 했다. 이러한 방식을 통해 다음과 같은 ‘jamo’-level trigram을 얻을 수 있다.&lt;/p&gt;

\[\{\lt,ㅁ,ㅓ\},\{ㅓ,ㄱ,ㅇ\},\{ㄱ,ㅇ,ㅓ\},\{ㅆ,ㄷ,ㅏ\},\{ㅓ,ㅆ,ㄷ\},\{ㅏ,e,\gt\}\]

&lt;h2 id=&quot;subword-information-skip-gram&quot;&gt;Subword Information Skip-Gram&lt;/h2&gt;

&lt;h1 id=&quot;experiments&quot;&gt;Experiments&lt;/h1&gt;

&lt;h2 id=&quot;corpus&quot;&gt;Corpus&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-Subword-level-Word-Vector-Representation-for-Korean/09-19-2020-21.42.46.jpg&quot; alt=&quot;Subword-level%20Word%20Vector%20Representation%20for%20Korea%2030b57d4d751a4283a16ae362d09b11b1/09-19-2020-21.42.46.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;한국어 Wikipedia, 뉴스 기사, Sejong Corpus에서 corpus를 수집했다. 0.12 billion개의 token과 638,708개의 unique words를 얻었는데, 이 중 10번 미만 등장한 corpus는 제외했다.&lt;/p&gt;

&lt;h3 id=&quot;korean-wikipedia&quot;&gt;Korean Wikipedia&lt;/h3&gt;

&lt;p&gt;0.4million개의 기사에서 3.3million개의 sentence와 43.4million개의 word를 얻었다.&lt;/p&gt;

&lt;h3 id=&quot;online-news-articles&quot;&gt;Online News Articles&lt;/h3&gt;

&lt;p&gt;5개의 major 신문사의 사회, 정치, 경제, 국제, 문화, IT의 6가지 분야에서 기사를 수집했다. 2017년 9월~11월의 기사들을 사용했다. 3.2million개의 sentence와 47.1million개의 word를 얻었다.&lt;/p&gt;

&lt;h3 id=&quot;sejong-corpus&quot;&gt;Sejong Corpus&lt;/h3&gt;

&lt;p&gt;Sejong Corpus는 1998년~2007년 사이의 뉴스 기사, 사전, 소설 등의 formal text와 TV, 라디오의 대본 등의 informal text에서 추출한 corpus이다. Wikipedia나 New Article에서 얻을 수 없는 corpus를 얻어낼 수 있었다.&lt;/p&gt;

&lt;h2 id=&quot;evaluation-tasks-and-datasets&quot;&gt;Evaluation Tasks and Datasets&lt;/h2&gt;

&lt;p&gt;similarity task와 analogy task를 통해 word vector의 성능을 측정하고자 했다. 하지만 각각의 task에 대한 한국어 evaluation dataset이 존재하지 않아 evaluation dataset을 개발해 사용했다. 동시에 감정 분석 downstream task를 진행했다.&lt;/p&gt;

&lt;h3 id=&quot;word-similarity-evaluation-dataset&quot;&gt;Word Similarity Evaluation Dataset&lt;/h3&gt;

&lt;p&gt;한국어를 모국어로 사용하는 학생 두 명이 353개의 영어 단어 쌍을 번역한다. 353개의 한국어 단어쌍이 생성된다. 이후 다른 14명의 한국인이 한국어 단어 쌍에 대해 0~10 사이의 유사도 점수를 매긴다. 각 단어 쌍에 매겨진 점수 중에서 최대, 최소 점수를 제외하고 평균을 매긴다. 영어 단어 쌍과 한국어 단어 쌍 사이의 상관계수는 0.82로 매우 유사했다.&lt;/p&gt;

&lt;h3 id=&quot;word-analogy-evaluation-dataset&quot;&gt;Word Analogy Evaluation Dataset&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;Semantic 차원
    &lt;ol&gt;
      &lt;li&gt;
        &lt;p&gt;수도-국가 (Captial-Country)&lt;/p&gt;

        &lt;p&gt;ex) 아테네 : 그리스 = 바그다드 : 이라크&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;남성-여성 (Male-Female)&lt;/p&gt;

        &lt;p&gt;ex) 왕자 : 공주 = 신사 : 숙녀&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;이름-국적 (Name-Nationality)&lt;/p&gt;

        &lt;p&gt;ex) 간디 : 인도 = 링컨 : 미국&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;국가-언어 (Country-Language)&lt;/p&gt;

        &lt;p&gt;ex) 아르헨티나 : 스페인어 = 미국어 : 영어&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;기타 (Miscellaneous)&lt;/p&gt;

        &lt;p&gt;ex) 개구리 : 욜챙이 = 말 : 망아지&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;Syntactic 차원
    &lt;ol&gt;
      &lt;li&gt;
        &lt;p&gt;격 (case)&lt;/p&gt;

        &lt;p&gt;ex) 교수 : 교수가 = 축구 : 축구가&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;시제 (tense)&lt;/p&gt;

        &lt;p&gt;ex) 싸우다 : 싸웠다 = 오다 : 왔다&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;태 (voice)&lt;/p&gt;

        &lt;p&gt;ex) 팔았다 : 팔렸다 = 평가했다 : 평가됐다&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;동사 변형 (verb form)&lt;/p&gt;

        &lt;p&gt;ex) 가다 : 가고 = 쓰다 : 쓰고&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;높임법 (honorific)&lt;/p&gt;

        &lt;p&gt;ex) 도왔다 : 도우셨다 = 됐다 : 되셨다&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;sentiment-analysis&quot;&gt;Sentiment Analysis&lt;/h3&gt;

&lt;p&gt;네이버 영화 감정 분석 dataset을 사용했다. Classifier로 300개의 hidden layer, dropout=0.5의 LSTM을 사용했다.&lt;/p&gt;

&lt;h3 id=&quot;comparison-models&quot;&gt;Comparison Models&lt;/h3&gt;

&lt;p&gt;모든 model의 training epoch=5, #negative samples=5, window size=5, #dimension=300으로 동일하다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Skip-Gram (SG)&lt;/p&gt;

    &lt;p&gt;word-level Skip Gram이다. 모든 unique word에 대해서 unique vector가 부여됐다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Character-level Skip-Gram (SISG(ch))&lt;/p&gt;

    &lt;p&gt;character-level n-gram이다. n=2-4이다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Jamo-level Skip-Gram with Empty Jongsung Symbol (SISG(jm))&lt;/p&gt;

    &lt;p&gt;‘jamo’-level의 n-gram이다. 비어 있는 종성 symbol \(e\)를 추가했다. n=3-6이다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;results&quot;&gt;Results&lt;/h1&gt;

&lt;h3 id=&quot;word-similarity&quot;&gt;Word Similarity&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-Subword-level-Word-Vector-Representation-for-Korean/09-19-2020-22.22.32.jpg&quot; alt=&quot;Subword-level%20Word%20Vector%20Representation%20for%20Korea%2030b57d4d751a4283a16ae362d09b11b1/09-19-2020-22.22.32.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;단어 유사성에 대해 인간의 판단과 model의 cosine 유사도에 대해서 스피어만 상관 계수를 분석한다. word-level skip-gram인 SG보다 character n-gram을 적용한 SISG가 훨씬 더 좋은 성능을 보였다. ‘jamo’-level로 더 깊게 분해한 model이 가장 좋은 성능을 보였다.&lt;/p&gt;

&lt;h3 id=&quot;word-analogy&quot;&gt;Word Analogy&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-Subword-level-Word-Vector-Representation-for-Korean/09-19-2020-22.23.02.jpg&quot; alt=&quot;Subword-level%20Word%20Vector%20Representation%20for%20Korea%2030b57d4d751a4283a16ae362d09b11b1/09-19-2020-22.23.02.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;a:b=c:d의 4개의 단어가 주어진다. 왕:왕비 = 남자:여자 와 같은 형태이다. 여기서 a + b - c와 d 사이의 cosine 유사도를 구한다.&lt;/p&gt;

&lt;h3 id=&quot;sentiment-analysis-1&quot;&gt;Sentiment Analysis&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-Subword-level-Word-Vector-Representation-for-Korean/09-19-2020-22.23.11.jpg&quot; alt=&quot;Subword-level%20Word%20Vector%20Representation%20for%20Korea%2030b57d4d751a4283a16ae362d09b11b1/09-19-2020-22.23.11.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;word-level skip-gram보다 character-level, ‘jamo’-level이 더 좋은 성능을 보였다. 하지만 word-level은 F1 Score에서 본 논문에서 제시한 model보다는 낮지만, character-level, ‘jamo’-level보다 더 좋은 수치를 보였다. 이는 영화 리뷰라는 dataset의 특성 상 고유 명사가 많이 등장하는데, word-level이 고유 명사를 더 잘 잡아내기 때문으로 추측할 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;effect-of-size-n-in-both-n-grams&quot;&gt;Effect of Size n in both n-grams&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-Subword-level-Word-Vector-Representation-for-Korean/09-19-2020-22.23.18.jpg&quot; alt=&quot;Subword-level%20Word%20Vector%20Representation%20for%20Korea%2030b57d4d751a4283a16ae362d09b11b1/09-19-2020-22.23.18.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;‘jamo’-level에서의 n은 증가할수록 대체로 더 좋은 성능을 보여주지만, character-level에서의 n은 그렇지 않다. 이는 한국어의 특성에서 기인하는데, 대부분의 한국어 word는 6자 이하(97.2%)이기 때문에, n=6은 과도하게 큰 값이다. 4자 이하의 word는 전체 한국어 word의 82.6%를 차지하기 때문에 n=4로도 충분하다고 볼 수 있다.&lt;/p&gt;

&lt;h1 id=&quot;conclusion-and-discussions&quot;&gt;Conclusion and Discussions&lt;/h1&gt;

&lt;p&gt;한국어 character를 어떻게 ‘jamo’-level로 분해하는지에 대한 방법론을 제시했다는 점에서 의의가 있다. 특히 비어있는 종성 symbol \(e\)를 추가해 일반화된 표현을 가능하게 했다는 점, inter-character하게 ‘jamo’-level로 분해하는지에 대해서 새로운 방식을 제안했다. 또한 word 단위에서 similarity, analogy 측정을 위한 dataset을 개발했다. sentiment classification task를 통해 word vector 학습이 downstream NLP task에도 큰 영향을 미친다는 점도 알 수 있다.&lt;/p&gt;

&lt;p&gt;한국어를 ‘jamo’-level로 분해하는 방식은 syntatic, semantic의 양 측면에 있어서 모두 긍정적이다. inter-character ‘jamo’-level로 분해해 각종 조사 및 어미에 대해서 syntatic한 feature를 잡아낼 수 있었다. (주어 뒤의 조사 ~은, 동사 뒤의 조사 ~고~, 과거 시제 ~었, 경어체 ~시~ 등) 심지어 더 같은 의미의 더 짧은 character로 축약도 가능했다. (되었다 → 됐다) character level n-gram은 word의 semantic한 feature를 잡아낼 수 있도록 했다. 이러한 방식 덕분에 기존의 word vector보다 더 좋은 성능을 보일 수 있었다.&lt;/p&gt;
</description>
        <pubDate>Tue, 19 Jan 2021 00:00:00 -0600</pubDate>
        <link>http://0.0.0.0:4000/Subword-level-Word-Vector-Representation-for-Korean/</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/Subword-level-Word-Vector-Representation-for-Korean/</guid>
        
        
      </item>
    
      <item>
        <title>Sequence To Sequence Learning With Neural Networks</title>
        <description>&lt;h1 id=&quot;sequence-to-sequence-learning-with-neural-networks&quot;&gt;Sequence to Sequence Learning with Neural Networks&lt;/h1&gt;
&lt;p&gt;title: Sequence to Sequence Learning with Neural Networks
subtitle: seq2seq
categories: Paper Review
tags: NLP
date: 2021-01-19 12:59:59 +0000
last_modified_at: 2021-01-19 12:59:59 +0000
—&lt;/p&gt;

&lt;p&gt;Archive Link: https://arxiv.org/abs/1409.3215
Created: Sep 21, 2020 3:14 PM
Field: NLP
Paper Link: https://arxiv.org/pdf/1409.3215.pdf
Status: completed
Submit Date: Sep 10, 2014&lt;/p&gt;

&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;DNN (Deep Neural Network)는 음성 인식, 사물 인식 등에서 꾸준한 성과를 내어왔다. 하지만 input size가 fixed된다는 한계점이 존재하기 때문에 sequencial problem을 제대로 해결할 수 없다는 한계점이 존재했다. 본 논문에서는 2개의 LSTM (Long Short Term Memory)을 각각 Encoder, Decoder로 사용해 sequencial problem을 해결하고자 했다. 이를 통해 많은 성능 향상을 이루어냈으며, 특히나 long sentence에서 더 큰 상승 폭을 보였다. 이에 더해 단어를 역순으로 배치하는 방식으로도 성능을 향상시켰다.&lt;/p&gt;

&lt;h1 id=&quot;the-model&quot;&gt;The model&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-Sequence-to-Sequence-Learning-with-Neural-Networks/Untitled.png&quot; alt=&quot;Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks%20291498ef530149d190ef2b186d28d51f/Untitled.png&quot; /&gt;&lt;/p&gt;

\[h_t = sigmoid\left(W^{hx}x_t + W^{hh}h_{t-1}\right)\\y_t = W^{yh}h_t\]

\[p\left(y_1,\cdots,y_{T'}|x_1,\cdots,x_T\right)=\prod_{t=1}^{T'}p\left(y_t|v,y_1,\cdots,y_{t-1}\right)\]

&lt;p&gt;RNN은 기본적으로 sequencial problem에 매우 적절한 model이다. 하지만 input size와 output size가 다른 경우에 대해서는 좋은 성능을 보일 수 없었다. 본 논문에서 제시하는 model은 Encoder LSTM에서 하나의 context vector를 생성한 뒤 Decoder LSTM에서 context vector를 이용해 output sentence를 생성하는 방식으로 RNN의 한계점을 극복하고자 했다. input과 output sentence 간의 mapping을 하는 것이 아닌, input sentence를 통해 encoder에서 context vector를 생성하고, 이를 활용해 decoder에서 output sentence를 만들어내는 것이다. Encoder LSTM의 output인 context vector는 Encoder의 마지막 layer에서 나온 output이다. 이를 Decoder LSTM의 첫번째 layer의 input으로 넣게 된다. 여기서 주목할만한 점은 input sentence에서의 word order를 reverse해 사용했다는 것이다. 또한 &lt;EOS&gt; (End of Sentence) token을 각 sentence의 끝에 추가해 variable length sentence를 다뤘다.&lt;/EOS&gt;&lt;/p&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;p&gt;WMT’14의 English to French dataset으로 실험을 진행했다. source / target language 각각에 fixed size vocabulary를 사용했다 (source: 160,000 / target: 80,000). OOV는 “UNK” token으로 대체된다. long sequence에서는 source sentence를 reverse시킨 경우가 특히나 성능이 더 좋았다. 구체적인 수치로 BLEU score가 25.9에서 30.6으로 증가했다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;원래의 순서대로 나열된 단어의 경우 source와 target에서의 연결되는 단어쌍(pair of word) 사이의 거리가 모두 동일하다. 하지만 reverse시킬 경우에는 sentence에서 앞에 위치한 단어일수록 단어쌍 사이의 거리가 짧아지게 된다. 이는 결국 sentence에서 뒤에 위치한 단어들에 대해서는 오히려 reverse하지 않았을 때보다 단어쌍 사이의 거리가 멀어지는 결과를 낳는다. 생각해보면 결국 reverse한 뒤나, 하지 않았을 때에나 단어쌍 사이의 거리 mean값은 동일하다. 그런데 왜 reverse시 더 좋은 성능이 나오는 것인지 의문일 수 있는데, sequencial problem의 개념으로 다시 돌아와 생각해보면 어느정도 이유를 추론 가능하다. sequencial problem에서는 앞쪽에 위치한 data가 뒤의 모든 data에 영향을 주기 때문에 앞에 위치한 data일 수록 중요도가 더 높다고 할 수 있다. 따라서 reverse를 통해 source sentence에서 앞쪽에 위치한 data(word)들의 target sentence에서의 연관 word와의 거리를 줄이는 것은 더 중요도 높은 data에 대해 더 좋은 성능을 보장하게 되는 효과를 낳는다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;dataset의 대부분은 short length sentence이기에 mini batch 사용 시 각 batch 마다 아주 적은 수의 long length sentence가 포함되는 문제가 존재했다. 따라서 각 batch마다 대략적으로 비슷한 length를 가진 sentence가 포함되도록 normalization을 수행한 뒤 실험을 진행했다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-Sequence-to-Sequence-Learning-with-Neural-Networks/05-16-2020-17.33.39.jpg&quot; alt=&quot;Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks%20291498ef530149d190ef2b186d28d51f/05-16-2020-17.33.39.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-Sequence-to-Sequence-Learning-with-Neural-Networks/05-16-2020-17.33.54.jpg&quot; alt=&quot;Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks%20291498ef530149d190ef2b186d28d51f/05-16-2020-17.33.54.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;SOTA(State of the Art)에 비해 0.5 낮은 BLEU Score를 달성했다. OOV가 여전히 존재함에도 SOTA와 동등한 성능을 달성했다는 것은 충분히 의미가 있다.&lt;/p&gt;

&lt;p&gt;위에서 언급했듯이 long Sentence에서도 매우 좋은 성능을 보였다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-Sequence-to-Sequence-Learning-with-Neural-Networks/05-16-2020-17.39.20.jpg&quot; alt=&quot;Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks%20291498ef530149d190ef2b186d28d51f/05-16-2020-17.39.20.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-Sequence-to-Sequence-Learning-with-Neural-Networks/05-16-2020-17.42.43.jpg&quot; alt=&quot;Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks%20291498ef530149d190ef2b186d28d51f/05-16-2020-17.42.43.jpg&quot; /&gt;&lt;/p&gt;
</description>
        <pubDate>Tue, 19 Jan 2021 00:00:00 -0600</pubDate>
        <link>http://0.0.0.0:4000/Sequence-to-Sequence-Learning-with-Neural-Networks/</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/Sequence-to-Sequence-Learning-with-Neural-Networks/</guid>
        
        
      </item>
    
      <item>
        <title>Roberta A Robustly Optimized Bert Pretraining Approach</title>
        <description>&lt;h1 id=&quot;roberta-a-robustly-optimized-bert-pretraining-approach&quot;&gt;RoBERTa: A Robustly Optimized BERT Pretraining Approach&lt;/h1&gt;
&lt;p&gt;title: RoBERTa: A Robustly Optimized BERT Pretraining Approach
subtitle: RoBERTa
categories: Paper Review
tags: NLP
date: 2021-01-19 13:00:33 +0000
last_modified_at: 2021-01-19 13:00:33 +0000
—&lt;/p&gt;

&lt;p&gt;Archive Link: https://arxiv.org/abs/1907.11692
Created: Oct 5, 2020 1:08 AM
Field: NLP
Paper Link: https://arxiv.org/pdf/1907.11692.pdf
Status: completed
Submit Date: Jul 26, 2019&lt;/p&gt;

&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;BERT 계열의 model들은 지금까지 매우 뛰어난 성능을 보여왔다. 본 논문에서는 BERT에 대한 추가적인 연구를 통해 기존의 BERT model들이 undertrained되었음을 보여주고, 다음의 개선 방안들을 제시한다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;더 긴 시간, 더 큰 batch size의 training&lt;/li&gt;
  &lt;li&gt;NSP 제거&lt;/li&gt;
  &lt;li&gt;long sequence에 대한 학습&lt;/li&gt;
  &lt;li&gt;MLM에서의 동적인 masking 정책&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;본 논문의 Contribution은 다음과 같다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;downstream task에서의 성능 향상을 위한 BERT model의 design choice, training 전략을 제시&lt;/li&gt;
  &lt;li&gt;새로운 CC-NEWS dataset을 도입&lt;/li&gt;
  &lt;li&gt;적절한 design choice에 기반한 MLM이 다른 여러 method 대비 좋은 성능을 보임을 입증&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;background&quot;&gt;Background&lt;/h1&gt;

&lt;p&gt;BERT에 대한 overview&lt;/p&gt;

&lt;p&gt;BERT Review 참고 &lt;a href=&quot;https://www.notion.so/BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding-685492e6b895410381c261b24f3f0d9e&quot;&gt;BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;experimental-setup&quot;&gt;Experimental Setup&lt;/h1&gt;

&lt;h2 id=&quot;implementation&quot;&gt;Implementation&lt;/h2&gt;

&lt;p&gt;대부분의 hyperparameters의 값은 BERT와 동일한 값을 채택했다. 하지만 peak learning rate와 #warmup steps, Adam epsilon에 대해서는 tuning을 진행했다. 전체 sequence의 최대 길이 T는 512로 했다.&lt;/p&gt;

&lt;h2 id=&quot;data&quot;&gt;Data&lt;/h2&gt;

&lt;p&gt;BERT-style의 pretraining 방식은 data의 양에 따라 성능이 결정된다. BERT 이후 더 큰 dataset을 사용한 여러 연구가 진행되었으나, 그 dataset이 공개되지는 않았다. 본 논문에서는 총 160GB의 4개의 English dataset을 사용했다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Book Corpus + Wikipedia&lt;/p&gt;

    &lt;p&gt;Original BERT에서 사용했던 dataset으로, 총 16GB이다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;CC-News&lt;/p&gt;

    &lt;p&gt;총 63million개의 2016/09~2019/02 사이의 뉴스 기사를 crawling한 dataset으로, 총 76GB이다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;OpenWebText&lt;/p&gt;

    &lt;p&gt;Reddit과 같은 web site에서 URL 기반으로 crawling을 한 dataset으로, 총 38GB이다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Stories&lt;/p&gt;

    &lt;p&gt;story-like dataset으로, 총 31GB이다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;evaluation&quot;&gt;Evaluation&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;GLUE&lt;/p&gt;

    &lt;p&gt;BERT Review 참고 &lt;a href=&quot;https://www.notion.so/BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding-685492e6b895410381c261b24f3f0d9e&quot;&gt;BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;SQuAD&lt;/p&gt;

    &lt;p&gt;BERT Review 참고 &lt;a href=&quot;https://www.notion.so/BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding-685492e6b895410381c261b24f3f0d9e&quot;&gt;BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;RACE&lt;/p&gt;

    &lt;p&gt;ReAding Comprehension from Examinations이다. 중국에서의 Enghlish Examination에서 추출한 28,000개의 단락과 100,000개의 질문이 존재한다. 각 단락은 여러 질문과 함께 등장하는데, 그 중 하나의 알맞은 질문을 고르는 task이다. 기존의 다른 comprehension dataset에 비해 passage의 길이가 길고, 추론 질문의 비율이 높다는 점에서 차이가 있다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;training-procedure-analysis&quot;&gt;Training Procedure Analysis&lt;/h2&gt;

&lt;p&gt;BERT model L=12, H=768, A=12, #params: 110M으로 은 고정해둔 상태로 실험을 진행한다.&lt;/p&gt;

&lt;h2 id=&quot;static-vs-dynamic-masking&quot;&gt;Static vs Dynamic Masking&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-RoBERTa-A-Robustly-Optimized-BERT-Pretraining-Approach/10-03-2020-16.01.28.jpg&quot; alt=&quot;RoBERTa%20A%20Robustly%20Optimized%20BERT%20Pretraining%20Appr%204a1cb342927840b2a7b4a0865bfc6c53/10-03-2020-16.01.28.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;본 논문에서는 Dynamic Masking 기법을 도입했다. 기존의 BERT에서의 Masking Rule은 전처리 과정에서 한 번 수행한 masking이 계속 유지되는 Static Masking이다. 이는 매 epoch마다 동일한 masking으로 학습을 하게 됨을 의미한다. Dynamic Masking은 training data를 10배로 복제해 각각의 training data마다 다른 masking을 수행했다. 같은 비율의 masking 정책 하에서 (80%/10%/10% 등) 다른 word가 masking되는 것이다. 이를 40 epochs동안 수행하는데, 결국 같은 masking으로 총 4epochs의 학습이 이루어지게 되는 것이다. 이러한 Dynamic Masking 기법은 dataset이 클 수록 Static Masking 대비 더 큰 성능 향상을 보였다. 위 Table에서 볼 수 있듯이 static 대비 dynamic masking이 조금이나마 더 좋은 성능을 보였다.&lt;/p&gt;

&lt;h2 id=&quot;model-input-format-and-next-sentence-prediction&quot;&gt;Model Input Format and Next Sentence Prediction&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-RoBERTa-A-Robustly-Optimized-BERT-Pretraining-Approach/10-03-2020-18.21.21.jpg&quot; alt=&quot;RoBERTa%20A%20Robustly%20Optimized%20BERT%20Pretraining%20Appr%204a1cb342927840b2a7b4a0865bfc6c53/10-03-2020-18.21.21.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;NSP가 성능 향상에 얼마나 기여하는지에 대해서는 많은 연구가 있어왔고, 때로는 각 논문마다 다른 결과를 도출해내기도 했다. 이를 검증하기 위해 위와 같은 실험을 진행했다. 각 항목에 대해서 자세히 살펴보겠다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;SEGMENT-PAIR&lt;/p&gt;

    &lt;p&gt;여러 문장으로 이루어질 수 있는 segment의 pair이다. 한 pair는 최대 512 tokens까지 가질 수 있다. NSP를 포함한다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;SENTENCE-PAIR&lt;/p&gt;

    &lt;p&gt;문장의 pair이다. 한 pair는 최대 512 tokens까지 가질 수 있다. 당연하게도 평균적인 #tokens가 SEGMENT-PAIR보다 작기 때문에, batch size를 늘려 SEGMENT-PAIR의 한 batch와 total #tokens가 비슷해지도록 했다. NSP를 포함한다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;FULL-SENTENCES&lt;/p&gt;

    &lt;p&gt;여러 문장들의 sequences이다. 최대 512 tokens까지 가질 수 있다. 문장이 중간에 끊기는 일은 없도록 한다. 특수한 경우로, 한 document가 끝났음에도 새로운 문장이 삽입될 수 있을 경우, 다음 document의 첫 문장부터 이어서 삽입되게 된다. 이 경우에 있어서는 서로 다른 document에서 온 문장 사이에 특수한 seperator token이 삽입된다. NSP를 포함하지 않는다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;DOC-SENTENCES&lt;/p&gt;

    &lt;p&gt;FULL-SENTENCES와 유사하지만, 서로 다른 document에서 온 문장이 연속되는 일이 없도록 한다. 당연하게도 FULL-SENTENCES에 비해 평균 #tokens가 낮을 수 밖에 없기 때문에, 이 역시 batch size를 늘려 FULL-SENTENCES의 한 batch와 total #tokens가 비슷해지도록 했다. NSP를 포함하지 않는다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;SEGMENT-PAIR와 SENTENCE-PAIR를 비교해보자. SENTENCE-PAIR가 더 낮은 성능을 보였다. SENTENCE-PAIR는 model이 long-range dependencies를 학습할 능력이 없다고 가정한 것인데, 실제로는 BERT model이 long sequences에 대해서도 dependency를 학습할 수 있음을 알 수 있다.&lt;/p&gt;

&lt;p&gt;한편, DOC-SENTENCES와 BERT_BASE를 비교해보면 DOC-SENTENCES가 original BERT보다 더 좋은 성능을 보인다는 것을 확인할 수 있다. 이는 NSP가 오히려 downstream task의 성능에 악영향을 미친다는 것을 보여준다.&lt;/p&gt;

&lt;p&gt;마지막으로, FULL-SENTENCES와 DOC-SENTENCES를 비교하면 한 document 안의 문장만 묶는 DOC-SENTENCES가 미약하게나마 더 좋은 성능을 보인다는 것을 확인할 수 있다. 하지만 DOC-SENTENCES는 batch size가 각 batch마다 다르기 때문에, 본 논문의 이후 실험에서는 편리성을 위해 FULL-SENTENCES를 사용하기로 한다.&lt;/p&gt;

&lt;h2 id=&quot;training-with-large-batches&quot;&gt;Training with large batches&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-RoBERTa-A-Robustly-Optimized-BERT-Pretraining-Approach/10-03-2020-18.37.29.jpg&quot; alt=&quot;RoBERTa%20A%20Robustly%20Optimized%20BERT%20Pretraining%20Appr%204a1cb342927840b2a7b4a0865bfc6c53/10-03-2020-18.37.29.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;batch-size를 증가시킬수록 optimization speed와 성능 향상에 기여한다는 것은 알려져 있다. BERT 역시 larget batch size가 효과가 있는지 확인을 해보기로 한다. batch size=256, #steps=1M은 original BERT_BASE의 값이다. 이와 동일한 computational cost를 갖는 다른 batch size &amp;amp; #steps를 실험해본다. 실제로 같은 computational cost를 갖는 경우에도 batch size가 커질수록 perplexity가 감소함을 확인할 수 있었다. end-task에서의 정확도도 상승했다.&lt;/p&gt;

&lt;h2 id=&quot;text-encoding&quot;&gt;Text Encoding&lt;/h2&gt;

&lt;p&gt;BPE는 등장 빈도를 기반으로 subword를 생성해내는 기법으로, OOV가 없다는 장점이 있다. 기존의 BPE는 모두 character 단위로 이루어졌는데, original BERT도 이를 채택했다. 본 논문에서는 unicode character 단위가 아닌 byte 단위로 하는 BPE를 도입하기로 한다. original BERT의 BPE는 vocabulary size가 30K였다면, 새로운 방식은 50K 정도의 큰 vocabulary size가 필요하다. 하지만 기존에는 필수적이던 전처리 과정이 필요없다는 장점을 갖는다. 사실 새로운 BPE는 약간의 성능 하락을 보여주지만, universal한 encoding 방식을 도입했다는 점에서 미미한 정도의 성능 하락을 감안하고서라도 채택해볼 만 하다.&lt;/p&gt;

&lt;h1 id=&quot;roberta&quot;&gt;RoBERTa&lt;/h1&gt;

&lt;p&gt;정리하자면, &lt;strong&gt;RoBERTa&lt;/strong&gt;는 &lt;strong&gt;R&lt;/strong&gt;obustly &lt;strong&gt;o&lt;/strong&gt;ptimized &lt;strong&gt;BERT&lt;/strong&gt; &lt;strong&gt;a&lt;/strong&gt;pproach의 약자로, 다음의 4가지 특징을 갖는다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;dynamic masking 기법&lt;/li&gt;
  &lt;li&gt;FULL-SENTENCES&lt;/li&gt;
  &lt;li&gt;large mini batches&lt;/li&gt;
  &lt;li&gt;byte-level BPE&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-RoBERTa-A-Robustly-Optimized-BERT-Pretraining-Approach/10-03-2020-18.57.17.jpg&quot; alt=&quot;RoBERTa%20A%20Robustly%20Optimized%20BERT%20Pretraining%20Appr%204a1cb342927840b2a7b4a0865bfc6c53/10-03-2020-18.57.17.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;original BERT_LARGE model과 성능을 비교하기 위해서 RoBERTa의 model 크기를 BERT_LARGE와 동일하게 했다. 또한 original BERT에서 사용했던 dataset으로만 pretraining한 경우, 추가적인 dataset으로 pretraining한 경우, pretraining 횟수를 100K에서 300K, 500K로 증가시킨 경우를 비교했다. RoBERTa는 BERT_LARGE나 XLNet_LARGE와 동일한 조건에서도 더 높은 성능을 보였으며, 당연하게도 가장 많은 pretraining을 시킨 경우가 가장 좋은 성능을 보였다.&lt;/p&gt;

&lt;h2 id=&quot;glue-results&quot;&gt;GLUE Results&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-RoBERTa-A-Robustly-Optimized-BERT-Pretraining-Approach/10-03-2020-19.17.34.jpg&quot; alt=&quot;RoBERTa%20A%20Robustly%20Optimized%20BERT%20Pretraining%20Appr%204a1cb342927840b2a7b4a0865bfc6c53/10-03-2020-19.17.34.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Single-task single models on dev는 GLUE의 각 subtask에 대해 별개로 fine-tuning을 진행한 model들이다.&lt;/p&gt;

&lt;p&gt;Ensembles on test는 GLUE leaderboard에 있는 다른 score와 비교한 것이다. 특이하게 RoBERTa는 RTE, SST, MRPC subtask에 대해서 pretrained RoBERTa에서 fine-tuning을 시작하지 않고, MNLI single-task model에서 fine-tuning을 시작했다. 이 경우가 더 좋은 성능을 보였다고 한다.&lt;/p&gt;

&lt;p&gt;Single-task, single models에서는 RoBERTa가 9개의 모든 GLUE subtask에서 SOTA를 달성했다. 주목할만한 점은, 여기서의 RoBERTa는 original BERT_LARGE와 동일한 model architecture, 동일한 masking rule(static masking)을 적용했다는 점이다. 이는 굳이 dataset size나 training time을 배제하더라도, training objective(NSP 제거)가 얼마나 큰 영향을 미치는지를 보여준다.&lt;/p&gt;

&lt;p&gt;Ensembles on test에서 RoBERTa는 전체 9개 중 4개의 subtask에서 SOTA를 달성했다. 비교 대상인 다른 model들과 달리 RoBERTa는 multi-task fine-tuning을 수행하지 않았다는 점에서 큰 의미가 있다.&lt;/p&gt;

&lt;h2 id=&quot;squad-results&quot;&gt;SQuAD Results&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-RoBERTa-A-Robustly-Optimized-BERT-Pretraining-Approach/10-03-2020-19.17.49.jpg&quot; alt=&quot;RoBERTa%20A%20Robustly%20Optimized%20BERT%20Pretraining%20Appr%204a1cb342927840b2a7b4a0865bfc6c53/10-03-2020-19.17.49.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;SQuAD 2.0에 대해서는 추가적으로 answerable에 대한 binary classification을 수행하고, 기존의 loss와 더했다. 한편, RoBERTa는 original BERT나 XLNet과 달리 pretraining에서 추가적인 QA dataset을 사용하지 않고, 바로 SQuAD에 대해 fine-tuning을 진행했다. 그럼에도 불구하고 BERT나 XLNet에 비해 더 좋은 성능을 보였다.&lt;/p&gt;

&lt;h2 id=&quot;race-results&quot;&gt;RACE Results&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-RoBERTa-A-Robustly-Optimized-BERT-Pretraining-Approach/10-03-2020-19.17.57.jpg&quot; alt=&quot;RoBERTa%20A%20Robustly%20Optimized%20BERT%20Pretraining%20Appr%204a1cb342927840b2a7b4a0865bfc6c53/10-03-2020-19.17.57.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;RoBERTa 는 RACE task에서도 Middle school data와 High school data 모두에서 BERT_LARGE나 XLNET_LARGE 대비 더 좋은 성능을 보였다.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;BERT model의 성능을 향상시키는 여러 방법을 제시했다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;더 많은 횟수, 더 큰 batch size, 더 많은 data로 pretraining&lt;/li&gt;
  &lt;li&gt;NSP 제거&lt;/li&gt;
  &lt;li&gt;longer sequences로 pretraining&lt;/li&gt;
  &lt;li&gt;dynamic masking&lt;/li&gt;
  &lt;li&gt;byte 기반 BPE&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;이를 종합한 RoBERTa는 GLUE, SQuAD, RACE에서 SOTA를 달성했다.&lt;/p&gt;
</description>
        <pubDate>Tue, 19 Jan 2021 00:00:00 -0600</pubDate>
        <link>http://0.0.0.0:4000/RoBERTa-A-Robustly-Optimized-BERT-Pretraining-Approach/</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/RoBERTa-A-Robustly-Optimized-BERT-Pretraining-Approach/</guid>
        
        
      </item>
    
      <item>
        <title>Neural Machine Translation By Jointly Learning To Align And Translate</title>
        <description>&lt;h1 id=&quot;neural-machine-translation-by-jointly-learning-to-align-and-translate&quot;&gt;Neural Machine Translation By Jointly Learning To Align And Translate&lt;/h1&gt;
&lt;p&gt;title: Neural Machine Translation By Jointly Learning To Align And Translate
subtitle: Attention seq2seq
categories: Paper Review
tags: NLP
date: 2021-01-19 13:01:18 +0000
last_modified_at: 2021-01-19 13:01:18 +0000
—&lt;/p&gt;

&lt;p&gt;Archive Link: https://arxiv.org/abs/1409.0473
Created: Sep 21, 2020 3:15 PM
Field: NLP
Paper Link: https://arxiv.org/pdf/1409.0473.pdf
Status: completed
Submit Date: Sep 1, 2014&lt;/p&gt;

&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;

&lt;p&gt;기존의 seq2seq model에서 사용된 LSTM을 사용한 encoder-decoder model은 sequential problem에서 뛰어난 성능을 보였다. 하지만 encoder에서 생성해낸 context vector를 decoder에서 sentence로 만들어내는 위와 같은 방식에서 고정된 vector size는 긴 length의 sentence에서 bottleneck이 된다는 사실을 발견했다. 본 논문에서는 이러한 문제점을 해결하기 위해 source sentence의 정보를 context vector 하나에 담는 것이 아닌, 각 시점마다의 context vector를 따로 생성해 decoder에서 사용했다.&lt;/p&gt;

&lt;h1 id=&quot;decoder&quot;&gt;Decoder&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-Neural-Machine-Translation-By-Jointly-Learning-To-Align-And-Translate/Untitled.png&quot; alt=&quot;Neural%20Machine%20Translation%20By%20Jointly%20Learning%20To%20%20aee56874a5c645b08df4a6b823d6e1f7/Untitled.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-Neural-Machine-Translation-By-Jointly-Learning-To-Align-And-Translate/Untitled%201.png&quot; alt=&quot;Neural%20Machine%20Translation%20By%20Jointly%20Learning%20To%20%20aee56874a5c645b08df4a6b823d6e1f7/Untitled%201.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-Neural-Machine-Translation-By-Jointly-Learning-To-Align-And-Translate/Untitled%202.png&quot; alt=&quot;Neural%20Machine%20Translation%20By%20Jointly%20Learning%20To%20%20aee56874a5c645b08df4a6b823d6e1f7/Untitled%202.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;\(i\)번째 token으로 \(y_i\)가 등장할 조건부 확률에 대한 수식이다. \(y_1 \sim y_{i-1}\) (\(y_i\) 이전의 output sentence)와 \(x\)(input sentence 전체)에 대해 다음 token으로 \(y_i\)가 생성될 조건부 확률이다. 이는 \(g\) 함수에 \(y_{i-1}\), \(s_i\),  \(c_i\)를 인자로 넣어 생성된 값이다. \(y_{i-1}\)은 직전 시점 \(i-1\)에서 생성한 output token이고, \(s_i\)는 현재 시점 \(i\)에서의 RNN hidden state, \(c_i\)는 현재 시점 i에 생성된 context vector이다. 직관적으로 해석해보면 이전 단어 \(y_{i-1}\) 이후에 나올 단어 \(y_i\)를 예측하는 것인데, 이 때 이전 output sentence의 상태 정보를 모두 포함하고 있는 \(s_i\)를 입력으로 받음으로써 output sentence의 문맥을 반영하고, input sentence에 대한 context vector \(c_i\)를 통해 input sentence의 문맥을 반영한다. 이전 seq2seq model에서는 context vector가 input sentence 전체에 대한 vector였는데, 이번 attention seq2seq model에서는 특정 시점 i에 대한 context vector \(c_i\)가 주어진다. 즉, context vector가 input sentence 전체에 대한 하나의 vector가 아니라 각 시점 i에 대해 \(c_i\)가 각각 정의된다는 것이다. 아래에서는 \(c_i\)에 대해 좀 더 자세하게 살펴본다.&lt;/p&gt;

\[c_i=\sum^{T_x}_{j=1}{\alpha_{ij}h_j}\]

&lt;p&gt;\(c_i\)는 \(a_{ij}\)와 \(h_j\)에 대해 \(j\)부터 \(T_x\)까지 더한 vector이다. \(j\)부터 \(T_x\)까지의 의미는 input sentence의 처음부터 끝까지 각 input token에 대해 \(j\)로 순회한다는 의미이다. 즉, \(j\)는 input sentence에서의 token index이다. 반대로 \(i\)는 output sentence에서 현재 시점 i이다. 정리하자면 \(j\)는 input에 대한 index, \(i\)는 output에 대한 index이다. 우리는 output의 \(i\) 시점에서 생성되는 context vector \(c_i\)에 대한 수식을 살펴보는 것이다. \(h_j\)는 input sentence 전체의 context를 포함하지만 동시에 특히 \(j\)번째 token과 그 주변에 대해 더 attention(집중)을 한 vector이다.\(a_{ij}\)는 \(i\)번째 output token이 \(j\)번째 input token과 align될 확률값을 뜻한다. \(a_{ij}\)에 대해 더 살펴보자.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-Neural-Machine-Translation-By-Jointly-Learning-To-Align-And-Translate/Untitled%203.png&quot; alt=&quot;Neural%20Machine%20Translation%20By%20Jointly%20Learning%20To%20%20aee56874a5c645b08df4a6b823d6e1f7/Untitled%203.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-Neural-Machine-Translation-By-Jointly-Learning-To-Align-And-Translate/Untitled%204.png&quot; alt=&quot;Neural%20Machine%20Translation%20By%20Jointly%20Learning%20To%20%20aee56874a5c645b08df4a6b823d6e1f7/Untitled%204.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;\(e_{ij}\)는 \(i\)번째에 들어올 output token과 \(j\)번째 input token이 얼마나 서로 잘 match되는지에 대한 값이다. output의 문맥을 반영하기 위해 \(s_{i-1}\)를 input으로 받고, \(j\)번째 input token에 대한 attention을 주기 위해 \(h_j\)를 input으로 받는다. 이렇게 완성된 \(e_{ij}\)를 softmax한 \(a_{ij}\)는 \(i\)번째 들어올 output token과 \(j\)번째 input token이 align될 확률값을 뜻한다.&lt;/p&gt;

&lt;p&gt;다시\(c_i\)의 의미로 되돌아와보면, \(c_i\)는 \(i\) 시점에서 모든 input sentence token \(j\)에 대해 \(a_{ij}\)와 \(h_j\)를 곱한 vector에 대한 합이다. \(a_{ij}\)는 현재 시점 \(a_i\)에서 생성될 output token이 \(j\)번째 input token과 align될 확률값이며, \(h_j\)는 input sentence에 대한 context이되, \(j\)번째 input token에 특히 attention한 vector이다. 결론적으로 \(c_i\)는 현재 시점 \(i\)에서 input sentence의 어느 token에 더 attention을 해야 하는지에 대한 context라고 직관적으로 해석 가능하다.&lt;/p&gt;

&lt;p&gt;위의 model이 기존 seq2seq model에 비해 가지는 이점은 encoder가 고정된 size context vector 1개에 모든 input sentence token에 대한 attention을 담을 필요가 없다는 것이다. 왜냐하면 decoder model에서 단지 context vector 1개만 사용해 output sentence를 생성해내는 것이 아니라, \(h_j\)와 같은 input sentence token에 대한 attention data를 사용하기 때문이다.&lt;/p&gt;

&lt;h1 id=&quot;encoder&quot;&gt;Encoder&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-Neural-Machine-Translation-By-Jointly-Learning-To-Align-And-Translate/Untitled%205.png&quot; alt=&quot;Neural%20Machine%20Translation%20By%20Jointly%20Learning%20To%20%20aee56874a5c645b08df4a6b823d6e1f7/Untitled%205.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;기존 seq2seq model에서는 encoder는 1개의 context vector를 생성해내기 위해 존재했다. 하지만 본 model은 위에서 언급했듯이 하나의 context vector가 아닌 각 시점에 대한 context vector를 각각 생성한다. 이 때 사용하는 \(h_j\)를 구하는 것이 encoder의 역할이다. \(h_j\)는 위에서 언급했듯이 \(j\)번째 input token에 대해 attention한 vector이다. 이 때 attention한다는 의미는 \(j\)번째 input token에 대해 당연히 가장 높은 가중치를 주고, \(j\)번째에서 멀어질수록 낮은 가중치를 주는 것이다. 이는 \(j-1\), \(j-2\) …의 역방향, \(j+1\), \(j+2\) …의 순방향, 즉 양방향에 대해 모두 적용되어야 한다. 이를 위해 사용한 것이 Bidirectional RNN이다. 순방향에 대한 \(h_j\)를 생성하고, input sentence의 끝에 도달하면 다시 역방향에 대한 \(h_J\)를 생성한다. 그 후 순방향에 대한 \(\overrightarrow{h_j}\)와 역방향에 대한 \(\overleftarrow{h_j}\)를 함께 반영해 최종 \(h_j\)를 만들어낸다.&lt;/p&gt;

&lt;h1 id=&quot;results&quot;&gt;Results&lt;/h1&gt;

&lt;p&gt;본 논문에서 개발한 attention seq2seq model을 RNNsearch라고 명명한다. 이전 seq2seq model은 RNNencdec라고 명명한다.&lt;/p&gt;

&lt;p&gt;model 명 뒤의 숫자는 train 시 사용했던 dataset의 최대 setnence length이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-Neural-Machine-Translation-By-Jointly-Learning-To-Align-And-Translate/05-21-2020-23.48.17.jpg&quot; alt=&quot;Neural%20Machine%20Translation%20By%20Jointly%20Learning%20To%20%20aee56874a5c645b08df4a6b823d6e1f7/05-21-2020-23.48.17.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;가장 주목할만한 점은 sentence length에 관계없이 robust한 결과를 보여줬다는 것이다. 이를 통해 fixed length context vector에 모든 context를 저장함으로써 발생한 bottleneck을 해결했다는 것을 알 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-Neural-Machine-Translation-By-Jointly-Learning-To-Align-And-Translate/05-21-2020-23.49.09.jpg&quot; alt=&quot;Neural%20Machine%20Translation%20By%20Jointly%20Learning%20To%20%20aee56874a5c645b08df4a6b823d6e1f7/05-21-2020-23.49.09.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;UNK (Out Of Vocabulary)를 포함한 경우와 포함하지 않은 경우 모두 기존 seq2seq model보다 월등한 수치를 보여줬다. RNNsearch-50*는 더이상 성능 향상이 없을 때까지 계속 training을 시킨 model이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-Neural-Machine-Translation-By-Jointly-Learning-To-Align-And-Translate/05-21-2020-23.48.55.jpg&quot; alt=&quot;Neural%20Machine%20Translation%20By%20Jointly%20Learning%20To%20%20aee56874a5c645b08df4a6b823d6e1f7/05-21-2020-23.48.55.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위는 \(a_{ij}\)를 시각화한 그림인데, 대체로 monotonic한 match를 볼 수 있다. English-French translate이기에 그렇다. 하지만 (a)를 보면 조사나 명사에 대해서 monotonic하지 않은 case가 있음에도 불구하고 정확히 align을 했다는 사실을 확인 가능하다. (d)에서는 본 model에서 채택한 soft-align 방식의 이점이 드러난다. soft-align이란 가장 높은 확률값을 가진 token pair 1개만을 채택해 align하는 것이 아니라 여러 token에 대해 각각의 align probability를 적용해 soft하게 align했다는 의미이다. 만약 soft-align이 아닌 hard-align을 했다면 [the man]의 2개의 token을 각각 [l’ homme]의 두 token 중 하나에 align해야 하는데 이러한 작업은 translation에 결코 도움이 되지 않는다. 따라서 soft-align을 사용해 여러 token에 대한 align을 모두 고려하는 방식이 적합하다는 것을 알 수 있다.&lt;/p&gt;
</description>
        <pubDate>Tue, 19 Jan 2021 00:00:00 -0600</pubDate>
        <link>http://0.0.0.0:4000/Neural-Machine-Translation-By-Jointly-Learning-To-Align-And-Translate/</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/Neural-Machine-Translation-By-Jointly-Learning-To-Align-And-Translate/</guid>
        
        
      </item>
    
      <item>
        <title>Mass Masked Sequence To Sequence Pre Training For Language Generation</title>
        <description>&lt;h1 id=&quot;mass-masked-sequence-to-sequence-pre-training-for-language-generation&quot;&gt;MASS: Masked Sequence to Sequence Pre-training for Language Generation&lt;/h1&gt;
&lt;p&gt;title: MASS: Masked Sequence to Sequence Pre-training for Language Generation
subtitle: MASS
categories: Paper Review
tags: NLP
date: 2021-01-19 12:59:35 +0000
last_modified_at: 2021-01-19 12:59:35 +0000
—&lt;/p&gt;

&lt;p&gt;Archive Link: https://arxiv.org/abs/1905.02450
Created: Sep 21, 2020 3:19 PM
Field: NLP
Paper Link: https://arxiv.org/pdf/1905.02450.pdf
Status: not checked
Submit Date: Jun 21, 2019&lt;/p&gt;

&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;

&lt;p&gt;BERT에서 영감을 받아 Pre-training / fine-tuning, encoder/decoder를 채택한 MAsked Sequence to Sequence (MASS) model을 만들어냈다. random하게 input sentence에 연속적인 mask를 부여한 뒤 decoder가 predict하는 방식으로 encoder와 decoder를 Pre-training시켜 Language Generation Task에 적합한 Model을 개발했다. 특히 dataset이 적은 Language Generation task에서 비약적인 성능 향상이 있었다.&lt;/p&gt;

&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;Pre-training은 target task에 대한 labeled data(pair data)가 적으면서 해당 language에 대한 data(unpaired data)는 많을 때에 가장 적합하다고 할 수 있다. BERT는 language understanding을 목표로 하는 방식이기에 language generation task에는 적합하지 않다. MASS의 BERT와 Masking rule에서 다음과 같은 차이점을 둔다. 첫번째로, MASK token이 연속적으로 배치되고, 해당 MASK Token을 예측하면서 encoder는 unmasked token들의 context를 학습할 수 있도록 한다. 두번째로 decoder의 target token에도 MASK를 부여함으로써 predict 시에 encoder를 더 많이 활용할 수 있도록 한다.&lt;/p&gt;

&lt;h1 id=&quot;mass&quot;&gt;MASS&lt;/h1&gt;

&lt;h2 id=&quot;sequence-to-sequence-learning&quot;&gt;Sequence to Sequence Learning&lt;/h2&gt;

&lt;p&gt;source sentence를 \(x\), target sentence를 \(y\)라고 한다. 각각 domain \(X\)와 \(Y\)에 속한다. sentence pair를 다음과 같이 정의할 수 있다.&lt;/p&gt;

\[\left(x,y\right) \in \left(X,Y\right), \\x= \left(x_1,x_2, ..., x_m\right)\\y = \left(y_1,y_2, ..., y_n\right)\]

&lt;p&gt;Objective Function은 다음과 같다. domain \(X\)와 \(Y\)에 대한 모든 sentence pair들에 대해 \(x\)가 주어졌을 때 \(y\)를 구하는 조건부 확률의 log liklihood를 더한 것이다.&lt;/p&gt;

\[L(\theta;(X,Y)) = \sum_{\left(x,y\right)\in\left(X,Y\right)} {log P\left(y|x;\theta\right)}\]

&lt;p&gt;조건부 확률을 구하는 수식은 다음과 같다. source sentence 전체와 target sentence에서 현재 token 이전의 모든 token들이 주어졌을 때 현재 token에 대한 조건부 확률이다.&lt;/p&gt;

\[P\left(y|x;\theta\right)=\prod_{t=1}^n{P\left(y_t|y_{&amp;lt;t},x;\theta\right)}\]

&lt;p&gt;\(y_{&amp;lt;t}\)는  \(y_1\sim y_{t-1}\)의 token들이다.&lt;/p&gt;

&lt;h2 id=&quot;masked-sequence-to-sequence-pre-training&quot;&gt;Masked Sequence to Sequence Pre-training&lt;/h2&gt;

&lt;p&gt;MASS는 BERT와 달리 MASK token이 discrete하게 분포되어 있지 않고 연속적으로 뭉쳐져 있다. 이에 따라 새로운 parameter \(k\)가 등장한다. \(k\)는 MASK token의 개수인데, \(k\)개의 MASK token은 연속되어 있다. MASK token이 \(u\)부터 \(v\)까지 분포되어 있다면 \(0&amp;lt;u&amp;lt;v&amp;lt;m\) (\(m\)은 전체 sentence 길이)이고, \(k = v - u + 1\)이다. Pre-training에서 사용하는 Objective Function은 다음과 같다. 조건부 확률의 조건으로 다음의 2가지 값이 주어지게 된다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;MASK가 씌워진 input sentence 전체&lt;/li&gt;
  &lt;li&gt;input sentence에서 MASK가 씌워진 token들 중 현재 token \(x_t\) 이전의 token들의 MASK 씌워지기 이전 원본 token&lt;/li&gt;
&lt;/ol&gt;

\[L(\theta;X)=\frac{1}{\vert X\vert}\sum_{x\in X}log\ P\left(x^{u:v}|x^{\backslash u:v};\theta\right)\\=\frac{1}{\vert X\vert}\sum_{x\in X}log\prod_{t=u}^vP\left(x_t^{u:v}|x_{&amp;lt;t}^{u:v},x^{\backslash u:v};\theta\right)\]

&lt;p&gt;\(x^{u:v}\)는 sentence \(x\)에서 \(u\)부터 \(v\)까지의 tokens를 뜻하고, \(x^{\backslash u:v}\)는 \(u\)부터 \(v\)까지 MASK된 sentence \(x\) 전체를 뜻한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-MASS-Masked-Sequence-to-Sequence-Pre-training-for-Language-Generation/09-04-2020-14.43.42.jpg&quot; alt=&quot;MASS%20Masked%20Sequence%20to%20Sequence%20Pre-training%20for%20%2071c3529aece54ebab2fa54feb4adda92/09-04-2020-14.43.42.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;구체적인 예시를 살펴보자. 위의 figure는 \(x_3, x_4, x_5, x_6\)이 masking된 상황이다. \(k=4\)이고, \(u=3, v=6\)이다. Encoder의 input 으로는 masking된 input sentence \(x^{\backslash u:v}\)가 들어오게 되는데, 이 경우에는 \(x^{\backslash 3:6}\)이다. Attention 기법을 적용해 Decoder로 값이 넘어오고, Decoder에서는 새로운 input으로 \(x^{u:v}\), 이 경우에는 \(x^{3:6}\)을 입력으로 받는다. 이 때 input sentence에서 masking이 되지 않은 token들 (\(x_1, x_2, x_7,x_8)\)의 경우에는 Decoder에 input으로 들어오지 않는다. Decoder의 input으로 들어온 token들 \(x_3, x_4, x_5, x_6\) 중 실제로는 \(x^{u:v}_{&amp;lt;t}\)로 사용되기 때문에 마지막 token \(x_6\)은 사용되지 않는다.&lt;/p&gt;

&lt;h2 id=&quot;discussions&quot;&gt;Discussions&lt;/h2&gt;

&lt;h3 id=&quot;special-case--k1-km&quot;&gt;Special Case ( k=1, k=m)&lt;/h3&gt;

&lt;p&gt;MASS에서 hyperparameter \(k\)는 매우 중요한 parameter이다. \(k\)가 특수한 값일 때에 대해서 살펴보자.&lt;/p&gt;

&lt;p&gt;\(k=1\)인 경우는 사실 BERT에서의 MLM(Masked Langage Model)이다. BERT의 MLM에 대한 자세한 설명은 아래를 참조하자.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.notion.so/Copy-of-BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding-5cb659c4a2164cfa8ffc5dadfc411993&quot;&gt;Copy of BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-MASS-Masked-Sequence-to-Sequence-Pre-training-for-Language-Generation/09-04-2020-15.15.36.jpg&quot; alt=&quot;MASS%20Masked%20Sequence%20to%20Sequence%20Pre-training%20for%20%2071c3529aece54ebab2fa54feb4adda92/09-04-2020-15.15.36.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;BERT의 MLM은 MASK token에 대해서 predict만 하는 방식으로 Pre-training을 수행했다. 즉 Decoder에 어떠한 input도 추가적으로 주어지지 않고, Encoder에서 넘어온 Context Vector만을 사용해 MASK token을 predict하는 training이다. 이는 MASS에서 \(k=1\)일 때의 경우이다.&lt;/p&gt;

&lt;p&gt;한편, \(k=m\) (\(m\)은 sentence의 token 개수)인 경우는 일반적인 Language Generation Model이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-MASS-Masked-Sequence-to-Sequence-Pre-training-for-Language-Generation/09-04-2020-15.15.46.jpg&quot; alt=&quot;MASS%20Masked%20Sequence%20to%20Sequence%20Pre-training%20for%20%2071c3529aece54ebab2fa54feb4adda92/09-04-2020-15.15.46.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;\(k=m\)인 경우는 사실 일반적인 Language Model의 경우이다. \(k=m\)라는 것은 다시 말해 input sentence의 모든 token이 masking되었다는 의미이고, 이는 Encoder의 input으로 아무 값도 들어오지 않는 경우와 같다. 한편 Decoder의 입장에서는 input으로 original sentence의 masked token들이 들어오게 되는데, original sentence는 모두 masking되었으므로 모든 token이 Decoder로 들어오는 경우이다. 이는 결국 Encoder가 없이 Decoder만 작동하는 상황이라고 볼 수 있다. 일반적인 GPT model와 같다.&lt;/p&gt;

&lt;p&gt;위의 두 가지 special case와 일반적인 case를 Table로 정리하면 아래와 같다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-MASS-Masked-Sequence-to-Sequence-Pre-training-for-Language-Generation/09-04-2020-15.07.10.jpg&quot; alt=&quot;MASS%20Masked%20Sequence%20to%20Sequence%20Pre-training%20for%20%2071c3529aece54ebab2fa54feb4adda92/09-04-2020-15.07.10.jpg&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;comparison-with-existing-model&quot;&gt;Comparison with existing model&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;MASK token만 predict하게 함으로써 Encoder는 unmasked token들에 대한 context를 학습하게 되고, decoder가 encoder로부터 더 좋은 정보를 가져갈 수 있도록 한다. (encoder가 context vector를 제대로 생성해내도록 한다.)&lt;/li&gt;
  &lt;li&gt;MASK token을 연속적으로 배치함으로써 Decoder가 단순 word들이 아닌 subsentence를 만들어낼 수 있도록 한다. (better language modeling capability)&lt;/li&gt;
  &lt;li&gt;Decoder의 input으로 source sentence의 unmasked token들이 들어오지 못하게 함으로써 Decoder의 input token들에서 정보를 활용하기보다 Encoder에서 넘어온 Context Vector의 정보를 활용할 수 있도록 했다.&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;experiments-and-results&quot;&gt;Experiments and Results&lt;/h1&gt;

&lt;h2 id=&quot;mass-pre-training&quot;&gt;MASS Pre-training&lt;/h2&gt;

&lt;h3 id=&quot;model-configuration&quot;&gt;Model Configuration&lt;/h3&gt;

&lt;p&gt;6개의 encoder, decoder layer를 가진 Transformer를 Base Model로 선택했다. NMT를 위해 source language와 target language에 대해 각각 monolingual data로 pre-train을 진행했다. English-French, English-German, English-Romanian의 3가지 pair를 사용했다. 각 pair에 대해서 별개로 학습을 진행했으며, 이 때 source language와 target language를 구분하기 위해 새로운 language embedding을 encoder input과 decoder input에 추가했다. Text Summarization과 Conversational Response Generation task에 대해서는 모두 English에 대해서만 pre-train을 진행했다.&lt;/p&gt;

&lt;h3 id=&quot;datasets&quot;&gt;Datasets&lt;/h3&gt;

&lt;p&gt;WMT News Crawl Dataset을 사용했다. English, French, German, Romanian에 대해서 Pre-train을 진행했다. 이 중 Romanian의 경우에는 data가 적은 language이다. low-resource language에 대한 MASS의 pre-training 성능을 측정하기 위해 채택했다. 모든 language에 대해 BPE를 적용했다.&lt;/p&gt;

&lt;h3 id=&quot;pre-training-details&quot;&gt;Pre-Training Details&lt;/h3&gt;

&lt;p&gt;BERT와 동일한 masking rule을 채택했다.MASK token으로 변경되는 token 중 실제로 변경되는 token은 80%이고, 다른 random한 token으로 변경되는 것이 10%, 변경되지 않는 것이 10%이다. hyperparameter \(k\)는 전체 sentence 길이 \(m\)의 50%와 비슷한 수치가 되도록 설정했다. decoder의 input으로 들어오는 sentence에 대해서는 기존의 original sentence에서의 positional encoding은 수정되지 않는다. Adam Optimizer를 사용했고, lr은 0.0001이며, batch_size는 3000이다.&lt;/p&gt;

&lt;p&gt;fine-tuning을 수행할 dataset이 적은 경우(paired sentence가 적은 경우)에 대해서도 성능을 측정한다. 특히 아예 fine-tuning data가 없는 상태에서도 NMT를 잘 수행할 수 있는지에 대해서 살펴본다.&lt;/p&gt;

&lt;h2 id=&quot;fine-tuning-on-nmt&quot;&gt;Fine-Tuning on NMT&lt;/h2&gt;

&lt;h3 id=&quot;experimental-setting&quot;&gt;Experimental Setting&lt;/h3&gt;

&lt;p&gt;Unsupervised NMT를 수행하기 위해서 back-translation을 사용한다. bilingual data가 없기 때문에 soruce language data로 가상의 bilingual data를 생성해내는 것이다. Pre-Training과 동일하게 Adam Optimizer, lr=0.0001을 채택했으며, batck_size는 2000이다. evaluation을 위해 BLEU Score를 사용했다.&lt;/p&gt;

&lt;h3 id=&quot;results-on-unsupervised-nmt&quot;&gt;Results on Unsupervised NMT&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-MASS-Masked-Sequence-to-Sequence-Pre-training-for-Language-Generation/09-04-2020-16.53.36.jpg&quot; alt=&quot;MASS%20Masked%20Sequence%20to%20Sequence%20Pre-training%20for%20%2071c3529aece54ebab2fa54feb4adda92/09-04-2020-16.53.36.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;RNN 계열의 Model들(1,2행)과 Pre-train 방식이 아닌 Transformer Model(3,4행), Pre-train Transfor Model(5행)들을 모두 능가했다. Unsupervised NMT는 난제이기에 절대적인 Score는 낮지만, 기존의 SOTA Model인 XLM을 능가했다는 점에서 의미가 있다.&lt;/p&gt;

&lt;h3 id=&quot;compared-with-other-pre-training-methods&quot;&gt;Compared with Other Pre-training Methods&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-MASS-Masked-Sequence-to-Sequence-Pre-training-for-Language-Generation/09-04-2020-16.59.37.jpg&quot; alt=&quot;MASS%20Masked%20Sequence%20to%20Sequence%20Pre-training%20for%20%2071c3529aece54ebab2fa54feb4adda92/09-04-2020-16.59.37.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;다양한 Pre-train Methods를 적용한 Model들과 Unsupervised NMT에서의 BLEU Score를 비교해본다. BERT와 동일한 방식으로 Pre-train을 진행한 BERT+LM Model, denoising auto-encoder Pre-train 방식을 적용한 DAE를 모두 능가했다.&lt;/p&gt;

&lt;h3 id=&quot;experiments-on-low-resource-nmt&quot;&gt;Experiments on Low-Resource NMT&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-MASS-Masked-Sequence-to-Sequence-Pre-training-for-Language-Generation/09-04-2020-17.04.30.jpg&quot; alt=&quot;MASS%20Masked%20Sequence%20to%20Sequence%20Pre-training%20for%20%2071c3529aece54ebab2fa54feb4adda92/09-04-2020-17.04.30.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Pre-train은 20000 step 진행했으며, bilingual dataset의 sample 크기가 10K, 100K, 1M인 경우에 대해서 각각의 언어에 대해 별개로 성능을 측정했다. baseline model은 pre-train 과정이 없는 model이다. 모든 경우에 있어서 MASS가 baseline model을 압도했으며, 특히나 Sample의 크기가 작을 수록(fine-tuning을 적게 수행할수록) 성능의 차이가 컸다.&lt;/p&gt;

&lt;h2 id=&quot;fine-tuning-on-text-summarization&quot;&gt;Fine-Tuning on Text Summarization&lt;/h2&gt;

&lt;h3 id=&quot;experiment-setting&quot;&gt;Experiment Setting&lt;/h3&gt;

&lt;p&gt;Gigaword corpus를 fine-tuning data로 사용했다. sample size가 10K, 100K, 1M, 3.8M인 경우에 대해서 별개로 성능을 측정했으며, encoder의 input은 article로, decoder의 output은 title로 설정했다.성능 측정은 ROUGE-1, ROUGE-2, ROUGE-L에 대한 F1 score로 측정했다. beam size=5인 beam search를 사용했다.&lt;/p&gt;

&lt;h3 id=&quot;results&quot;&gt;Results&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-MASS-Masked-Sequence-to-Sequence-Pre-training-for-Language-Generation/09-04-2020-17.12.48.jpg&quot; alt=&quot;MASS%20Masked%20Sequence%20to%20Sequence%20Pre-training%20for%20%2071c3529aece54ebab2fa54feb4adda92/09-04-2020-17.12.48.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;pre-training을 수행하지 않은 basemodel과 비교를 수행했으며, dataset이 적은 경우에 대해서 압도적인 성능 격차를 보였다는 것을 확인할 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;compared-with-other-pre-training-methods-1&quot;&gt;Compared with Other Pre-Training Methods&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-MASS-Masked-Sequence-to-Sequence-Pre-training-for-Language-Generation/09-04-2020-17.14.56.jpg&quot; alt=&quot;MASS%20Masked%20Sequence%20to%20Sequence%20Pre-training%20for%20%2071c3529aece54ebab2fa54feb4adda92/09-04-2020-17.14.56.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;다른 Pre-training model에 대해서도 더 좋은 성능을 보였다.&lt;/p&gt;

&lt;h2 id=&quot;fine-tuning-on-conversational-response-generation&quot;&gt;FIne-Tuning on Conversational Response Generation&lt;/h2&gt;

&lt;h3 id=&quot;experimental-setting-1&quot;&gt;Experimental Setting&lt;/h3&gt;

&lt;p&gt;Cornell movie dialog corpus를 Dataset으로 사용했다. 총 140K의 pair 중에서 10K는 validation set, 20K는 test set, 나머지는 모두 training set으로 사용했다. Perplexity(PPL)을 성능 측정 단위로 사용했다.&lt;/p&gt;

&lt;h3 id=&quot;results-1&quot;&gt;Results&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-MASS-Masked-Sequence-to-Sequence-Pre-training-for-Language-Generation/09-04-2020-17.18.54.jpg&quot; alt=&quot;MASS%20Masked%20Sequence%20to%20Sequence%20Pre-training%20for%20%2071c3529aece54ebab2fa54feb4adda92/09-04-2020-17.18.54.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Sample 크기가 10K, 110K인 경우에 대해서 성능을 측정했다. MASS는 모든 경우에서 Pre-training을 수행하지 않은 Baseline Model과, Pre-training을 수행한 BERT Model보다 더 좋은 성능을 보였다. PPL은 더 낮은 Score가 더 좋은 성능을 뜻한다.&lt;/p&gt;

&lt;h2 id=&quot;analysis-of-mass&quot;&gt;Analysis of MASS&lt;/h2&gt;

&lt;h3 id=&quot;study-of-different-k&quot;&gt;Study of Different k&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-MASS-Masked-Sequence-to-Sequence-Pre-training-for-Language-Generation/09-04-2020-17.25.48.jpg&quot; alt=&quot;MASS%20Masked%20Sequence%20to%20Sequence%20Pre-training%20for%20%2071c3529aece54ebab2fa54feb4adda92/09-04-2020-17.25.48.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;hyperparameter \(k\)에 대해서 자세히 살펴보자. \(k\)의 값 변화에 따른 Score들을 측정해본다.&lt;/p&gt;

&lt;p&gt;(a)와 (b)는 각각 English, French에 대해 Pre-training을 시킨 직후(fine-tuning 없이)의 PPL Score를 나타낸 것이다. \(k\)가 \(m\)의 50%~70% 인 구간에서 가장 좋은 수치를 보였다.&lt;/p&gt;

&lt;p&gt;(c)는 English-French NMT에 대한 BLEU Score이다. (d)는 Text Summarization에 대한 ROUGUE score이다. (e)는 Conversational Response Generation에 대한 PPL Score이다. 모두 공통적으로 \(k\)가 \(m\)의 50%인 구간에서 가장 좋은 수치를 보였다.&lt;/p&gt;

&lt;p&gt;\(k\)가 \(m\)의 50%라는 수치는 직관적으로 이해했을 때에도 가장 적합하다.&lt;/p&gt;

&lt;p&gt;\(k\)의 값이 감소한다면 masking을 덜 수행하게 되므로 Encoder Input에 변형이 덜 발생한다는 의미이며, 동시에 Decoder Input으로 들어오는 값이 감소함을 뜻한다. 따라서 Encoder에 대한 의존도를 높이게 된다.&lt;/p&gt;

&lt;p&gt;반대로 \(k\)의 값이 증가한다면 masking을 더 많이 수행하게 되므로 Encoder Input에 변형이 더 발생한다는 의미이며, 동시에 Decoder Input으로 들어오는 값이 증가함을 뜻한다. 따라서 Decoder에 대한 의존도를 높이게 된다.&lt;/p&gt;

&lt;p&gt;Language Generation task에서는 Encoder(source)와 Decoder(target) 중 어느 쪽으로도 편향되지 않아야 좋은 성능을 나타낼 것이다. 따라서 \(k\)가 \(m\)의 50%라는 수치가 가장 적합함을 직관적으로 이해할 수 있다.&lt;/p&gt;

&lt;p&gt;위의 Figure에서도 볼 수 있듯이 당연하게도 \(k=1\)인 경우(BERT의 MLM), \(k=m\)인 경우(General Language Model) 모두 Language Generation task에서는 좋은 성능을 보이지 못한다.&lt;/p&gt;

&lt;h3 id=&quot;ablation-study-of-mass&quot;&gt;Ablation Study of MASS&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-MASS-Masked-Sequence-to-Sequence-Pre-training-for-Language-Generation/09-04-2020-17.37.03.jpg&quot; alt=&quot;MASS%20Masked%20Sequence%20to%20Sequence%20Pre-training%20for%20%2071c3529aece54ebab2fa54feb4adda92/09-04-2020-17.37.03.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;MASS에서 추가된 새로운 Masking Rule 다음의 2가지로 정리할 수 있다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;MASK token을 연속적으로 배치&lt;/li&gt;
  &lt;li&gt;encoder input의 unmasked token을 decoder input에서 masking&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;위의 Table의 Discrete는 1번 rule을 제거한 것이고(비연속적으로 MASK token 배치), Feed는 2번 rule을 제거한 것이다(decoder input이 original sentence). Unsupervised English to French NMT에서 MASS가 가장 좋은 성능을 보였다.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;MASS는 Dataset이 적은 경우(또는 Dataset이 아예 없는 경우)의 Language Generation task에서 기존의 SOTA를 능가하는 성능을 보였다. 특히나 Unsupervised NMT에서 비약적인 성능 향상을 이뤄냈다.&lt;/p&gt;
</description>
        <pubDate>Tue, 19 Jan 2021 00:00:00 -0600</pubDate>
        <link>http://0.0.0.0:4000/MASS-Masked-Sequence-to-Sequence-Pre-training-for-Language-Generation/</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/MASS-Masked-Sequence-to-Sequence-Pre-training-for-Language-Generation/</guid>
        
        
      </item>
    
      <item>
        <title>Kr Bert A Small Scale Korean Specific Language Model</title>
        <description>&lt;h1 id=&quot;kr-bert-a-small-scale-korean-specific-language-model&quot;&gt;KR-BERT: A Small-Scale Korean-Specific Language Model&lt;/h1&gt;
&lt;p&gt;title: KR-BERT: A Small-Scale Korean-Specific Language Model
subtitle: KR-BERT
categories: Paper Review
tags: NLP Korean
date: 2021-01-19 13:01:42 +0000
last_modified_at: 2021-01-19 13:01:42 +0000
—&lt;/p&gt;

&lt;p&gt;Archive Link: https://arxiv.org/abs/2008.03979
Created: Nov 13, 2020 9:50 PM
Field: NLP
Paper Link: https://arxiv.org/pdf/2008.03979.pdf
Status: completed
Submit Date: Aug 10, 2020&lt;/p&gt;

&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;기존의 BERT model은 104개의 language의 Wikipedia dataset으로 학습된 model이다. 범용적으로 사용될 수 있다는 장점에도 불구하고, model의 크기가 과도하게 크다는 단점이 존재한다. 또한 non-English downstream task에서 좋은 성능을 보여주지 못하는 경우가 많다는 한계도 명확하다. 특히나 Korean과 같은 언어에서는 한계가 두드러진다.&lt;/p&gt;

&lt;p&gt;Korean NLP task를 해결하기 위한 BERT model은 다음과 같은 이유들로 인해 많은 어려움이 있다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Korean이 교착어라는 특성으로 인해 과도하게 많은 형태소&lt;/li&gt;
  &lt;li&gt;Hangul의 과도하게 많은 character (10,000개 이상)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;본 논문에서는 위와 같은 Korean의 한계점에도 불구하고 Korean-specific한 BERT model을 고안해냈다. 우선 Multilingual BERT model에 비해 model의 size를 과감히 줄이고, sub-characters BPE를 사용했다. 또한 Bidirectional WordPiece Tokenizer를 사용해 Korean의 linguistic한 특성을 반영하고자 했다. KR-BERT model은 다른 Multilingual BERT Model의 성능을 모든 task에서 능가했고, 이에 더해 KorBERT나 KoBERT와 같은 기존의 Korean-specific model과도 동등하거나 더 좋은 성능을 보였다. 이는 KR-BERT의 작은 model 크기를 고려하면 매우 유의미한 결과이다.&lt;/p&gt;

&lt;h1 id=&quot;related-work&quot;&gt;Related Work&lt;/h1&gt;

&lt;h2 id=&quot;models-after-bert&quot;&gt;Models after BERT&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-KR-BERT-A-Small-Scale-Korean-Specific-Language-Model/11-13-2020-18.44.37.jpg&quot; alt=&quot;KR-BERT%20A%20Small-Scale%20Korean-Specific%20Language%20Mod%20efca31692a294cd195649aaf8d7f1881/11-13-2020-18.44.37.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;BERT 이후로 XLNet과 RoBERTa와 같은 대규모 dataset을 사용한 model들이 많이 등장했다. 그에 비해 DistilBERT나 ALBERT와 같이 #parameters를 줄이고, dataset도 늘리지 않은 small model들도 등장했다.&lt;/p&gt;

&lt;h2 id=&quot;recent-korean-bert-models&quot;&gt;Recent Korean BERT models&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-KR-BERT-A-Small-Scale-Korean-Specific-Language-Model/11-13-2020-18.44.42.jpg&quot; alt=&quot;KR-BERT%20A%20Small-Scale%20Korean-Specific%20Language%20Mod%20efca31692a294cd195649aaf8d7f1881/11-13-2020-18.44.42.jpg&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;the-need-for-a-small-scale-language-specific-model&quot;&gt;The Need for a Small-scale Language-specific Model&lt;/h1&gt;

&lt;p&gt;Korean NLP task에서 multilingual BERT model은 아래와 같은 한계를 지닌다.&lt;/p&gt;

&lt;h2 id=&quot;limit-of-corpus-domain&quot;&gt;Limit of Corpus Domain&lt;/h2&gt;

&lt;p&gt;multilingual BERT는 104개의 language로 구성된 Wikipedia data로 pretrain된 model이다. German이나 French와 같은 data가 풍부한 language에 대해서는 Wikipedia에 더해 추가적인 dataset을 활용해 pretrain을 수행했다. 그러나 그 외 대부분의 language는 Wikipedia dataset만으로 pretrain되었다. Wikipedia dataset은 다양한 corpus를 포함하고 있지 않기에 제대로 된 학습을 기대하기 어렵다.&lt;/p&gt;

&lt;h2 id=&quot;considering-language-specific-properties&quot;&gt;Considering Language-specific Properties&lt;/h2&gt;

&lt;h3 id=&quot;rare-character-problem&quot;&gt;Rare “Character” Problem&lt;/h3&gt;

&lt;p&gt;English와 같은 Alphabet을 사용하는 language는 OOV가 적을 수 밖에 없다. 전체 character가 26개에 불과하기 때문이다. 반면 Korean은 syllable 기반이기 때문에 무려 11,172개의 character가 존재한다. 그러나 multilingual BERT에서는 이 중 오직 1,187개의 character만이 포함되었다. 나머지 character에 대해서는 제대로 학습이 되었다고 볼 수 없는 것이다.&lt;/p&gt;

&lt;h3 id=&quot;inadequacy-for-morphologically-rich-languages&quot;&gt;Inadequacy for Morphologically Rich Languages&lt;/h3&gt;

&lt;p&gt;Korean은 교착어이다. 때문에 English와 같은 language보다 훨씬 많은 형태소를 가짐은 물론 French나 German과 같은 굴절어 보다도 더 많은 형태소를 갖는다. 대표적인 교착어인 Japanese나 Korean은 동사의 활용형만 하더라도 수많은 다른 형태를 갖는다.&lt;/p&gt;

&lt;h3 id=&quot;lack-of-meaningful-tokens&quot;&gt;Lack of Meaningful Tokens&lt;/h3&gt;

&lt;p&gt;character-level의 Korean은 음절 단위인데, 각 음절의 구분은 발음에서의 가치만 있을 뿐 의미론적으로 큰 가치가 없는 구분이다. 오히려 자음/모음 (문자소) 단위가 의미를 갖는 경우가 더 많다. multilingual BERT는 모든 language에 universal하게 적용되는 model을 위해 character-level로 설계가 되었기 때문에 Korean NLP task에 적합하지 않다.&lt;/p&gt;

&lt;h2 id=&quot;large-scale-of-the-model&quot;&gt;Large Scale of the Model&lt;/h2&gt;

&lt;p&gt;XLNet이나 RoBERTa와 같은 대규모 model은 매우 많은 parameters와 큰 dataset, 큰 vocabulary를 사용했다. 그러나 이러한 대규모 model은 자원의 제약이 너무 많이 가해지기 때문에 작은 vocabulary, 적은 parameters, 적은 training dataset으로도 좋은 성능을 보이는 것을 목표로 했다.&lt;/p&gt;

&lt;h1 id=&quot;models&quot;&gt;Models&lt;/h1&gt;

&lt;p&gt;총 4가지 version의 KR-BERT에 대해 제시하고 비교한다. 우선 가장 작은 의미의 단위를 character-level(음절 단위)과 sub-character-level(자음/모음 단위)로 구분한다. 각각의 경우에 대해 BERT의  Original Tokenizer(WordPiece)를 사용한 것과 Bidirectional WordPiece Tokenizer를 사용한 것을 비교한다.&lt;/p&gt;

&lt;h2 id=&quot;subcharacter-text-representation&quot;&gt;Subcharacter Text Representation&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-KR-BERT-A-Small-Scale-Korean-Specific-Language-Model/11-13-2020-20.07.23.jpg&quot; alt=&quot;KR-BERT%20A%20Small-Scale%20Korean-Specific%20Language%20Mod%20efca31692a294cd195649aaf8d7f1881/11-13-2020-20.07.23.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;자음/모음 단위 구분을 통해 얻을 수 있는 이점은 동사나 형용사에 붙는 활용형을 정확하게 잡아낼 수 있다는 것이다. Table 3의 “갔”, “감”, “간”, “갈”은 모두 “가다”의 “가”에 여러 활용형이 붙은 경우이다. 하지만 이를 character-level로 분석하게 되면 모두 별개의 token이 된다. sub-character level로 분석을 함으로써 실제 “가다”의 의미를 파악해 낼 수 있는 것이다.&lt;/p&gt;

&lt;h2 id=&quot;subword-vocabulary&quot;&gt;Subword Vocabulary&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-KR-BERT-A-Small-Scale-Korean-Specific-Language-Model/11-13-2020-20.13.13.jpg&quot; alt=&quot;KR-BERT%20A%20Small-Scale%20Korean-Specific%20Language%20Mod%20efca31692a294cd195649aaf8d7f1881/11-13-2020-20.13.13.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;BPE의 성능은 vocabulary size에 따라 결정된다. 이는 heuristic하게 결정해야 하는데, 8000~20000 사이의 vocabulary size에 대해 test를 진행한 뒤 100,000 step에서의 Masked LM Accuracy를 비교한 결과 vocabulary size가 10,000일 때에 가장 성능이 좋다는 결론을 도출해냈다.&lt;/p&gt;

&lt;p&gt;이후 Korean text에서 빈번하게 사용되는 외국어(Alphabet, 한자, 일본어 등)에 대해 heuristic하게 token을 추가했다.&lt;/p&gt;

&lt;p&gt;[Table 4]에서 볼 수 있듯이 KR-BERT는 character-level과 sub-character-level 모두에 있어서 Multilingual BERT나 KorBERT보다 훨씬 작은 크기의 vocabulary를 사용했다.&lt;/p&gt;

&lt;h3 id=&quot;subword-tokenization&quot;&gt;Subword Tokenization&lt;/h3&gt;

&lt;p&gt;기존의 WordPiece Tokenization과 본 논문에서 새로 제안한 Bidirectional WordPiece Tokenization을 모두 사용해 둘을 비교한다.&lt;/p&gt;

&lt;h3 id=&quot;baselines&quot;&gt;Baselines&lt;/h3&gt;

&lt;p&gt;Multilingual BERT나 KorBERT는 BPE를 사용한 WordPiece Tokenization를 채택했다. 반면 KoBERT는 Unigram LM을 사용한 SentencePiece Tokenization을 채택했다.&lt;/p&gt;

&lt;h3 id=&quot;bidirectional-wordpiece-tokenizer&quot;&gt;Bidirectional WordPiece Tokenizer&lt;/h3&gt;

&lt;p&gt;BPE를 forward로만 진행하지 않고, backward로도 동시에 진행하는 것이다. forward와 backward 각각의 pair를 생성한 뒤, 두 후보 중 더 등장 빈도가 높은 쪽을 선택하게 된다. 이는 한국어의 문법적 특성에 따라 고안된 방식이다. 한국어의 명사는 상대적으로 긴 어근을 갖고 주로 짧은 접두사들이 앞에 붙게 된다. 반면 동사의 경우에는 짧은 어근을 갖고 주로 짧은 접미사들이 뒤에 붙게 된다. Bidirectional BPE는 이러한 경우들에 대해 적절한 tokenizing을 수행할 수 있도록 돕는다.&lt;/p&gt;

&lt;h2 id=&quot;comparison-with-other-korean-models&quot;&gt;Comparison with Other Korean Models&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-KR-BERT-A-Small-Scale-Korean-Specific-Language-Model/11-13-2020-20.55.26.jpg&quot; alt=&quot;KR-BERT%20A%20Small-Scale%20Korean-Specific%20Language%20Mod%20efca31692a294cd195649aaf8d7f1881/11-13-2020-20.55.26.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-KR-BERT-A-Small-Scale-Korean-Specific-Language-Model/11-13-2020-20.55.37.jpg&quot; alt=&quot;KR-BERT%20A%20Small-Scale%20Korean-Specific%20Language%20Mod%20efca31692a294cd195649aaf8d7f1881/11-13-2020-20.55.37.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;[Table 4]를 보면 KR-BERT는 Multilingual BERT, KorBERT에 비해 더 적은 vocabulary, 더 적은 parameter, 더 적은 data size를 갖는다는 것을 확인할 수 있다. 반면 KoBERT에 비해서는 더 많은 vocabulary, 더 많은 parameter를 갖지만 dataset은 더 적다.&lt;/p&gt;

&lt;p&gt;[Table 5]는 각 model들의 vocabulary이 어떤 비율로 구성되어 있는지를 보여준다. Korean Specific한 model들이 Multilingual BERT보다 Korean words와 Korean subwords의 비율이 압도적으로 높다는 것을 확인할 수 있다.&lt;/p&gt;

&lt;p&gt;[Table 6]은 실제로 Tokenization이 어떻게 이루어지는지 구체적인 단어 예시를 통해 보여준다. “냉장고”는 Multilingual BERT와 KorBERT, KoBERT에서 모두  “냉”, “장”, “고”로 tokenizing된다. 반면 KR-BERT에서는 token level과 tokenizer에 관계없이 모든 model에 있어서 “냉장고”라는 하나의 token으로 분류한다. “냉장고”를 각 character 별로 단순하게 tokenizing한 것에 비해 의미론적으로 더 알맞게 tokenization이 된  것이다.&lt;/p&gt;

&lt;p&gt;“춥다”는 Multilingual BERT에서는 아예 OOV로 판별이 된다. KorBERT와 KoBERT에서는 모두 “춥”, “다”로 tokenizing하게 된다. 그러나 KR-BERT에서는 character level은 “춥”, “다”로 다른 Korean Specific Model과 동일하게 tokenizing을 하지만, sub-character level에서는 “추”, “ㅂ다”로 tokenizing을 한다. sub-character level의 tokenizing이 더 적절한 결과를 도출해낸다는 것을 확인할 수 있다.&lt;/p&gt;

&lt;p&gt;“뱃사람”은 Multilingual BERT에서는 OOV이고, KorBERT와 KoBERT에서는 “뱃”, “사람”으로 tokenizing된다. character level의 KR-BERT에서도 마찬가지의 결과를 보여준다. 반면 sub-character level KR-BERT는 “배”, “ㅅ”, “사람”으로 tokenizing을 한다. Korean의 문법적 특성인 ‘사이시옷’까지 잡아낸 것이다.&lt;/p&gt;

&lt;p&gt;“마이크”는 Multilingual BERT와 KoBERT에서는 “마”, “이”, “크”로, KorBERT에서는 “마이”, “크”로 tokenizing된다. 반면 KR-BERT에서는 모든 model에서 동일하게 “마이크”로 tokenizing한다. 외래어 표기에 있어서 기존 model에 비해 더 강력한 성능을 보여주는 것이다.&lt;/p&gt;

&lt;h1 id=&quot;experiments-and-results&quot;&gt;Experiments and Results&lt;/h1&gt;

&lt;p&gt;여러 Korean NLP downstream task에 대해서 Multilingual BERT와 기존의 Korean Specific Model, KR-BERT를 비교한다. sentiment classification, question answering, named entity recognition, paraphrase detection에 대해서 실험을 진행했다.&lt;/p&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;h3 id=&quot;masked-lm-accuracy&quot;&gt;Masked LM Accuracy&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-KR-BERT-A-Small-Scale-Korean-Specific-Language-Model/11-13-2020-21.08.19.jpg&quot; alt=&quot;KR-BERT%20A%20Small-Scale%20Korean-Specific%20Language%20Mod%20efca31692a294cd195649aaf8d7f1881/11-13-2020-21.08.19.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;KR-BERT의 모든 model이 KoBERT보다 더 좋은 MLM Accuracy를 보여준다. 또한 KR-BERT 내에서 Bidirectional WordPiece를 사용한 model이 조금 더 나은 결과를 보여준다.&lt;/p&gt;

&lt;h3 id=&quot;downstream-tasks&quot;&gt;Downstream tasks&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-KR-BERT-A-Small-Scale-Korean-Specific-Language-Model/11-13-2020-21.08.26.jpg&quot; alt=&quot;KR-BERT%20A%20Small-Scale%20Korean-Specific%20Language%20Mod%20efca31692a294cd195649aaf8d7f1881/11-13-2020-21.08.26.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;sentiment classification은 Naver Sentiment Movie Corpus Dataset을, question answering은 KorQuAd Dataset을, named entity recognition는 KorNER Dataset을, paraphrase detection은 Korean Paired Question Dataset을 사용했다.&lt;/p&gt;

&lt;p&gt;모든 경우에 있어서 Multilingual BERT는 Korean Specific Model의 최고 성능을 능가하지 못했다. KR-BERT는 KorQuAD와 KorNER에서 가장 좋은 성능을 보여준다. 반면 NSMC와 Paraphrase Detection에 있어서는 KorBERT가 근소하게 더 높은 수치를 보여준다. 하지만 그럼에도 불구하고 KorQuAD와 KorNER에서의 KorBERT와 KR-BERT의 차이는 7%로 매우 높다는 점, KorBERT의 model size와 풍부한 dataset을 고려한다면 매우 유의미한 결과이다.&lt;/p&gt;

&lt;h2 id=&quot;analysis-of-downstream-tasks&quot;&gt;Analysis of Downstream Tasks&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-KR-BERT-A-Small-Scale-Korean-Specific-Language-Model/11-13-2020-21.08.31.jpg&quot; alt=&quot;KR-BERT%20A%20Small-Scale%20Korean-Specific%20Language%20Mod%20efca31692a294cd195649aaf8d7f1881/11-13-2020-21.08.31.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-KR-BERT-A-Small-Scale-Korean-Specific-Language-Model/11-13-2020-21.08.45.jpg&quot; alt=&quot;KR-BERT%20A%20Small-Scale%20Korean-Specific%20Language%20Mod%20efca31692a294cd195649aaf8d7f1881/11-13-2020-21.08.45.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;사실 KR-BERT model 중에서 sub-character Bidirectional WordPiece model이 일관되게 최고의 성능을 보여주지는 못한다. 하지만 그럼에도 다른 model들에 비해 일관되게 좋은 성능을 유지한다는 점에서 긍정적이다.&lt;/p&gt;

&lt;p&gt;NSMC의 경우에는 웹사이트 사용자들의 data라는 점에서 noise나 문법적 오류가 상대적으로 많고, unformal한 data이다. NER은 task의 특성 상 당연하게도 고유 명사가 많으므로 OOV의 비율이 높을 것이다. 또한 KorQuAD와 Paraphrase Detection은 상대적으로 formal한 data일 것이다.&lt;/p&gt;

&lt;p&gt;[Table 9]를 보면 bidirectional 방식과 sub-character level이 문법적 오류를 더 정확하게 잡아낸다는 점을 확인할 수 있다. “이영화”는 사실 “이”, “영화”의 두 단어로 구분되어야 하지만 중간의 공백이 삽입되지 않은 경우이다. 이에 대해 Bidirectional WordPiece KR-BERT만이 “이”, “영화”로 정확하게 tokenizing을 수행한다. Bidirectional이 아닌 KR-BERT는 “이영”, “화”로 잘못된 tokenizing을 수행했다.&lt;/p&gt;

&lt;p&gt;“재밌는뎅”의 경우에는 “재밌는데”에 “ㅇ”라는 nosie가 추가된 경우이다. 이는 sub-character level KR-BERT가 정확하게 잡아내는데, “재미”, “ㅆ”, “는데”, “ㅇ”로 tokenizing을 수행한다. 반면 character-level KR-BERT는 “재”, “밌”, “는”, “뎅”으로 잘못된 tokenizing을 수행한다.&lt;/p&gt;

&lt;p&gt;NER과 같은 OOV 비율이 높은 task에 대해서는 sub-character level이 더 좋은 성능을 보여준다. 이는 [Table 10]에서 OOV rate를 확인했을 때 sub-character level이 character level 대비 OOV가 훨씬 낮다는 점을 보면 당연한 결과이다.&lt;/p&gt;

&lt;p&gt;KorQuAD나 Paraphrase Detection과 같은 formal data의 경우에는 WordPiece가 Bidirectional WordPiece보다 더 좋은 성능을 보여준다.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;Korean-specific BERT model인 KR-BERT model을 제안했다. 기존의 Korean-specific model에 비해 더 작은 규모에서 더 적은 dataset으로 동등하거나 더 좋은 성능을 보여줬다. 이 과정에서 sub-character level tokenizing, Bidirectional BPE를 사용해 Korean의 문법적 특성을 잡아냈다.&lt;/p&gt;
</description>
        <pubDate>Tue, 19 Jan 2021 00:00:00 -0600</pubDate>
        <link>http://0.0.0.0:4000/KR-BERT-A-Small-Scale-Korean-Specific-Language-Model/</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/KR-BERT-A-Small-Scale-Korean-Specific-Language-Model/</guid>
        
        
      </item>
    
      <item>
        <title>Efficient Estimation Of Word Representations In Vector Space</title>
        <description>&lt;h1 id=&quot;efficient-estimation-of-word-representations-in-vector-space&quot;&gt;Efficient Estimation of Word Representations in Vector Space&lt;/h1&gt;
&lt;p&gt;title: Efficient Estimation of Word Representations in Vector Space
subtitle: Word2Vec
categories: Paper Review
tags: NLP
date: 2021-01-19 13:01:02 +0000
last_modified_at: 2021-01-19 13:01:02 +0000
—&lt;/p&gt;

&lt;p&gt;Archive Link: https://arxiv.org/abs/1301.3781
Created: Dec 12, 2020 7:48 PM
Field: NLP
Paper Link: https://arxiv.org/pdf/1301.3781.pdf
Status: completed
Submit Date: Jan 16, 2013&lt;/p&gt;

&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;one-hot encoding 방식은 word를 단순하게 표현하는 방법이다. word 자체가 갖는 정보를 담고 있지 않고 단순하게 index만을 담고 있는데, index 역시 word에 내재된 어떤 정보와도 관련이 없다. 본 논문에서는 word vector에 word 자체가 담고 있는 의미를 확실하게 담아내고자 했다. 구체적으로, 단순하게 유사한 단어들이 vector 공간에서 가까운 거리를 갖는 것에 그치지 않고 syntax, semantic 관점에서의 다양한 similarity를 반영하고자 했다. 동시에 one-hot encoding의 단점인 sparse vector problem을 해결해 dimension이 작으면서도 distribute한 word vector를 생성해냈다. 그 결과 sentence에서 단어가 등장하는 위치가 비슷한 word들의 vector가 가깝게 위치하는 것은 물론(syntax), “King” - “Man” + “Woman” = “Queen” 과 같은 vector 연산까지 정확하게 수행해낼 수 있었다(semantic).&lt;/p&gt;

&lt;h1 id=&quot;model-architectures&quot;&gt;Model Architectures&lt;/h1&gt;

&lt;p&gt;여러 model들을 비교하기 위해서 우선 computational complexity를 정의한다. model의 computational complexity는 #parameters로 정의한다. training complexity는 \(E \times T \times Q\)로 정의하는데, \(E\)는 #epochs이고, \(T\)는 len(training dataset), \(Q\)는 model specific하게 정의된 value이다.&lt;/p&gt;

&lt;h2 id=&quot;feedforward-neural-net-language-model-nnlm&quot;&gt;Feedforward Neural Net Language Model (NNLM)&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;example: “what will the fat cat sit on”, \(N=4\)&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-Efficient-Estimation-of-Word-Representations-in-Vector-Space/Untitled.png&quot; alt=&quot;Efficient%20Estimation%20of%20Word%20Representations%20in%20Ve%200c819dde7f4d46c08a5a496f2efa5329/Untitled.png&quot; /&gt;&lt;/p&gt;

    &lt;p&gt;출처: &lt;a href=&quot;https://wikidocs.net/45609&quot;&gt;https://wikidocs.net/45609&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;\(N\): input word 개수&lt;/p&gt;

&lt;p&gt;\(V\): vocabulary size&lt;/p&gt;

&lt;p&gt;\(D\): word representation dimenstion&lt;/p&gt;

&lt;p&gt;\(H\): hidden layer size&lt;/p&gt;

&lt;p&gt;일반적인 feed forward neural network를 사용한 language model이다.&lt;/p&gt;

&lt;p&gt;이전의 단어들 중 \(N\)개의 word에 대한 one-hot encoding을 input으로 받는다. 이 때 \(N\)은 n-gram에서의 \(N\)과 유사한 의미이며, hyper-parameter이다.&lt;/p&gt;

&lt;p&gt;각각의 input word에 대해 weight matrix \(W_P\)(\(V \times D\))를 곱한다. one-hot encoding과 \(W_P\)를 곱하는 것은 one-hot encoding에서 1인 index를 사용해 \(W_P\)에서 해당 row만 뽑아내는 것과 동일하다. 즉, \(W_P\)를 lookup table로 사용하는 것이다.&lt;/p&gt;

&lt;p&gt;이렇게 \(D\) 차원의 word vector를 얻어내는데, input에 대한 word vector들을 모두 concatenate해 projection layer \(P\)(\(N \times D)\)를 만들어낸다. 이는 여러 word에 대한 input을 하나의 matrix로 표현한 \(X\)(N \times V\()와\)W_P$$를 곱하는 연산과 동일하다. 이러한 연산의 의미를 직관적으로 이해해보자면 one-hot word vector가 아닌 embedding word vector를 얻는 과정이라고 볼 수 있다.&lt;/p&gt;

&lt;p&gt;이후 projection layer \(P\)에 새로운 weight matrix \(W_H\)(\(D \times H\))를 곱한 뒤 activation function에 넣어 hidden layer를 생성해낸다. 이는 여러 word embedding vector를 하나의 vector로 축약시키는 과정이다.&lt;/p&gt;

&lt;p&gt;hidden layer에서는 softmax와 cross entropy loss를 사용해 output에 대한 one-hot vector를 만들어낸다.&lt;/p&gt;

&lt;p&gt;전체 과정의 computational complexity는 다음과 같다.&lt;/p&gt;

\[Q = N \times D + N \times D \times H + H \times V\]

&lt;p&gt;위 수식에서 가장 부하가 큰 연산은 hidden layer에서 output을 만들어내는 연산인 \(H \times V\)이지만 hierarchical softmax를 사용하게 되면 \(H \times \log_2{V}\)로 연산량을 줄일 수 있다. 이 경우에는 가장 부하가 큰 연산은 projection layer에서 hidden layer를 만들어내는 연산인 \(N \times D \times H\)가 된다.&lt;/p&gt;

&lt;h2 id=&quot;recurrent-neural-net-language-model-rnnlm&quot;&gt;Recurrent Neural Net Language Model (RNNLM)&lt;/h2&gt;

&lt;p&gt;RNN을 사용한 language Model이다. RNN의 경우에는 projection layer를 제거한다. 또한 고정된 개수의 word만을 input으로 받는 NNLM과 달리 이전의 모든 word들에 대한 정보를 recurrent하게 hidden layer에 담게 된다.&lt;/p&gt;

&lt;p&gt;전체 과정의 computational complexitiy는 다음과 같다.&lt;/p&gt;

\[Q=H \times H + H \times V\]

&lt;p&gt;\(D\)를 \(H\)와 동일하게 만들었기 떄문에 위와 같은 수식이 된다. 역시나 동일하게 hierarchical softmax를 사용하면 \(H \times V\)를 \(H \times \log_2{V}\)로 줄일 수 있다. 이 경우네는 가장 부하가 큰 연산은 \(H \times H\)가 된다.&lt;/p&gt;

&lt;h1 id=&quot;new-log-linear-models&quot;&gt;New Log-linear Models&lt;/h1&gt;

&lt;p&gt;computational complexity를 줄이기 위해 2가지 단계를 제안한다.  continuous bag-of-words model(CBOW)을 사용하는 단계와 continuous skip-gram model(Skip-gram)을 사용하는 단계이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-Efficient-Estimation-of-Word-Representations-in-Vector-Space/12-12-2020-16.47.44.jpg&quot; alt=&quot;Efficient%20Estimation%20of%20Word%20Representations%20in%20Ve%200c819dde7f4d46c08a5a496f2efa5329/12-12-2020-16.47.44.jpg&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;continuous-bag-of-words-model&quot;&gt;Continuous Bag-of-Words Model&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-Efficient-Estimation-of-Word-Representations-in-Vector-Space/Untitled%201.png&quot; alt=&quot;Efficient%20Estimation%20of%20Word%20Representations%20in%20Ve%200c819dde7f4d46c08a5a496f2efa5329/Untitled%201.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;출처: &lt;a href=&quot;https://wikidocs.net/22660&quot;&gt;https://wikidocs.net/22660&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;NNLM에서 hidden layer를 제거한것이다. CBOW의 목표는 NNLM과 동일하게 word vector 1개를 예측하는 model이다. 다만 NNLM에서는 이전 word들만을 사용해 다음 word를 예측했다면, CBOW에서는 양방향(이전/이후)의 word 총 \(N\)개를 사용해 예측을 진행한다.&lt;/p&gt;

&lt;p&gt;CBOW는 NNLM에 비해 computational complexity를 감소시켰는데, NNLM에서의 projection layer는 activation function을 사용하지 않는 linear layer였다. 반면 hidden layer는 activation function을 사용하는 non-linear layer였다. hierarichial softmax를 사용한다는 가정 하에 가장 연산량이 많이 소요되는 layer가 hidden layer였으므로 이를 제거해 전체 연산량을 줄인 것이다. NNLM에서 hidden layer의 존재 의미는 여러 word embedding vector를 하나의 vector로 압축하는 것이었다면, CBOW에서는 이를 non-linear layer를 거치지 않고 단순하게 평균을 내게 된다. 따라서 CBOW의 projection layer는 word embedding vector(\(W_p\)에서의 row)들의 평균이다.&lt;/p&gt;

&lt;p&gt;computational complexity는 다음과 같다.&lt;/p&gt;

\[Q = N \times D + D \times \log_2{V}\]

&lt;h2 id=&quot;continuous-skip-gram-model&quot;&gt;Continuous Skip-gram Model&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-Efficient-Estimation-of-Word-Representations-in-Vector-Space/Untitled%202.png&quot; alt=&quot;Efficient%20Estimation%20of%20Word%20Representations%20in%20Ve%200c819dde7f4d46c08a5a496f2efa5329/Untitled%202.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;출처: &lt;a href=&quot;https://wikidocs.net/22660&quot;&gt;https://wikidocs.net/22660&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;CBOW와 유사하지만 input/output이 서로 뒤바뀐경우이다. 현재 word를 통해 이전, 이후의 word를 예측하는 model이다. 여러 word에 대해 prediction을 수행하기 때문에 당연하게도 연산량은 CBOW에 비해 많다. 하지만 skip-gram은 input word vector를 평균내지 않고 온전히 사용하기 때문에 등장 빈도가 낮은 word들에 대해 CBOW 대비 train 효과가 크다는 장점이 있다. CBOW에서는 각 word vector들을 평균내서 사용하기 때문에 등장 빈도가 낮은 word들은 제대로 된 학습을 기대하기 힘들다.&lt;/p&gt;

&lt;p&gt;computational complexity는 다음과 같다.&lt;/p&gt;

\[Q = C \times ( D + D \times \log_2{V})\]

&lt;p&gt;새로운 variable \(C\)가 등장하는데 \(C\)는 predict할 word의 개수와 관련된 값이다. 구체적으로, \(C\)는 predict할 word와 현재 word의 maximum distance이다. \([1,C)\)의 범위에서 random하게 value \(R\)을 뽑고, 현재 word 이전 \(R\)개, 이후 \(R\)개의 word에 대해서 predict를 수행한다. \(R\)의 기댓값은 \(\frac{1+(C-1)}{2}=\frac{C}{2}\)이고, predict 수행 횟수는 \(2R=2\times\frac{C}{2}=C\)이므로 전체 computational complexity는 1회 수행할 때의 값에 \(C\)를 곱한 것이다.&lt;/p&gt;

&lt;h1 id=&quot;results&quot;&gt;Results&lt;/h1&gt;

&lt;p&gt;word embedding의 성능을 측정하던 기존의 방식들은 유사한 word를 찾아내는 것이 대부분이었다. 예를 들면 “France”와 “Italy”가 유사한지를 측정하는 것 등이다. 본 논문에서는 이러한 방식에서 한 발 더 나아가 word 사이의 상관 관계를 뽑아내 다른 word에 적용시키는 방식을 도입했다. 예를 들면 “big”-“biggest”와 유사한 상관 관계를 갖는 word를 “small”에 대해 찾아 “smallest”를 맞추는 것이다. 본 논문에서 제시한 model로 학습한 word vector는 단순한 algebraic operation으로 이러한 task를 해결할 수 있다. X = vector(“biggest”) - vector(“big”) + vector(“small”)을 수행하면 X=vector(“smallest”)가 나올 것이다.&lt;/p&gt;

&lt;p&gt;이와 같은 semantic한 의미까지 제대로 담은 word vector를 활용할 경우 수많은 NLP task에서 뛰어난 성능 향상을 보이게 된다.&lt;/p&gt;

&lt;h2 id=&quot;task-description&quot;&gt;Task Description&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-Efficient-Estimation-of-Word-Representations-in-Vector-Space/12-12-2020-17.48.17.jpg&quot; alt=&quot;Efficient%20Estimation%20of%20Word%20Representations%20in%20Ve%200c819dde7f4d46c08a5a496f2efa5329/12-12-2020-17.48.17.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위와 같은 semantic, syntax 관계들을 목록화시켰다. 각 관계들에 대해 직접 word pair들을 수집하고, 각 word pair를 모두 섞어 random한 pair들을 만들어낸다. 이렇게 생성해낸 dataset으로 test를 수행하는 것이다. 이 때 정답과 완전히 동일한 word를 예측한 경우에만 정답으로 간주한다. 동의어나 유사어에 대해서도 오답 처리를 하기 때문에 사실상 100% accuracy는 불가능한 task이다.&lt;/p&gt;

&lt;h2 id=&quot;maximization-of-accuracy&quot;&gt;Maximization of Accuracy&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-Efficient-Estimation-of-Word-Representations-in-Vector-Space/12-12-2020-17.48.27.jpg&quot; alt=&quot;Efficient%20Estimation%20of%20Word%20Representations%20in%20Ve%200c819dde7f4d46c08a5a496f2efa5329/12-12-2020-17.48.27.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;word vector의 dimension과 #training words를 통해 실험을 진행했다. dimensionality나 #training words 중 하나를 고정시킨 뒤 다른 하나만을 증가시킬 경우 일정한 수준 이상으로 accuracy가 증가하지 않는 현상을 보였다. 기존의 많은 연구에서 단순히 training dataset의 크기만을 늘려가며 성능을 높이려 했지만, 많은 word가 train된다면 이에 대한 정보들을 담을 수 있는 충분한 dimension이 확보되어야 한다는 사실을 알 수 있다.&lt;/p&gt;

&lt;h2 id=&quot;comparison-of-model-architectures&quot;&gt;Comparison of Model Architectures&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-Efficient-Estimation-of-Word-Representations-in-Vector-Space/12-12-2020-17.48.35.jpg&quot; alt=&quot;Efficient%20Estimation%20of%20Word%20Representations%20in%20Ve%200c819dde7f4d46c08a5a496f2efa5329/12-12-2020-17.48.35.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;RNNLM이 가장 낮은 성능을 보였다. CBOW와 Skip-gram은 semantic, syntactic, relatedness에서 모두 NNLM을 능가했다. 특히나 Skip-gram은 Semantic Accuracy에서 다른 model들을 압도했다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-Efficient-Estimation-of-Word-Representations-in-Vector-Space/12-12-2020-17.48.43.jpg&quot; alt=&quot;Efficient%20Estimation%20of%20Word%20Representations%20in%20Ve%200c819dde7f4d46c08a5a496f2efa5329/12-12-2020-17.48.43.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;다른 여러 NNLM과 비교했을 때에도 CBOW와 skip-gram은 훨씬 더 좋은 성능을 보여준다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-Efficient-Estimation-of-Word-Representations-in-Vector-Space/12-12-2020-17.48.48.jpg&quot; alt=&quot;Efficient%20Estimation%20of%20Word%20Representations%20in%20Ve%200c819dde7f4d46c08a5a496f2efa5329/12-12-2020-17.48.48.jpg&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;microsoft-research-sentence-completion-challenge&quot;&gt;Microsoft Research Sentence Completion Challenge&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-Efficient-Estimation-of-Word-Representations-in-Vector-Space/12-12-2020-19.38.14.jpg&quot; alt=&quot;Efficient%20Estimation%20of%20Word%20Representations%20in%20Ve%200c819dde7f4d46c08a5a496f2efa5329/12-12-2020-19.38.14.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Microsoft Research Sentence Completion Challenge는 1040개의 sentence가 주어지는게, 각 sentence는 1개의 word가 빠져 있다. 각 sentence에서 빠진 word를 predict하는 task이다. 이 task에서 skip-gram 단독으로는 기존의 model들에 비해 다소 낮은 수치를 보였지만, RNNLM과 결합한 뒤에는 SOTA를 달성했다.&lt;/p&gt;

&lt;h1 id=&quot;examples-of-the-learned-relationships&quot;&gt;Examples of the Learned Relationships&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-Efficient-Estimation-of-Word-Representations-in-Vector-Space/12-12-2020-17.49.09.jpg&quot; alt=&quot;Efficient%20Estimation%20of%20Word%20Representations%20in%20Ve%200c819dde7f4d46c08a5a496f2efa5329/12-12-2020-17.49.09.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;단어 사이의 상관관계를 분석해 다른 단어에 대해 유사한 관계를 갖는 단어를 예측하는 task에서 본 논문의 model은 60%의 정확도를 달성했다. 더 높은 정확도를 달성하기 위해서는 더 많은 dataset을 사용하고, 또 각 단어 사이의 상관관계 vector를 여러 단어쌍 사이의 subtract vector의 평균으로 만들어내면 될 것이다.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;CBOW와 skip-gram이라는 새로운 word embedding 학습 방법을 제안했다. 기존의 여러 model들에 비해 연산량이 현저히 적고, 간단한 model임에도 매우 높은 성능을 보였다. 또한 word embedding vector의 syntax, semantic 성능을 측정할 수 있는 새로운 dataset을 제시했다.&lt;/p&gt;
</description>
        <pubDate>Tue, 19 Jan 2021 00:00:00 -0600</pubDate>
        <link>http://0.0.0.0:4000/Efficient-Estimation-of-Word-Representations-in-Vector-Space/</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/Efficient-Estimation-of-Word-Representations-in-Vector-Space/</guid>
        
        
      </item>
    
      <item>
        <title>Deep Contextualized Word Representations</title>
        <description>&lt;h1 id=&quot;deep-contextualized-word-representations&quot;&gt;Deep contextualized word representations&lt;/h1&gt;
&lt;p&gt;title: Deep contextualized word representations
subtitle: ELMo
categories: Paper Review
tags: NLP
date: 2021-01-19 13:01:33 +0000
last_modified_at: 2021-01-19 13:01:33 +0000
—&lt;/p&gt;

&lt;p&gt;Archive Link: https://arxiv.org/abs/1802.05365
Created: Jan 2, 2021 1:02 AM
Field: NLP
Paper Link: https://arxiv.org/pdf/1802.05365.pdf
Status: completed
Submit Date: Feb 15, 2018&lt;/p&gt;

&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;word2vec이나 glove와 같은 기존의 word embedding 방식은 다의어의 모든 의미를 담아내기 곤란하다는 심각한 한계점을 갖고 있다. ELMo(Embeddings from Language Models)는 이러한 한계점을 극복하기 위해 embedding에 sentence의 전체 context를 담도록 했다. pre-train된 LSTM layer에 sentence 전체를 넣어 각 word의 embedding을 구해내는 model이다. 이로 인해 기존의 embedding에 비해 복잡한 syntax, semantic한 특징들을 담아낼 수 있었고, 다의어 문제도 해결했다. LSTM은 forward, backward 2가지 방향을 사용하는데 각 LSTM은 다음 word를 예측하는 model이다. 이러한 LSTM layer를 다층으로 구성해 context-dependent한 word embedding을 생성해내게 된다. ELMo를 사용한 word embedding은 수많은 NLP task에서 성능 향상을 이끌어낼 수 있다.&lt;/p&gt;

&lt;h1 id=&quot;elmo-embeddings-from-language-models&quot;&gt;ELMo: Embeddings from Language Models&lt;/h1&gt;

&lt;p&gt;ELMo와 여타 word embedding model과의 가장 큰 차이점은 각 단어마다 고정된 word embedding을 사용하는 것이 아닌 pre-trained model 자체를 하나의 function 개념으로 사용한다는 것이다. 이전의 word2vec 등은 model을 train시킨 뒤 각 word마다의 embedding vector만을 추출해 사용했지만, ELMo는 word마다 embedding vector가 특정되지 않기에 이와 같은 방식이 불가능하고, ELMo model을 NLP model의 앞에 연결하는 방식으로 사용하게 된다.&lt;/p&gt;

&lt;h2 id=&quot;bidirectional-language-models&quot;&gt;Bidirectional language models&lt;/h2&gt;

&lt;p&gt;input sequence가 \(N\)개의 token들 (\(t_1, t_2, ..., t_N\))이라고 가정하자. \(t_k\)는 모두 각 word에 대한 context-independent token이다.&lt;/p&gt;

&lt;p&gt;forward LSTM은 \(t_1, ... , t_{k-1}\)이 주어졌을 때 \(t_k\)를 예측하는 model이다.&lt;/p&gt;

&lt;p&gt;\(j\)번째 LSTM layer에서 \(k\)번째 token에 대한 forward LSTM output은 \(\overrightarrow{h}_{k,j}\)이다. 마지막 LSTM layer의 output인 \(\overrightarrow{h}_{k,L}\)을 softmax에 넣어 최종적으로 \(t_{k+1}\)을 예측하게 된다.&lt;/p&gt;

\[p(t_1, t_2, ... , t_N) = \prod^N_{k=1}{p(t_k \vert t_1, t_2, ..., t_{k-1})}\]

&lt;p&gt;backward LSTM은 \(t_{k+1}, t_{k+2}, ... ,t_N\)이 주어졌을 때 \(t_k\)를 예측하는 model이다.&lt;/p&gt;

&lt;p&gt;\(j\)번째 LSTM layer에서 \(k\)번째 token에 대한 backward LSTM output은 \(\overleftarrow{h}_{k,j}\)이다. 마지막 LSTM layer의 output인 \(\overleftarrow{h}_{k,L}\)을 softmax에 넣어 최종적으로 \(t_{k-1}\)을 예측하게 된다.&lt;/p&gt;

&lt;p&gt;\(\)p(t_1, t_2, … , t_N) = \prod^N_{k=1}{p(t_k \vert t_{k+1}, t_{k+2}, …, t_{N})}\(\)&lt;/p&gt;

&lt;p&gt;biLM은 위의 두 LSTM을 결합한 것이다. 두 방향에 대한 log likelihood를 최대화하는 것을 목표로 한다.&lt;/p&gt;

\[\sum^N_{k=1} {(\log p(t_k \vert t_1, ... , t_{k-1} ; \Theta_x, \overrightarrow{\Theta}_{\text{LSTM}}, \Theta_s) + \log p(t_k \vert t_{k+1}, ... , t_{N} ; \Theta_x, \overleftarrow{\Theta}_{\text{LSTM}}, \Theta_s))}\]

&lt;p&gt;\(\Theta_x\)는 token representation(\(t_1, ... , t_N\))에 대한 parameter이고, \(\Theta_s\)는 softmax layer에 대한 parameter이다. 이 두 parameter는 전체 direction에 관계 없이 같은 값을 공유하지만, LSTM의 parameter들은 두 LSTM model이 서로 다른 값을 갖는다.&lt;/p&gt;

&lt;h2 id=&quot;elmo&quot;&gt;ELMo&lt;/h2&gt;

&lt;p&gt;ELMo에서는 새로운 representation을 사용하는데, 이를 얻기 위해서는 LSTM layer의 개수를 \(L\)이라고 했을 때 총 \(2L+1\)개의 representation을 concatenate해야 한다. input representation layer 1개와 forward, backward LSTM 각각 \(L\)개이다.&lt;/p&gt;

&lt;p&gt;\(\)R_k = {x_k, \overrightarrow{h}&lt;em&gt;{k,j}, \overleftarrow{h}&lt;/em&gt;{k,j} \vert j=1, … , L}\(\)&lt;/p&gt;

&lt;p&gt;input representation layer를 \(j=0\)으로, \(\overrightarrow{h}_{k,j}\)와 \(\overleftarrow{h}_{k,j}\)의 concatenation을 \(h_{k,j}\)로 표현한다면 다음과 같은 일반화된 수식으로 ELMO representation을 표현할 수 있다.&lt;/p&gt;

\[R_k = \{h_{k,j} \vert j=0, ..., L\}\]

&lt;p&gt;결국 \(R_k\)는 \(k\)번째 token에 대한 모든 representation이 연결되어 있는 것인데, 이를 사용해 최종 ELMo embedding vector를 만들어내게 된다.&lt;/p&gt;

\[\text{ELMo}^{task}_k=E(R_k;\Theta^{task}) = \gamma^{task} \sum^L_{j=0} {s_j^{task}h_{k,j}}\]

&lt;p&gt;각 LSTM layer의 output인 \(h_{k,j}\)를 모두 더하는데 이 때 각각에 softmax-normalized weights \(s_j\)를 곱한 뒤 더하게 된다. 당연하게도 \(\sum^L_{j=0}s_j^{task}=1\)이다. 마지막에 최종적으로 \(\gamma\)를 곱하게 되는데, scale parameter이다. \(s_j\)와 \(\gamma\)는 모두 learnable parameter이면서 optimization에서 매우 중요한 역할을 담당한다.&lt;/p&gt;

&lt;h2 id=&quot;using-bilms-for-supervised-nlp-tasks&quot;&gt;Using biLMs for supervised NLP tasks&lt;/h2&gt;

&lt;p&gt;supervised downstream task에 ELMo를 적용하는 구체적인 방법은 간단하다. 대부분의 supervised NLP model들의 input은 모두 context-independent token들의 sequence이다. 이러한 공통점 덕분에 동일한 방법으로 대부분의 task에 ELMo를 적용할 수 있다. 우선 NLP model의 input layer와 다음 layer 사이에 ELMo를 삽입한다. 이후 ELMo 이후 layer에서는 input을 \([x_k;\text{ELMo}_k^{task}]\)로 사용한다. 즉, input token과 ELMo embedding vector의 concatenation을 사용하는 것이다. NLP model을 train할 때에는 ELMo의 weight들은 모두 freeze시킨다. 즉, ELMo model은 NLP model이 train될 때 함께 train되지 않는다.&lt;/p&gt;

&lt;p&gt;SNLI, SQuAD와 같은 특정 task에서는 RNN의 output \(h_k\)를 \([h_k;\text{ELMo}_k^{task}]\)로 교체했을 때 더 좋은 성능을 보이기도 했다. 또한 일반적으로 dropout과 \(\lambda \Vert w \Vert^2_2\)를 loss에 더하는 regularization을 사용했을 때 더 좋은 성능을 보였다.&lt;/p&gt;

&lt;h2 id=&quot;pre-trained-bidirectional-language-model-architecture&quot;&gt;Pre-trained bidirectional language model architecture&lt;/h2&gt;

&lt;p&gt;ELMo는 기존의 pre-trained biLM와 큰 구조는 비슷하지만 몇가지 차이점이 존재하는데, 가장 큰 차이점은 LSTM layer 사이에 residual connection을 사용했다는 점이다. 이를 통해 input의 feature를 더 잘 전달하고 gradient vanishing을 해결할 수 있다. \(L=2\)개의 biLSTM layer를 사용했고, 4096개의 unit과 512차원의 projection을 사용했다. biLSTM의 input으로는 2048 character n-gram을 CNN에 넣는 char-CNN embedding을 사용했다.&lt;/p&gt;

&lt;h1 id=&quot;evaluation&quot;&gt;Evaluation&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-Deep-contextualized-word-representations/01-01-2021-23.00.50.jpg&quot; alt=&quot;Deep%20contextualized%20word%20representations%204e91492dc98641ada8f1fdfac764a546/01-01-2021-23.00.50.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ELMo를 단순하게 추가하는 것만으로도 baseline model에 비해 성능이 향상됐고, 이를 통해 SOTA를 달성할 수 있었다.&lt;/p&gt;

&lt;h1 id=&quot;analysis&quot;&gt;Analysis&lt;/h1&gt;

&lt;h2 id=&quot;alternate-layer-weighting-schemes&quot;&gt;Alternate layer weighting schemes&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-Deep-contextualized-word-representations/01-01-2021-23.01.02.jpg&quot; alt=&quot;Deep%20contextualized%20word%20representations%204e91492dc98641ada8f1fdfac764a546/01-01-2021-23.01.02.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ELMo representation을 사용하지 않고 단순하게 LSTM의 마지막 layer의 output (\(h_{k,L}\))을 사용하는 방법도 있다. 이러한 방식은 biLM, CoVe 등 기존의 많은 연구에서 시도되었는데 이와 ELMo representation을 사용한 경우를 비교해본다. Table 2의 Last Only는 마지막 LSTM layer의 output만을 word embedding으로 사용하는 경우이다.&lt;/p&gt;

&lt;p&gt;\(\lambda\)는 softmax-normalized weights \(s_j\)에 대한 regularization parameter인데, 0~1 사이의 값을 갖는다. \(\lambda\)가 1에 가까울수록 각 layer에서의 output들을 평균에 가깝게 계산해 최종 vector를 생성해내고 (\(s_j\)가 모두 유사한 값), \(\lambda\)가 0에 가까울수록 각 layer에서의 output들에 다양한 값들이 곱해서 더해진다.&lt;/p&gt;

&lt;p&gt;Table 2에서는 task에 관계 없이 동일한 경향성을 보이는데, baseline model보다 CoVe와 같은 마지막 LSTM layer의 output을 word embedding으로 사용한 model이 더 좋은 성능을 보였다. 또한 CoVe보다 ELMo가 더 좋은 성능을 보였는데, 이 중 \(\lambda\)가 낮은 경우가 더 좋은 결과를 보였다.&lt;/p&gt;

&lt;h2 id=&quot;where-to-include-elmo&quot;&gt;Where to include ELMo?&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-Deep-contextualized-word-representations/01-01-2021-23.01.17.jpg&quot; alt=&quot;Deep%20contextualized%20word%20representations%204e91492dc98641ada8f1fdfac764a546/01-01-2021-23.01.17.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위에서 언급했듯 supervised NLP model에 ELMo를 적용할 때에는 input layer의 직후에 ELMo를 삽입했다. SQuAD, SNLI, SRL의 baseline model은 모두 biRNN model인데, ELMo를 biRNN 직후에도 삽입을 한 뒤 성능을 비교했다. SQuAD와 SNLI에 있어서는 ELMo를 biRNN 이후에도 추가하는 것이 더 좋은 성능을 보여줬는데, 이는 SNLI와 SQuAD는 biRNN 직후 attention layer가 있는데, biRNN과 attention layer 사이에 ELMo를 추가함으로써 ELMo representation에 attention이 직접적으로 반영됐기 때문이라고 유추해 볼 수 있다.&lt;/p&gt;

&lt;h2 id=&quot;what-information-is-captured-by-the-bilms-representations&quot;&gt;What information is captured by the biLM’s representations?&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-Deep-contextualized-word-representations/01-01-2021-23.22.09.jpg&quot; alt=&quot;Deep%20contextualized%20word%20representations%204e91492dc98641ada8f1fdfac764a546/01-01-2021-23.22.09.jpg&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;word-sense-disambiguation&quot;&gt;Word sense disambiguation&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-Deep-contextualized-word-representations/01-01-2021-23.22.19.jpg&quot; alt=&quot;Deep%20contextualized%20word%20representations%204e91492dc98641ada8f1fdfac764a546/01-01-2021-23.22.19.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;WSD는 다의어의 의미를 구분짓는 task로 embedding이 얼마나 semantic 정보를 잘 담고 있는지에 대한 지표이다. ELMo는 WSD-specific한 model과 동등한 수치를, CoVe보다는 월등히 높은 수치를 달성했다. 주목할만한 점은 ELMo의 first LSTM layer의 output보다는 second layer (top layer)의 output이 WSD에서 좋은 성능을 보였다는 점이다.&lt;/p&gt;

&lt;h3 id=&quot;pos-tagging&quot;&gt;POS tagging&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-Deep-contextualized-word-representations/01-01-2021-23.22.28.jpg&quot; alt=&quot;Deep%20contextualized%20word%20representations%204e91492dc98641ada8f1fdfac764a546/01-01-2021-23.22.28.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;POS tagging은 word의 품사를 tagging하는 task로 embedding이 얼마나 syntax 정보를 잘 담고 있는지에 대한 지표이다. 여기서도 ELMo는 POS tagging-specific model과 동둥한 수준의 성능을, CoVe보다는 월등히 높은 성능을 보여줬다. WSD와는 다르게 오히려 first LSTM layer의 output이 top layer의 output보다 POS tagging에서 더 좋은 성능을 보였다는 점이 주목할 만하다.&lt;/p&gt;

&lt;h3 id=&quot;implications-for-supervised-tasks&quot;&gt;Implications for supervised tasks&lt;/h3&gt;

&lt;p&gt;결론적으로 ELMo에서 각 layer는 담고 있는 정보의 종류가 다르다고 할 수 있는데, 층이 낮은 layer(input layer에 가까운 layer)일수록 syntax 정보를, 층이 높은 layer(output layer에 가까운 layer)일수록 semantic 정보를  저장한다.&lt;/p&gt;

&lt;h2 id=&quot;sample-efficiency&quot;&gt;Sample efficiency&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-Deep-contextualized-word-representations/01-01-2021-23.47.27.jpg&quot; alt=&quot;Deep%20contextualized%20word%20representations%204e91492dc98641ada8f1fdfac764a546/01-01-2021-23.47.27.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ELMo의 사용은 일정 수준 이상의 성능 달성에 필요한 parameter update 횟수 및 전체 training set size를 획기적으로 줄여준다. SRL task에 있어서 ELMo 사용 이전 baseline model의 경우에는 486 epoch가 지나서야 score가 수렴했는데, ELMo를 추가하고 난 뒤에는 10 epoch만에 baseline model의 score를 능가했다.&lt;/p&gt;

&lt;p&gt;Figure 1에서는 같은 크기의 dataset에서 ELMo를 사용하는 경우가 훨씬 더 좋은 성능을 낸다는 것을 보여준다. 심지어 SRL task에서는 ELMo를 사용한 model이 training dataset의 단 1%을 학습했을 때 달성한 수치와 baseline model이 training dataset의 10%를 학습했을 때의 수치가 동일하다는 것을 확인할 수 있다.&lt;/p&gt;

&lt;h2 id=&quot;visualization-of-learned-weights&quot;&gt;Visualization of learned weights&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-Deep-contextualized-word-representations/01-01-2021-23.47.33.jpg&quot; alt=&quot;Deep%20contextualized%20word%20representations%204e91492dc98641ada8f1fdfac764a546/01-01-2021-23.47.33.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;softmax-normalized parameter \(s_j\)를 시각화한 것이다. ELMo를 biRNN의 input과 output에 사용했을 때를 각각 나눠 비교했다. ELMo가 input에 사용된 경우에는 대개 first LSTM layer가 선호되는 경향을 보였다. 특히나 SQuAD에서 이러한 경향성이 가장 두드러지게 나타났다. 반면 ELMO가 output에 사용된 경우에는 weight가 균형있게 분배되었지만 낮은 layer가 조금 더 높은 선호를 보였다.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;biLM을 사용해 높은 수준의 context를 학습하는 ELMo model을 제안했다. ELMo model을 사용하면 대부분의 NLP task에서 성능이 향상되었다. 또한 layer의 층이 올라갈수록 syntax보다는 semantic한 정보를 담아낸다는 사실도 발견해냈다. 때문에 어느 한 layer를 사용하는 것보다는 모든 layer의 representation을 결합해 사용하는 것이 전반적인 성능 향상에 도움이 된다는 결론을 내릴 수 있다.&lt;/p&gt;
</description>
        <pubDate>Tue, 19 Jan 2021 00:00:00 -0600</pubDate>
        <link>http://0.0.0.0:4000/Deep-contextualized-word-representations/</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/Deep-contextualized-word-representations/</guid>
        
        
      </item>
    
      <item>
        <title>Bert Pre Training Of Deep Bidirectional Transformers For Language Understanding</title>
        <description>&lt;h1 id=&quot;bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding&quot;&gt;BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding&lt;/h1&gt;
&lt;p&gt;title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
subtitle: BERT
categories: Paper Review
tags: NLP
date: 2021-01-19 13:00:09 +0000
last_modified_at: 2021-01-19 13:00:09 +0000
—&lt;/p&gt;

&lt;p&gt;Archive Link: https://arxiv.org/abs/1810.04805
Created: Sep 21, 2020 3:17 PM
Field: NLP
Paper Link: https://arxiv.org/pdf/1810.04805.pdf
Status: completed
Submit Date: Oct 11, 2018&lt;/p&gt;

&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;NLP에서도 pre-trained된 model을 사용하는 기법은 있었지만, pre-train에서 단방향의 architecture만 사용할 수 있다는 한계점이 있었다. 이는 양방향에서의 문맥 정보가 모두 중요한 token-level task에서 좋은 성능을 보이지 못하는 원인이 되었다. 본 논문에서는 MLM(Masked Language Model)을 사용해 bidirectional한 context도 담을 수 있는 BERT Model을 제시한다. MLM이란 문장에서 random하게 단어를 선택해 masking을 하고, model이 bidirectional context를 통해 해당 단어를 predict하도록 하는 것이다. BERT는 pre-train을 통해 task에 대한 학습이 아닌 language 자체에 대한 학습을 한 뒤, task에 맞게 fine-tuning을 하고, pre-training 과정에서 bidirectional한 context를 학습한다. 그 결과, BERT는 11개의 NLP task에서 SOTA를 달성했다.&lt;/p&gt;

&lt;h1 id=&quot;related-work&quot;&gt;Related Work&lt;/h1&gt;

&lt;p&gt;language의 context를 학습하는 pre-training 방법은 크게 2가지로 구분된다.&lt;/p&gt;

&lt;h2 id=&quot;feature-based-approach&quot;&gt;Feature-based Approach&lt;/h2&gt;

&lt;p&gt;task-specific한 model이 pre-trained된 model을 feature로 사용한다. pre-trained된 feature를 concat해서 model의 input으로 사용하는 방식 등이 있다.&lt;/p&gt;

&lt;h2 id=&quot;fine-tuning-approach&quot;&gt;Fine-tuning Approach&lt;/h2&gt;

&lt;p&gt;pre-train에서 일반적인 task를 위한 model을 학습하는데, 이 때 task-specific한 parameter를 최대한 배제한다. 이후 downstream task에서 이전에 학습된 parameter들을 task specific하게 fine-tuning한다. 즉, language에 대해 학습된 값으로 initialized된 상태에서 task specific한 layer를 추가한 뒤 fine-tuning을 시작한다. 이 때 pre-train된 parameter들과 task specific layer의 parameter들이 모두 학습된다.&lt;/p&gt;

&lt;h1 id=&quot;bert&quot;&gt;BERT&lt;/h1&gt;

&lt;p&gt;BERT는 Fine-tuning Approch를 채택했다. 따라서 Pre-training, Fine-tuning의 2가지 Step으로 구분된다.&lt;/p&gt;

&lt;p&gt;Pre-Training에서는 Unsupervised Learning을 통해 Language 자체의 representation을 학습한다. 특정 task에 부합하는 학습이 아닌 language의 일반적 특성을 학습한다. 이후 Fine-tuning에서는 각각의 task에 맞는 labeled data를 사용해 Supervised Learning을 수행한다. 이 과정에서 Pre-trained 단계에서 학습을 통해 얻어진 parameters의 값들도 변경된다. 즉, Pre-training은 parameters의 초기값이 language의 일반적 특성을 담는 값으로 시작하도록 설정하는 역할이다.&lt;/p&gt;

&lt;h2 id=&quot;architecture&quot;&gt;Architecture&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding/Untitled.png&quot; alt=&quot;BERT%20Pre-training%20of%20Deep%20Bidirectional%20Transforme%2017fcc0b61a15468490e784be91487627/Untitled.png&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;inputoutput-representation&quot;&gt;Input/Output Representation&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding/Untitled%201.png&quot; alt=&quot;BERT%20Pre-training%20of%20Deep%20Bidirectional%20Transforme%2017fcc0b61a15468490e784be91487627/Untitled%201.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Input은 Token Embedding + Segment Embedding + Position Embedding이다. Token은 실제 word에 대한 embedding, Segment Embedding은 몇번째 Sentence에 포함되는지에 대한 Embedding, Position Embedding은 Input에서 몇번째 word인지에 대한 Embedding이다. Transformer에서도 언급했듯이 병렬 처리를 위해 RNN을 제거하고, Sequential한 정보를 보존하기 위해 Position Embedding을 추가한 것이다. BERT는 Input으로 최대 2개의 Sentence까지 입력받을 수 있는데, 이는 Q&amp;amp;A task와 같은 2개의 문장에 대한 task도 처리할 수 있게 하기 위함이다. 이를 처리하기 위해 Seperate Token SEP을 추가했다. 이와 별개로 Classification을 위한 CLS Token도 Input Sequence의 제일 앞에 항상 위치하는데, Transformer Encoder의 최종 Output에서 CLS Token과 대응되는 값은 Classification을 처리하기 위해 sequence representation을 종합해서 담게 된다.&lt;/p&gt;

&lt;h2 id=&quot;pre-training&quot;&gt;Pre-training&lt;/h2&gt;

&lt;p&gt;BERT는 MLM과 NSP라는 2가지의 Unsupervised task를 사용해 Pre-training을 수행한다.&lt;/p&gt;

&lt;h3 id=&quot;mlm-masked-language-model&quot;&gt;MLM (Masked Language Model)&lt;/h3&gt;

&lt;p&gt;기존의 전통적인 Pre-train은 left to right model과 right to left model을 단순하게 concat한 뒤 사용했다는 점에서 제대로 된 Bidirectional context를 담지 못했다. BERT는 MLM을 사용해 진정한 의미의 Bidirectional context를 담게 된다. 기존의 Model들이 unidirectional model의 결과들을 concat해서 사용한 이유는, bidirectional model은 word 자기 자신을 masking하더라도 다층 Layer에서는 간접적으로 자기 자신에 대한 정보를 알 수 있기에 제대로 된 학습이 불가능했기 때문이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding/07-25-2020-16.59.58.jpg&quot; alt=&quot;BERT%20Pre-training%20of%20Deep%20Bidirectional%20Transforme%2017fcc0b61a15468490e784be91487627/07-25-2020-16.59.58.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 왼쪽 Model은 BERT로, Bidirectional Model이다. 반면 오른쪽 Model은 OpenAI GPT로, left to right Unidirectional Model이다. Bidirectional Model에서 다층 Layer일 경우에는 이전 Layer의 모든 Output에서 모든 Token에 대한 정보를 담게 되기 때문에 특정 Token을 Masking했다고 하더라도 다음 Layer에서는 자기 자신에 대한 정보를 간접적으로 참조할 수 있게 된다. 반면 Unidirectional Model에서는 이런 일이 발생하지 않는다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding/08-08-2020-17.23.54.jpg&quot; alt=&quot;BERT%20Pre-training%20of%20Deep%20Bidirectional%20Transforme%2017fcc0b61a15468490e784be91487627/08-08-2020-17.23.54.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이를 해결하기 위해 BERT는 Masking을 하되, 그 중 80%에 대해서만 실제 MASK Token으로 변경하고, 10%에 대해서는 random한 다른 Token으로, 나머지 10%에 대해서는 변경을 하지 않았다. Masking을 하는 비율은 전체 Word 중에서 15%만 수행했으므로, 실제로 MASK Token으로 변경되는 비율은 12%밖에 되지 않는다. 이를 통해 얻을 수 있는 이점은 Model이 모든 Token에 대해서 실제로 맞는 Token인지 의심을 할 수 밖에 없게 되기에 제대로 된 학습을 이뤄낼 수 있다. 이는 Bidirectional Model의 한계인 간접 참조도 해결했다.&lt;/p&gt;

&lt;h3 id=&quot;nsp-next-sentence-prediction&quot;&gt;NSP (Next Sentence Prediction)&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding/08-08-2020-17.25.30.jpg&quot; alt=&quot;BERT%20Pre-training%20of%20Deep%20Bidirectional%20Transforme%2017fcc0b61a15468490e784be91487627/08-08-2020-17.25.30.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;QA, NLI와 같은 task는 token단위보다 sentence 단위의 관계가 더 중요하다. 위의 MLM만으로 Pre-training을 하게 될 경우 token level의 정보만을 학습하기 때문에 NSP를 통해 sentence level의 정보도 담기로 한다. 두 문장이 서로 연결되는 문장인지를 isNext, NotNext의 Binary classification으로 해석하게 된다. 50%의 확률로 isNext, NotNext의 data를 생성한 뒤에 학습을 시킨다. 이 때 위에서 언급한 CLS Token이 주요하게 사용된다.&lt;/p&gt;

&lt;h3 id=&quot;pre-training-detail&quot;&gt;Pre-training Detail&lt;/h3&gt;

&lt;p&gt;실제 Pre-training 단계는 다음과 같이 진행된다.&lt;/p&gt;

&lt;p&gt;corpus에서 문장들을 뽑아내 두 문장의 sequence로 만들고, 각각 A, B를 Segment Embedding으로 부여한다. 50%의 확률로 B 문장은 실제로 A문장에 이어지는 문장이고(IsNext), 50%의 확률로 A문장에 이어지지 않는 문장이다(NotNext). 이후 Word Piece Tokenization을 한 뒤, Masking을 수행한다.&lt;/p&gt;

&lt;p&gt;Hyperparameters는 다음과 같다.&lt;/p&gt;

&lt;p&gt;batch size = 256 sequences&lt;/p&gt;

&lt;p&gt;sequence size = 512 tokens&lt;/p&gt;

&lt;p&gt;#epoch = 40&lt;/p&gt;

&lt;p&gt;Optimizer: Adam (learning rate = 1e-4, B_1 = 0.9, B_2 = 0.999, L2 weight decay = 0.01)&lt;/p&gt;

&lt;p&gt;dropout: 0.1 in all layers&lt;/p&gt;

&lt;p&gt;activation function: gelu&lt;/p&gt;

&lt;p&gt;loss function: sum of the mean MLM likelihood + mean NSP likelihood&lt;/p&gt;

&lt;p&gt;Pre-training에 BERT_BASE는 16개의 TPU로 4일, BERT_LARGE는 64개의 TPU로 4일이 소요됐다.&lt;/p&gt;

&lt;h2 id=&quot;fine-tuning&quot;&gt;Fine-tuning&lt;/h2&gt;

&lt;p&gt;Fine-tuning은 Pre-training에 비해 매우 빠른 시간 내에 완료된다. Fine-tuning에서는 각각의 task에 specific하게 input size, output size 등을 조정해야 한다. 또한 token-level task일 경우에는 모든 token들을 사용하고, sentence-level task일 경우에는 CLS token을 사용한다.&lt;/p&gt;

&lt;p&gt;Hyperparameters는 대부분은 pre-training과 동일하게 진행하는 것이 좋지만, batch size, learning rate, #epochs는 task-specific하게 결정해야 한다. 하지만 다음의 값들에 대해서는 대체적으로 좋은 성능을 보였다.&lt;/p&gt;

&lt;p&gt;batch size: 16, 32&lt;/p&gt;

&lt;p&gt;learing rate: 5e-5, 3e-5, 2e-5&lt;/p&gt;

&lt;p&gt;#epochs: 2, 3, 4&lt;/p&gt;

&lt;p&gt;dataset의 크기가 클 수록 Hyperparameter의 영향이 줄어들었으며, 대부분의 경우 Fine-tuning은 매우 빠른 시간 내에 완료되기 때문에 많은 parameters에 대해 테스트를 진행할 수 있었다.&lt;/p&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding/08-08-2020-18.06.17.jpg&quot; alt=&quot;BERT%20Pre-training%20of%20Deep%20Bidirectional%20Transforme%2017fcc0b61a15468490e784be91487627/08-08-2020-18.06.17.jpg&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;glue-general-language-understanding-evaluation&quot;&gt;GLUE (General Language Understanding Evaluation)&lt;/h3&gt;

&lt;p&gt;GLUE 에 맞춰 fine-tuning을 진행한다. CLS token에 대응하는 hidden layer의 state 값이 h차원의 vector C라고 한다면, Classification을 위해 k x h (k는 classification할 label의 수) 차원의 weight matrix W를 생성한다.&lt;/p&gt;

\[log(softmax(CW^T))\]

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding/08-06-2020-21.18.26.jpg&quot; alt=&quot;BERT%20Pre-training%20of%20Deep%20Bidirectional%20Transforme%2017fcc0b61a15468490e784be91487627/08-06-2020-21.18.26.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;BERT_BASE는 L=12, H=768, A=12, #Parameters=110M이고, BERT_LARGE는 L=24, H=1024, A=16, #Parameters=340M이다. (L: Layer 개수, H: hidden size, A: self-attention head 개수)&lt;/p&gt;

&lt;p&gt;batch size는 32, #epoch=3로 학습을 진행했다. learning rate는 5e-5, 4e-5, 3e-5, 2e-5 중 가장 잘 학습이 진행되는 것으로 선택했다. fine-tuning이 unstable한 경우가 있어서, random하게 restart하는 과정도 추가했다.&lt;/p&gt;

&lt;p&gt;BERT model은 GLUE의 모든 task에서 SOTA를 달성했다. base, large 모두 다른 model들을 4.5~7%정도 능가했다. 또한 BERT_LARGE model이 BERT_BASE를 모든 경우에서 능가했다. Model의 크기가 클 수록 성능이 좋다는 의미이다.&lt;/p&gt;

&lt;p&gt;BERT_BASE와 OpenAI GPT는 attention masking에서의 차이를 제외하고 모두 동일한 조건이었음에도 불구하고 4.5%의 성능 차이를 보였다. 이를 통해 unidirectional attention이 아닌 bidirectional attention을 적용한 것이 성능 향상에 매우 큰 기여를 했다는 것을 알 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;squad-v11&quot;&gt;SQuAD v1.1&lt;/h3&gt;

&lt;p&gt;SQuAD에 맞춰 Fine-tuning을 진행한다. SQuAD는 Q&amp;amp;A dataset이다. fine-tuning 학습 과정에서 Answer 문장의 시작 token S와 끝 token E를 구해내게 된다. transformer layer의 마지막 state들을 이용해 해당 값이 S나 E일 확률을 구하게 되는데, S와 T_i를 dot product한 후 softmax로 확률값으로 변환한다.&lt;/p&gt;

\[P_i = \frac{e^{(S or E) \cdot T_i}}{\sum _j{e^{(S or E) \cdot T_j}}}\]

&lt;p&gt;위의 score를 이용해서 \(S \cdot T_i\)와 \(E \cdot T_j\)를 더한 값이 가장 큰 &amp;lt;i, j&amp;gt;쌍 (단, j ≥ i)을 최종 Answer 영역으로 정한다.&lt;/p&gt;

\[\hat s_{i,j} = max_{j \geq i}\left( S \cdot T_i + E \cdot T_j\right)\]

&lt;h3 id=&quot;squad-v20&quot;&gt;SQuAD v2.0&lt;/h3&gt;

&lt;p&gt;SQuAD v1.1에서 대답이 불가능한 질문을 포함한 dataset이다. fine-tuning 학습 과정에서 CLS token을 이용해 대답 가능 여부를 Binary Classification하면서 token C를 구한다.&lt;/p&gt;

&lt;p&gt;대답이 불가능할 경우의 Score는 다음으로 계산한다.&lt;/p&gt;

\[s_{null} = S \cdot C + E \cdot C\]

&lt;p&gt;대답이 가능한 경우의 Score는 SQuAD v1.1과 동일하다. 대답 가능 여부는 두 Score를 비교해 판단하게 된다. 이 때 threshhold값 r이 사용된다.&lt;/p&gt;

\[\hat s_{i,j} &amp;gt; s_{null} + r\]

&lt;h3 id=&quot;swag-situations-with-adversarial-generations&quot;&gt;SWAG (Situations With Adversarial Generations)&lt;/h3&gt;

&lt;p&gt;일반적인 추론을 하는 dataset이다. sentence 1개에 추가적으로 4개의 sentence가 주어지고, 4개의 sentence 중 가장 적합한 것을 선택하는 task이다. 각각의 보기들에 대해 앞의 sentence와 묶어 segment embedding A, B를 부여한 data쌍을 만들어낸다. Score는 CLS token에 대응하는 token C와 task specific parameter 1개를 dot product한 뒤 softmax를 수행해서 구한다.&lt;/p&gt;

&lt;h2 id=&quot;ablation-studies&quot;&gt;Ablation Studies&lt;/h2&gt;

&lt;p&gt;동일한 환경(pre-training data, fine-tuning scheme, hyperparameters)에서 특정 조건만을 변경해 어느 정도의 영향을 끼치는지 분석했다.&lt;/p&gt;

&lt;h3 id=&quot;effect-of-pre-training-tasks&quot;&gt;Effect of Pre-training Tasks&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding/08-08-2020-14.49.27.jpg&quot; alt=&quot;BERT%20Pre-training%20of%20Deep%20Bidirectional%20Transforme%2017fcc0b61a15468490e784be91487627/08-08-2020-14.49.27.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;No NSP는 pre-training 단계에서 MLM만 수행하고, NSP는 수행하지 않은 model이다. LTR &amp;amp; No NSP는 NSP 는 수행하지 않고, MLM 대신 Left to Right의 Unidirectional attention을 적용한 model이다.&lt;/p&gt;

&lt;p&gt;BERT와 No NSP를 비교함으로써 NSP Pre-training이 성능 향상에 영향을 끼친다는 것을 알 수 있다. 한편, No NSP와 LTR &amp;amp; No NSP를 비교함으로써 Bidirectional Attention(MLM)이 성능 향상에 매우 큰 영향을 준다는 것을 알 수 있다. Token Level에서 Right to Left Context 정보를 얻기 위해 MLM이 아닌 BiLSTM을 추가했다. 해당 Model은 LTR &amp;amp; No NSP에 비해 SQuAD와 같은 task에서 매우 큰 성능 향상을 보였지만, 여타 task에서는 오히려 성능 하락을 보였다.&lt;/p&gt;

&lt;p&gt;물론 Bidirectional Context를 담기 위해 LTR과 RTL을 각각 학습시킨 뒤 두 token을 concatenation하는 방법도 있지만, 이는 비용이 2배나 높고, QA와 같은 RTL 학습이 불가능한 task에서는 적용이 불가능하다는 점, 결론적으로 MLM과 같은 Deep Bidirectional Model에 비해 성능이 낮다는 점에서 비효율적이다.&lt;/p&gt;

&lt;h3 id=&quot;effect-of-model-size&quot;&gt;Effect of Model Size&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding/08-08-2020-15.23.06.jpg&quot; alt=&quot;BERT%20Pre-training%20of%20Deep%20Bidirectional%20Transforme%2017fcc0b61a15468490e784be91487627/08-08-2020-15.23.06.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;GLUE task 중 3개를 뽑아 Model Size에 따라 성능을 측정했다. Model Size가 증가할수록 성능이 높아지는 경향을 확인할 수 있다. 특히 MRPC task는 pre-training task와 차이가 큰 task이면서 3600개의 적은 labeled training data를 사용했음에도 불구하고 Model Size가 증가함에 따라 성능도 향상됐다. 이를 통해 Model Size의 증가는 번역과 Langauge Modeling과 같은 큰 scale의 task에서도 성능 향상에 기여함은 물론, 충분한 pre-training이 있었다는 전제 하에 작은 scale의 task에서도 성능 향상에 기여함을 알 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;feature-based-approach-with-bert&quot;&gt;Feature-based Approach with BERT&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding/08-08-2020-17.02.13.jpg&quot; alt=&quot;BERT%20Pre-training%20of%20Deep%20Bidirectional%20Transforme%2017fcc0b61a15468490e784be91487627/08-08-2020-17.02.13.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;지금까지의 BERT는 모두 fine-tuning model이었다. Feature-based Approach가 갖는 장점은 크게 두 가지로 정리할 수 있다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Transformer Encoder의 Output에 몇몇 Layer를 추가하는 간단한 작업만으로는 해결할 수 없는 task들이 존재한다.&lt;/li&gt;
  &lt;li&gt;매우 큰 pre-training model의 경우 pre-trained된 features를 계속 update하지 않고 고정된 값으로 사용함으로써 연산량을 획기적으로 줄일 수 있다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;NER(Named Entity Recognition) task에 대해 Feature-based Approach를 적용해본다. Fine-tuning Step을 제거하고, Transformer Layer의 output을 그대로 768-dimensional BiLSTM의 input으로 사용한다. 그 뒤 classification layer를 통과시켜 결과를 도출해낸다. Fine-tuning Approach를 적용한 BERT_LARGE Model이 SOTA를 달성했지만, Fine-tunning을 적용한 BERT_BASE model과 Feature-based를 적용한 BERT_BASE Model의 F1 Score는 0.3밖에 차이가 나지 않는다. 연산량을 고려한다면 충분히 가치있는 결과이다. 결론적으로, BERT Model은 Fine-tunning Approach와 Feature-based Approach에서 모두 기존 Model들을 뛰어넘는다.&lt;/p&gt;

&lt;h3 id=&quot;effect-of-number-of-training-steps&quot;&gt;Effect of Number of Training Steps&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding/08-08-2020-17.52.27.jpg&quot; alt=&quot;BERT%20Pre-training%20of%20Deep%20Bidirectional%20Transforme%2017fcc0b61a15468490e784be91487627/08-08-2020-17.52.27.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Training을 많이 수행할 수록 성능은 향상되지만, 일정 수준 이상을 지나면 점차 converge하게 된다. MLM과 LTR을 비교했을 때 MLM이 수렴이 더 늦게 일어나기 때문에 Training에 더 많은 시간이 소요된다고 볼 수 있다. 하지만 절대적인 성능 수치는 시작과 거의 동시에 LTR을 뛰어넘는다.&lt;/p&gt;

&lt;h3 id=&quot;different-masking-procedure&quot;&gt;Different Masking Procedure&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding/08-08-2020-17.58.09.jpg&quot; alt=&quot;BERT%20Pre-training%20of%20Deep%20Bidirectional%20Transforme%2017fcc0b61a15468490e784be91487627/08-08-2020-17.58.09.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Masking Rate를 다르게 하며 성능을 비교해보자. MASK는 실제로 [MASK] token으로 변경된 비율을, SAME은 동일한 word로 남아있는 비율을, RND는 random한 다른 word로 변경된 비율을 뜻한다. BERT는 각각 80%, 10%, 10%를 채택했다. MASK가 100%나 RND가 100%인 경우에 성능이 최악이라는 것을 알 수 있다.&lt;/p&gt;
</description>
        <pubDate>Tue, 19 Jan 2021 00:00:00 -0600</pubDate>
        <link>http://0.0.0.0:4000/BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding/</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding/</guid>
        
        
      </item>
    
      <item>
        <title>Attention Is All You Need</title>
        <description>&lt;h1 id=&quot;attention-is-all-you-need&quot;&gt;Attention is All You Need&lt;/h1&gt;
&lt;p&gt;title: Attention is All You Need
subtitle: Transformer
categories: Paper Review
tags: NLP
date: 2021-01-19 13:00:41 +0000
last_modified_at: 2021-01-19 13:00:41 +0000
—&lt;/p&gt;

&lt;p&gt;Archive Link: https://arxiv.org/abs/1706.03762
Created: Sep 21, 2020 3:16 PM
Field: NLP
Paper Link: https://arxiv.org/pdf/1706.03762.pdf
Status: completed
Submit Date: Jun 12, 2017&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;cleanUrl&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;/nlp/attention-is-all-you-need&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;disqus&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;true&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;RNN과 LSTM을 사용한 Neural Network 접근 방식은 Sequencial Transduction Problem에서 매우 좋은 성능을 달성했다. 그 중 특히 Encoder-Decoder를 사용한 Attention Model이 state-of-art를 달성했다. 하지만 Recurrent Model은 본질적으로 한계가 존재하는데, 바로 Parallelization이 불가능하다는 문제점이다. 이는 Sequence의 길이가 긴 상황에서 매우 큰 단점이 된다. 최근의 연구들이 computation을 최소화하는 방향으로 Model의 성능 향상을 이뤄내기는 했지만, 결국 Recurrent Model의 본질적인 문제는 해결하지 못했다. 본 논문에서 소개하는 Transformer Model은 RNN을 제거해 Recurrent Model의 문제점에서 벗어났고,  Parallelization을 가능하게 했다. 이에 따라 매우 좋은 성능을 달성했다.&lt;/p&gt;
&lt;h1 id=&quot;model-architecture&quot;&gt;Model Architecture&lt;/h1&gt;

&lt;p&gt;Transformer Model은 attention seq2seq model과 비슷한 구조를 지닌다. Encoder-Decoder가 존재하고, fixed length의 하나의 context vector를 사용하는 방식이 아닌, 하나의 output word마다 각기 다른 새로운 attention을 갖는 context vector를 생성해 활용한다. 차이점은 RNN을 제거했다는 점이다. NLP에서 RNN을 사용하는 가장 큰 이유는 sequential 정보를 유지하기 위함(각 단어들의 순서 및 위치 정보를 활용하기 위함)이다. Transformer에서는 RNN 대신 FC Layer를 사용하되, 각 word vector마다 positional Encoding 과정을 추가해 각 word의 position 정보를 word vector 안에 추가했다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-Attention-is-All-You-Need/06-25-2020-16.22.01.jpg&quot; alt=&quot;Attention%20is%20All%20You%20Need%20c507a427409b4c17b5611326901ab369/06-25-2020-16.22.01.jpg&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;encoder&quot;&gt;Encoder&lt;/h3&gt;

&lt;p&gt;Transformer의 Encoder는 6개의 동일한 Encoder Layer를 Stack 구조로 쌓아올린 형태이다. 각각의 Encoder Layer는 2개의 SubLayer로 구분되는데, Self Attention Layer와 Feed Forward Layer이다. Self Attention Layer에서는 word vector에 attention을 담는다. 이 결과를 Feed Forward Layer를 거쳐 다음 Encoder Layer의 input으로 넣는다. 이러한 과정을 6회 반복해 최종 output을 Decoder로 넘겨준다. 각각의 Self Attention Layer는 Attention을 word vector에 담는 기능을 수행하는데, 이를 중첩해 반복하면 단어 단위가 아닌 점차 전체 문맥에서의 Attention을 담게 된다. Self Attention Layer의 동작은 아래에서 자세히 살펴보겠다. 각 SubLayer의 결과에는 항상 Normalization이 수행되는데, 이 때 SubLayer를 통과하기 전 input이 그대로 더해지게 된다. 이를 Residual Connection이라고 한다. Residual Connection은  BackProp시 gradient vanishing을 최소화시켜 Model 성능 향상을 돕는다.&lt;/p&gt;

&lt;h3 id=&quot;self-attention-layer&quot;&gt;Self Attention Layer&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-Attention-is-All-You-Need/06-25-2020-16.15.49.jpg&quot; alt=&quot;Attention%20is%20All%20You%20Need%20c507a427409b4c17b5611326901ab369/06-25-2020-16.15.49.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-Attention-is-All-You-Need/06-25-2020-16.16.23.jpg&quot; alt=&quot;Attention%20is%20All%20You%20Need%20c507a427409b4c17b5611326901ab369/06-25-2020-16.16.23.jpg&quot; /&gt;&lt;/p&gt;

\[Attention\left(Q,K,V\right)=softmax\left(\frac{QK^T}{\sqrt{d_k}}\right)V\]

&lt;p&gt;Self Attention Layer는 word vector에 attention을 추가하는 layer이다. Query, Key, Value가 사용된다. Query, Key, Value는 각각 input word에 대해 특정한 weight matrix를 곱한 결과값이다. 해당 weight matrix들은 학습되는 parameter들이다. 우선 각 단어들을 기준으로 나눠 살펴보자.&lt;/p&gt;

&lt;p&gt;현재 단어의 word vector를 \(x_i\)라고 하면, query는 \(x_i\)에 대한 query \(q_i\)이다. input word \(x_i\)에 대한 연관성을 묻는 Query(질의)이다. Key와 Value는 \(x_i\)와 연관성을 검사하는 대상 \(x_j\)에 대한 값이다. 사실 Key와 Value는 동일한 값인데, 사용되는 위치만이 다르다. \(q_i\)와 \(k_j\)를 곱하면 \(x_i\)와 \(x_j\)가 연관된 수치를 의미하는 score가 생성된다. 이를 Dependency라고 한다. Dependency를 softmax에 넣으면 확률값이 생성된다. 이를 \(v_j\)와 곱하고, 해당 값을 모두 더하면 attention이 담기긴 vector가 완성된다.&lt;/p&gt;

&lt;p&gt;query, key, value는 모두 위의 단어 예시에서 vector 단위였다. 이를 concatenate해 하나의 matrix로 만들 수 있다. 그렇다면 dot product 연산으로 모든 sentence 전체에 대한 attention matrix를 구할 수 있다. 이렇게 병렬 처리를 함으로써 model의 학습 속도를 증가시켰다. 위의 좌측 이미지는 이러한 과정을 표현한 것이다. Scale은 \(d_k\)의 제곱근으로 Q와 K의 dot product 결과값을 나누는 것인데, 절댓값이 너무 커져 softmax에서 gradient가 줄어드는 것을 방지하기 위함이다.&lt;/p&gt;

&lt;p&gt;사실 이런 dot product를 통해 attention을 구하는 하나의 과정 자체도 병렬적으로 동시에 여러 번 수행된다. 위의 우측 이미지를 보면 dot product 연산을 병렬적으로 h번 수행해 모두 concatuation을 해 하나의 matrix를 생성한다는 것을 알 수 있다. 해당 matrix를 기존의 input matrix size로 변경하기 위해 다른 weight matrix \(W\)와 dot product를 수행한 뒤 그 결과를 다음 Feed Forward Layer에 넘기게 된다.&lt;/p&gt;

&lt;h3 id=&quot;decoder&quot;&gt;Decoder&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-Attention-is-All-You-Need/The_transformer_encoder_decoder_stack.png&quot; alt=&quot;Attention%20is%20All%20You%20Need%20c507a427409b4c17b5611326901ab369/The_transformer_encoder_decoder_stack.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;출처: &lt;a href=&quot;https://jalammar.github.io/illustrated-transformer/&quot;&gt;https://jalammar.github.io/illustrated-transformer/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Decoder는 Encoder와 매우 비슷한 구조를 갖는다. 동일한 6개의 Decoder Layer를 Stack 구조로 쌓아올린 형태이고, 각각의 Decoder Layer에는 Encoder Layer와 같은 Self Attention Layer와 Feed Forward Layer가 존재한다. Encoder와의 차이점은 Self Attention Layer 이전에 Masked Layer가 있다는 점이다. 6개의 Decoder Layer를 거친 최종 output 값을 softmax를 사용해 확률값으로 구하는데, 이는 i번째 output word에 대한 조건부 확률값이다. 6개의 Decoder Layer는 모두 마지막 Encoder Layer의 output인 context vector를 input으로 받고, 동시에 이전 단계의 Decoder Layer에서 생성된 attention vector도 input으로 받는다. Masked Attention Layer에서는 현재 word의 기준으로 이후 word에 mask를 씌운 vector값이다. 현재 시점의 attention을 생성할 때 이후 word들의 영향을 없애기 위함이다.&lt;/p&gt;
</description>
        <pubDate>Tue, 19 Jan 2021 00:00:00 -0600</pubDate>
        <link>http://0.0.0.0:4000/Attention-is-All-You-Need/</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/Attention-is-All-You-Need/</guid>
        
        
      </item>
    
      <item>
        <title>An Empirical Study Of Tokenization Strategies For Various Korean Nlp Tasks</title>
        <description>&lt;h1 id=&quot;an-empirical-study-of-tokenization-strategies-for-various-korean-nlp-tasks&quot;&gt;An Empirical Study of Tokenization Strategies for Various Korean NLP Tasks&lt;/h1&gt;
&lt;p&gt;title: An Empirical Study of Tokenization Strategies for Various Korean NLP Tasks
subtitle: Korean Tokenizing
categories: Paper Review
tags: NLP Korean
date: 2021-01-19 12:59:23 +0000
last_modified_at: 2021-01-19 12:59:23 +0000
—&lt;/p&gt;

&lt;p&gt;Archive Link: https://arxiv.org/abs/2010.02534
Created: Oct 10, 2020 11:29 PM
Field: NLP
Paper Link: https://arxiv.org/pdf/2010.02534.pdf
Status: completed
Submit Date: Oct 6, 2020&lt;/p&gt;

&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;NLP에서 Tokenization은 전처리 과정에서 가장 중요한 issue 중 하나이다. 가장 적절한 Tokenization 전략을 찾기 위한 연구는 수도 없이 이루어져 왔다. 그 중 가장 대표적인 방식이 BPE이다. BPE는 많은 연구를 통해 보편적으로 가장 효율적인 Tokenization 기법으로 알려졌지만, 아직 language나 task에 구애받지 않고 가장 효율적인가에 대해서는 명확하지 않다. 본 논문에서는 English에 비해 언어 형태론적으로 더 난해한 언어인 Korean에 적합한 tokenization 기법을 찾아내고자 한다. BPE는 가장 보편적인 language인 English를 기준으로 연구된 방식이기에 Korean에 적합하지 않을 수 있다는 생각에서 시작된 연구이다. 본 논문에서는 Korean-English translation, natural language understanding, machine reading comprehension, natural language inference, semantic textual similarity, sentiment analysis, paraphrase identification 등 많은 task에서 실험을 진행했다.&lt;/p&gt;

&lt;h1 id=&quot;background&quot;&gt;Background&lt;/h1&gt;

&lt;h2 id=&quot;mecab-ko-a-korean-morphological-analyzer&quot;&gt;MeCab-ko: A Korean Morphological Analyzer&lt;/h2&gt;

&lt;p&gt;MeCab은 Conditional Random Fields(CRFs)를 기반으로 하는 Japanese 형태소 번역기이다. Japanese와 Korean의 형태론, 문법 상의 유사성에서 착안해 Korean에 적용시킨 것이 MeCab-ko이다. MeCab-ko는 Sejong Corpus를 통해 학습되었으며, 많은 Korean NLP task에서 사용되어 왔고 매우 좋은 성능을 보였다.&lt;/p&gt;

&lt;h2 id=&quot;byte-pair-encoding&quot;&gt;Byte Pair Encoding&lt;/h2&gt;

&lt;p&gt;BPE는 data에서의 등장 빈도를 기반으로 묶는 data-driven statistical alogirhtm이다.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.notion.so/Neural-Machine-Translation-of-Rare-Words-with-Subword-Units-795fb47eb52a42bab59906f90da61e90&quot;&gt;Neural Machine Translation of Rare Words with Subword Units&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;related-work&quot;&gt;Related Work&lt;/h1&gt;

&lt;p&gt;몇몇 연구에서는 단순 BPE보다 해당 language의 구문에 대한 정보를 기반으로 한 segmentation 기법과 BPE를 혼합해 적용하는 것이 더 좋은 성능을 보인다고 주장해왔다. 특히 non-English language, 그 중 형태론적으로 unique한 특성을 갖는 language에 대해서 더욱 두드러진다. Hindi/Bengali, Arabic, Latvian 등에 대해서 BPE와 함께 unique한 segmentation 기법을 혼용한 연구가 진행되었으며, Korean에 있어서도 동일한 연구가 진행되었다. 하지만 Tokenization이 아닌 NMT task에 있어서 사용되는 parallel corpus filtering 전처리에 관한 연구였다는 점에서 본 논문과는 목적이 다르다.&lt;/p&gt;

&lt;h1 id=&quot;tokenization-strategies&quot;&gt;Tokenization Strategies&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-An-Empirical-Study-of-Tokenization-Strategies-for-Various-Korean-NLP-Tasks/10-10-2020-15.06.26.jpg&quot; alt=&quot;An%20Empirical%20Study%20of%20Tokenization%20Strategies%20for%20%202aedcd7cbfe34b1b9c25caf885ff22a5/10-10-2020-15.06.26.jpg&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;consonant-and-vowel-cv&quot;&gt;Consonant and Vowel (CV)&lt;/h2&gt;

&lt;p&gt;자모 단위로 tokenizing을 하는 기법이다. 공백에 대해서 special token \(\star\)를 추가했다.&lt;/p&gt;

&lt;h2 id=&quot;syllable&quot;&gt;Syllable&lt;/h2&gt;

&lt;p&gt;음절 단위로 tokenizing을 하는 기법이다. 역시나 공백에 대한 special token \(\star\)를 사용한다.&lt;/p&gt;

&lt;h2 id=&quot;morpheme&quot;&gt;Morpheme&lt;/h2&gt;

&lt;p&gt;MeCab-ko의 형태소 단위 tokenizer를 사용한다. 하지만 이를 사용하면 original input에서의 공백이 제거가 되고, 따라서 original sentence로의 복원이 불가능해진다. 이를 해결하기 위해 공백 special token \(\star\)를 추가했다.&lt;/p&gt;

&lt;h2 id=&quot;subword&quot;&gt;Subword&lt;/h2&gt;

&lt;p&gt;SentencePiece를 사용한 BPE를 적용했다. original sentence의 단어 단위를 구분하기 위해서 original sentence의 공백에 대응하는 token \(\_\)를 매 단어의 시작에 삽입했다.&lt;/p&gt;

&lt;h2 id=&quot;morpheme-aware-subword&quot;&gt;Morpheme-aware Subword&lt;/h2&gt;

&lt;p&gt;위의 Subword 방식에서 한 발 더 나아가 언어론적 특징을 기반으로 한 segmentation 전략을 BPE와 결합한 방식이다. Morpheme 방식을 먼저 적용한 뒤, 형태소의 list에 대해서 BPE를 적용하게 된다. Morpheme를 적용한 후에 BPE를 사용하기 때문에 형태소 경계를 뛰어넘는 BPE는 발생하지 않는다. (“나랑 쇼핑하자.”에서 ‘쇼핑’, ‘하’는 각각이 별개의 형태소이기 때문에 (‘핑’,’하’)가 Pair로 묶일 수는 없다.)&lt;/p&gt;

&lt;h2 id=&quot;word&quot;&gt;Word&lt;/h2&gt;

&lt;p&gt;original input에서 공백을 기준으로 단어 단위로 tokenizing을 수행하는 가장 단순한 방식이다.&lt;/p&gt;

&lt;h1 id=&quot;experiments&quot;&gt;Experiments&lt;/h1&gt;

&lt;h2 id=&quot;korean-tofrom-english-machine-translation&quot;&gt;Korean to/from English Machine Translation&lt;/h2&gt;

&lt;h3 id=&quot;dataset&quot;&gt;Dataset&lt;/h3&gt;

&lt;p&gt;AI Hub에서 제공하는 Korean-English parallel corpus를 사용했다. 800K sentences pairs의 news data를 포함하고, 784K의 train data, 8K의 dev data, 8K의 test data로 구분했다.&lt;/p&gt;

&lt;h3 id=&quot;bpe-modeling&quot;&gt;BPE Modeling&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-An-Empirical-Study-of-Tokenization-Strategies-for-Various-Korean-NLP-Tasks/10-10-2020-15.20.52.jpg&quot; alt=&quot;An%20Empirical%20Study%20of%20Tokenization%20Strategies%20for%20%202aedcd7cbfe34b1b9c25caf885ff22a5/10-10-2020-15.20.52.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;BPE training에서 AI Hub의 data를 사용할지, Wiki의 data를 사용할지 결정하기 위해 실험을 진행한다. AI Hub의 data는 실제 task에서의 dataset과 동일하기 때문에 corpus set이 동일하다는 장점이 있는 반면, dataset의 크기가 작다. Wiki는 dataset의 크기가 크지만, news에서 사용되는 corpus set과는 차이가 있다는 단점이 있다. Korean-English Translation, English-Korean Translation으로 성능을 비교해보는데, English BPE는 동일하게 Wiki의 English data를 사용한 32K BPE model을 사용했다. 그 결과, AI Hub의 data보다 Wiki의 data가 더 좋은 성능을 보였다. 따라서 본 논문의 이후에서는 Korean BPE training을 위해 Wiki dataset을 사용한다.&lt;/p&gt;

&lt;h3 id=&quot;training&quot;&gt;Training&lt;/h3&gt;

&lt;p&gt;다양한 vocabulary size의 BPE model로 AI Hub news dataset에 대해서 tokenization 기법을 테스트한다. 우선 NMT task에서 SOTA를 달성한 Transformer model을 사용한다. 가장 보편적으로 사용되는 hyperparameter 값을 채택했다. FAIRSEQ를 사용해 실험을 진행했다. 50 epochs마다 checkpoint를 저장했다.&lt;/p&gt;

&lt;h3 id=&quot;results&quot;&gt;Results&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-An-Empirical-Study-of-Tokenization-Strategies-for-Various-Korean-NLP-Tasks/10-10-2020-15.41.07.jpg&quot; alt=&quot;An%20Empirical%20Study%20of%20Tokenization%20Strategies%20for%20%202aedcd7cbfe34b1b9c25caf885ff22a5/10-10-2020-15.41.07.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Ko-En, En-Ko task에서 모두 Subword와 Syllable가 Morpheme이나 Word보다 더 좋은 성능을 보였다. 이는 OOV Rate와 큰 관련이 있다. 한국어의 형태론은 너무 복잡한 규칙을 가져 수많은 형태소가 있기 때문에 64K 이하의 vocabulary size로는 OOV가 많이 발생할 수 밖에 없다. 하지만 Subword나 Syllable은 모두 음절 단위의 model이기 때문에 OOV가 훨씬 더 적게 발생하게 된다.&lt;/p&gt;

&lt;p&gt;한편 CV의 OOV Rate는 당연히 가장 적은 수치를 보여주는데, Syllable나 Subword에 비해서는 더 낮은 성능을 보여준다. 이를 통해 자모 단위는 문맥 정보를 담기에는 너무 작은 단위라는 것을 알 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-An-Empirical-Study-of-Tokenization-Strategies-for-Various-Korean-NLP-Tasks/10-10-2020-15.52.00.jpg&quot; alt=&quot;An%20Empirical%20Study%20of%20Tokenization%20Strategies%20for%20%202aedcd7cbfe34b1b9c25caf885ff22a5/10-10-2020-15.52.00.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Morpheme-aware Subword가 가장 높은 BLEU Scores를 보여준다. Subword와 Morpheme-aware Subword의 차이점은 BPE 이전에 Morpheme의 수행 여부인데, 이는 결국 형태소 경계를 넘어서는 BPE가 발생하는가(Token Spanning Morpheme Boundaries)에서 차이를 보인다. 위의 Table은 Subword에서 각 vocabulary size마다 발생하는 Tokens Spanning Morpheme Boundaries의 횟수를 보여준다. 6~37%의 수치를 보여준다. 이를 통해 형태소 단위의 구분은 tokenization에서 성능에 큰 영향을 미치며, 따라서 형태소 구분을 무시한 단순 BPE는 Korean Tokenizing에 적합하지 않다는 것을 알 수 있다.&lt;/p&gt;

&lt;h2 id=&quot;korean-natural-language-understanding-tasks&quot;&gt;Korean Natural Language Understanding Tasks&lt;/h2&gt;

&lt;p&gt;BERT model을 사용했다. KorQuAD, KorNLI, KorSTS, NSMC, PAWS의 5개 NLU downstream task에 대해서 테스트를 진행했다.&lt;/p&gt;

&lt;h3 id=&quot;downstream-tasks&quot;&gt;Downstream Tasks&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Machine Reading Comprehension: KorQuAD 1.0 Dataset&lt;/p&gt;

    &lt;p&gt;SQuAD를 Korean에 맞게 적용한 dataset이다. 10,645개의 지문과 66,181개의 질문이 포함되고, 각 지문에 대해 주어진 여러 질문 중 가장 적합한 질문을 선택하는 task이다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Natural Language Inference: KorNLI Dataset&lt;/p&gt;

    &lt;p&gt;950,354개의 sentence pair(전제, 추론)이 있고 각 pair에 대해 두 sentence 사이의 관계가 entailment, contradiction, neutral인지 classification을 수행하는 task이다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Semantic Textual Similarity: KorSTS Dataset&lt;/p&gt;

    &lt;p&gt;8628개의 sentence pair가 있고, 각 pair에 대해 0~5 사이의 semantic similarity를 도출해내는 task이다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Sentiment Analysis: NSMC Dataset&lt;/p&gt;

    &lt;p&gt;네이버 영화 review에서 추출한 400K의 sentence에 대해 0(negative)~1(positive) 사이의 sentiment Analysis를 도출해내는 task이다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Paraphrase Identification: PAWS-X Dataset&lt;/p&gt;

    &lt;p&gt;paraphrase identification dataset인 PAWS-X에서 Korean dataset만 추출해낸 53,338 sentence pairs에 대해 0(negative)~1(positive)의 paraphrase identification을 도출해내는 task이다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;training-1&quot;&gt;Training&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-An-Empirical-Study-of-Tokenization-Strategies-for-Various-Korean-NLP-Tasks/10-10-2020-16.17.38.jpg&quot; alt=&quot;An%20Empirical%20Study%20of%20Tokenization%20Strategies%20for%20%202aedcd7cbfe34b1b9c25caf885ff22a5/10-10-2020-16.17.38.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;large corpus로 pre-train된 BERT-Base model을 각 5개의 NLU task에 대해 별개로 fine-tuning시켜 실험을 진행했다. Korean Wiki Corpus(640MB)는 pre-train을 진행할 만큼 충분한 크기가 되지 못해 Namu-wiki에서 5.5GB의 corpus를 추출해내 Wiki Corpus와 함께 사용했다. hyperparameter는 batch size=1024, max sequence length=128, optimizer=AdamW, lr=5e-5, warm up steps=10K를 사용했다. pre-trained된 BERT Model을 Tensorflow에서 Pytorch로 convert한 뒤, HuggingFace Transformers를 사용해 fine-tuning을 진행했다. fine-tuning에서의 hyperparameter는 위 Table의 값을 사용했다.&lt;/p&gt;

&lt;h3 id=&quot;results-1&quot;&gt;Results&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-An-Empirical-Study-of-Tokenization-Strategies-for-Various-Korean-NLP-Tasks/10-10-2020-16.38.13.jpg&quot; alt=&quot;An%20Empirical%20Study%20of%20Tokenization%20Strategies%20for%20%202aedcd7cbfe34b1b9c25caf885ff22a5/10-10-2020-16.38.13.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 5개의 NLU task에 대해 6개의 tokenizing 기법을 사용해 각각 dev set, test set에서의 성능을 측정했다. 예외적으로 KorQuAD의 경우에는 test set이 부족해 dev set만 사용했다.&lt;/p&gt;

&lt;p&gt;KorQuAD task에서는 Subword 64K model이 가장 좋은 성능을 보였다. Morpheme와 Subword에서는 vocabulary size와 Score가 비례 관계이다. 하지만 Morpheme-aware Subword에서는 32K model이 제일 높은 수치를 달성했다. 결론적으로, Morpheme-aware Subword model에서는 성능과 vocabulary size 사이의 유의미한 상관관계를 찾을 수 없었다.&lt;/p&gt;

&lt;p&gt;나머지 다른 4개의 task에 대해서는 모두 Morpheme-aware Subword의 64K model이 가장 좋은 성능을 달성했다. tokenization 방식에 관계 없이 모두 다 vocabulary size와 score가 대체로 비례 관계를 보였다. 그러나 위에서 진행했던 NMT task에 있어서는 Morpheme-aware Subword에서의 높은 vocabulary size가 좋은 성능을 보장하지는 않았는데, 다소 배치되는 결과이다.&lt;/p&gt;

&lt;h1 id=&quot;discussion&quot;&gt;Discussion&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-An-Empirical-Study-of-Tokenization-Strategies-for-Various-Korean-NLP-Tasks/10-10-2020-16.57.50.jpg&quot; alt=&quot;An%20Empirical%20Study%20of%20Tokenization%20Strategies%20for%20%202aedcd7cbfe34b1b9c25caf885ff22a5/10-10-2020-16.57.50.jpg&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;token-length&quot;&gt;Token Length&lt;/h2&gt;

&lt;p&gt;token의 길이가 성능에 얼마나 영향을 미치는지 알아본다. token length는 한 token에 포함된 음절 개수의 평균으로 정의한다. CV의 경우에는 자모 단위이기 때문에 평균 token length는 0.33~0.5 사이의 값이다. 한 음절은 2~3개의 자모로 구성되어 있기 때문이다. Syllable은 음절 단위로 tokenizing을 한 것이기 때문에 평균 token length는 1의 고정 값을 갖는다. Morpheme는 형태소 단위로 tokenizing을 수행한 것이기 때문에 평균 token length가 vocabulary size에 따라 변하지 않고 일정하다. Subword나 Morpheme-aware-Subword는 모두 BPE를 사용하는 방식이기 때문에 vocabulary size가 증가할수록 token length도 증가하게 된다. 통계적인 빈도를 기반으로 vocabulary size에 따라 상위 N개를 pair로 묶기 때문이다. 위의 figure에는 word model이 누락됐는데, word model은 Ko-En과 En-Ko에서 각각 7.07, 18.42로 매우 낮은 Score를 보여줘 공간상의 제약으로 figure에서 제외했다.&lt;/p&gt;

&lt;p&gt;Figure 1을 분석해보자. 자모 단위로 tokenizing을 수행한 CV의 성능이 기준점이다. 대부분의 model은 평균 token length가 1.0~1.5인 구간에서 가장 좋은 성능을 보여준다. 평균 token length가 1.5를 넘어가기 시작하면서 점차 감소하는 경향을 보인다. 특히 평균 token length가 2.5에 달하는 word model의 경우에는 최악의 성능을 보여줬다.&lt;/p&gt;

&lt;h2 id=&quot;linguistic-awareness&quot;&gt;Linguistic Awareness&lt;/h2&gt;

&lt;p&gt;Figure 1에서 8K Subword model과 16K Morpheme-aware Subword model을 비교해보자. figure에서 파란 색 배경으로 강조 표시가 된 부분이다. 두 model은 평균 token length가 동일한 값이다. 두 model의 차이는 언어론적 지식을 사용했는가(형태소 경계를 넘어서는 pair를 생성했는가)에 있다. Ko-En과 En-Ko 두 task에서 모두 Morpheme-aware Subword model이 더 좋은 성능을 보여줬다는 것은 token length뿐만 아니라 linguistic awareness도 tokenization 전략 수립에 매우 중요한 factor라는 것을 보여준다.&lt;/p&gt;

&lt;h2 id=&quot;under-trained-tokens&quot;&gt;Under-trained Tokens&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-19-An-Empirical-Study-of-Tokenization-Strategies-for-Various-Korean-NLP-Tasks/10-10-2020-16.58.00.jpg&quot; alt=&quot;An%20Empirical%20Study%20of%20Tokenization%20Strategies%20for%20%202aedcd7cbfe34b1b9c25caf885ff22a5/10-10-2020-16.58.00.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 1에서 Morpheme model의 경우에만 예외적으로 CV보다 훨씬 못한 성능을 보여준다. 이러한 결과는 Morpheme model의 높은 OOV rate에서 비롯된다. 위의 Experiments에서 살펴본 NMT task에서의 result table을 확인해보면 Morpheme model의 OOV rate가 압도적으로 높다는 것을 확인할 수 있다(본 논의에서는 모든 task에서 최악의 성능을 보여줬던  Word model은 배제한다). OOV는 정의하자면 test set에서만 등장하고, train set에서는 등장하지 않았던 token을 의미한다. 즉, OOV rate가 높다는 것은 model 입장에서는 처음 보는 token이 test set에서 등장하는 비율을 의미한다. 완전히 처음 마주하는 token이 아닌 적게 마주한 token들의 비율에 대해서도 확인을 해보자. OOV가 아니라 하더라도 등장 빈도가 확연히 적은 token들에 대해서는 model이 under-train했을 가능성이 농후하기 때문이다. Figure 2에서는 실제로 등장 빈도가 낮은 token의 비중이 얼마나 되는지를 시각화 한 graph이다.  예상했던 바와 같이 OOV rate가 높은 Morpheme model이 훨씬 더 높은 수치를 보여준다는 것을 확인할 수 있다. 이는 결국 Morpheme model이 under-trained된 token의 비중이 높다는 것을 의미한다. 이러한 이유로 Morpheme model이 타 model 대비 확연히 낮은 성능을 보이는 것이다.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;여러 Korean NLP task에 대해서 다양한 tokenization 전략을 사용한 model들의 성능을 비교했다. Korean-English NMT task에서는 BPE에 언어론적 특성(형태소)를 더한 Morpheme-aware Subword Model이 가장 높은 성능을 보여줬다. NLU task의 KorQuAD를 제외한 모든 task에서 역시 Morpheme-aware Subword Model이 가장 좋은 수치를 달성했다. 이를 통해 각 language의 unique한 linguistic awareness가 model 성능 향상에 매우 큰 영향을 미친다는 사실을 도출해냈다.&lt;/p&gt;
</description>
        <pubDate>Tue, 19 Jan 2021 00:00:00 -0600</pubDate>
        <link>http://0.0.0.0:4000/An-Empirical-Study-of-Tokenization-Strategies-for-Various-Korean-NLP-Tasks/</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/An-Empirical-Study-of-Tokenization-Strategies-for-Various-Korean-NLP-Tasks/</guid>
        
        
      </item>
    
  </channel>
</rss>

<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Hansu Kim's Dev Blog</title>
    <description>Hansu Kim's Dev Blog</description>
    <link>http://0.0.0.0:4000/</link>
    <atom:link href="http://0.0.0.0:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Wed, 20 Jan 2021 01:02:10 -0600</pubDate>
    <lastBuildDate>Wed, 20 Jan 2021 01:02:10 -0600</lastBuildDate>
    <generator>Jekyll v4.2.0</generator>
    
      <item>
        <title>[NLP 논문 리뷰] Deep Contextualized Word Representations (ELMo)</title>
        <description>&lt;h2 id=&quot;paper-info&quot;&gt;Paper Info&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1802.05365&quot;&gt;Archive Link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1802.05365.pdf&quot;&gt;Paper Link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Submit Date: Feb 15, 2018&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;word2vec이나 glove와 같은 기존의 word embedding 방식은 다의어의 모든 의미를 담아내기 곤란하다는 심각한 한계점을 갖고 있다. ELMo(Embeddings from Language Models)는 이러한 한계점을 극복하기 위해 embedding에 sentence의 전체 context를 담도록 했다. pre-train된 LSTM layer에 sentence 전체를 넣어 각 word의 embedding을 구해내는 model이다. 이로 인해 기존의 embedding에 비해 복잡한 syntax, semantic한 특징들을 담아낼 수 있었고, 다의어 문제도 해결했다. LSTM은 forward, backward 2가지 방향을 사용하는데 각 LSTM은 다음 word를 예측하는 model이다. 이러한 LSTM layer를 다층으로 구성해 context-dependent한 word embedding을 생성해내게 된다. ELMo를 사용한 word embedding은 수많은 NLP task에서 성능 향상을 이끌어낼 수 있다.&lt;/p&gt;

&lt;h1 id=&quot;elmo-embeddings-from-language-models&quot;&gt;ELMo: Embeddings from Language Models&lt;/h1&gt;

&lt;p&gt;ELMo와 여타 word embedding model과의 가장 큰 차이점은 각 단어마다 고정된 word embedding을 사용하는 것이 아닌 pre-trained model 자체를 하나의 function 개념으로 사용한다는 것이다. 이전의 word2vec 등은 model을 train시킨 뒤 각 word마다의 embedding vector만을 추출해 사용했지만, ELMo는 word마다 embedding vector가 특정되지 않기에 이와 같은 방식이 불가능하고, ELMo model을 NLP model의 앞에 연결하는 방식으로 사용하게 된다.&lt;/p&gt;

&lt;h2 id=&quot;bidirectional-language-models&quot;&gt;Bidirectional language models&lt;/h2&gt;

&lt;p&gt;input sequence가 \(N\)개의 token들 (\(t_1, t_2, ..., t_N\))이라고 가정하자. \(t_k\)는 모두 각 word에 대한 context-independent token이다.&lt;/p&gt;

&lt;p&gt;forward LSTM은 \(t_1, ... , t_{k-1}\)이 주어졌을 때 \(t_k\)를 예측하는 model이다.&lt;/p&gt;

&lt;p&gt;\(j\)번째 LSTM layer에서 \(k\)번째 token에 대한 forward LSTM output은 \(\overrightarrow{h}_{k,j}\)이다. 마지막 LSTM layer의 output인 \(\overrightarrow{h}_{k,L}\)을 softmax에 넣어 최종적으로 \(t_{k+1}\)을 예측하게 된다.&lt;/p&gt;

\[p(t_1, t_2, ... , t_N) = \prod^N_{k=1}{p(t_k \vert t_1, t_2, ..., t_{k-1})}\]

&lt;p&gt;backward LSTM은 \(t_{k+1}, t_{k+2}, ... ,t_N\)이 주어졌을 때 \(t_k\)를 예측하는 model이다.&lt;/p&gt;

&lt;p&gt;\(j\)번째 LSTM layer에서 \(k\)번째 token에 대한 backward LSTM output은 \(\overleftarrow{h}_{k,j}\)이다. 마지막 LSTM layer의 output인 \(\overleftarrow{h}_{k,L}\)을 softmax에 넣어 최종적으로 \(t_{k-1}\)을 예측하게 된다.&lt;/p&gt;

&lt;p&gt;\(\)p(t_1, t_2, … , t_N) = \prod^N_{k=1}{p(t_k \vert t_{k+1}, t_{k+2}, …, t_{N})}\(\)&lt;/p&gt;

&lt;p&gt;biLM은 위의 두 LSTM을 결합한 것이다. 두 방향에 대한 log likelihood를 최대화하는 것을 목표로 한다.&lt;/p&gt;

\[\sum^N_{k=1} {(\log p(t_k \vert t_1, ... , t_{k-1} ; \Theta_x, \overrightarrow{\Theta}_{\text{LSTM}}, \Theta_s) + \log p(t_k \vert t_{k+1}, ... , t_{N} ; \Theta_x, \overleftarrow{\Theta}_{\text{LSTM}}, \Theta_s))}\]

&lt;p&gt;\(\Theta_x\)는 token representation(\(t_1, ... , t_N\))에 대한 parameter이고, \(\Theta_s\)는 softmax layer에 대한 parameter이다. 이 두 parameter는 전체 direction에 관계 없이 같은 값을 공유하지만, LSTM의 parameter들은 두 LSTM model이 서로 다른 값을 갖는다.&lt;/p&gt;

&lt;h2 id=&quot;elmo&quot;&gt;ELMo&lt;/h2&gt;

&lt;p&gt;ELMo에서는 새로운 representation을 사용하는데, 이를 얻기 위해서는 LSTM layer의 개수를 \(L\)이라고 했을 때 총 \(2L+1\)개의 representation을 concatenate해야 한다. input representation layer 1개와 forward, backward LSTM 각각 \(L\)개이다.&lt;/p&gt;

&lt;p&gt;\(\)R_k = {x_k, \overrightarrow{h}&lt;em&gt;{k,j}, \overleftarrow{h}&lt;/em&gt;{k,j} \vert j=1, … , L}\(\)&lt;/p&gt;

&lt;p&gt;input representation layer를 \(j=0\)으로, \(\overrightarrow{h}_{k,j}\)와 \(\overleftarrow{h}_{k,j}\)의 concatenation을 \(h_{k,j}\)로 표현한다면 다음과 같은 일반화된 수식으로 ELMO representation을 표현할 수 있다.&lt;/p&gt;

\[R_k = \{h_{k,j} \vert j=0, ..., L\}\]

&lt;p&gt;결국 \(R_k\)는 \(k\)번째 token에 대한 모든 representation이 연결되어 있는 것인데, 이를 사용해 최종 ELMo embedding vector를 만들어내게 된다.&lt;/p&gt;

\[\text{ELMo}^{task}_k=E(R_k;\Theta^{task}) = \gamma^{task} \sum^L_{j=0} {s_j^{task}h_{k,j}}\]

&lt;p&gt;각 LSTM layer의 output인 \(h_{k,j}\)를 모두 더하는데 이 때 각각에 softmax-normalized weights \(s_j\)를 곱한 뒤 더하게 된다. 당연하게도 \(\sum^L_{j=0}s_j^{task}=1\)이다. 마지막에 최종적으로 \(\gamma\)를 곱하게 되는데, scale parameter이다. \(s_j\)와 \(\gamma\)는 모두 learnable parameter이면서 optimization에서 매우 중요한 역할을 담당한다.&lt;/p&gt;

&lt;h2 id=&quot;using-bilms-for-supervised-nlp-tasks&quot;&gt;Using biLMs for supervised NLP tasks&lt;/h2&gt;

&lt;p&gt;supervised downstream task에 ELMo를 적용하는 구체적인 방법은 간단하다. 대부분의 supervised NLP model들의 input은 모두 context-independent token들의 sequence이다. 이러한 공통점 덕분에 동일한 방법으로 대부분의 task에 ELMo를 적용할 수 있다. 우선 NLP model의 input layer와 다음 layer 사이에 ELMo를 삽입한다. 이후 ELMo 이후 layer에서는 input을 \([x_k;\text{ELMo}_k^{task}]\)로 사용한다. 즉, input token과 ELMo embedding vector의 concatenation을 사용하는 것이다. NLP model을 train할 때에는 ELMo의 weight들은 모두 freeze시킨다. 즉, ELMo model은 NLP model이 train될 때 함께 train되지 않는다.&lt;/p&gt;

&lt;p&gt;SNLI, SQuAD와 같은 특정 task에서는 RNN의 output \(h_k\)를 \([h_k;\text{ELMo}_k^{task}]\)로 교체했을 때 더 좋은 성능을 보이기도 했다. 또한 일반적으로 dropout과 \(\lambda \Vert w \Vert^2_2\)를 loss에 더하는 regularization을 사용했을 때 더 좋은 성능을 보였다.&lt;/p&gt;

&lt;h2 id=&quot;pre-trained-bidirectional-language-model-architecture&quot;&gt;Pre-trained bidirectional language model architecture&lt;/h2&gt;

&lt;p&gt;ELMo는 기존의 pre-trained biLM와 큰 구조는 비슷하지만 몇가지 차이점이 존재하는데, 가장 큰 차이점은 LSTM layer 사이에 residual connection을 사용했다는 점이다. 이를 통해 input의 feature를 더 잘 전달하고 gradient vanishing을 해결할 수 있다. \(L=2\)개의 biLSTM layer를 사용했고, 4096개의 unit과 512차원의 projection을 사용했다. biLSTM의 input으로는 2048 character n-gram을 CNN에 넣는 char-CNN embedding을 사용했다.&lt;/p&gt;

&lt;h1 id=&quot;evaluation&quot;&gt;Evaluation&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-02-Deep-contextualized-word-representations/01.jpg&quot; alt=&quot;01.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ELMo를 단순하게 추가하는 것만으로도 baseline model에 비해 성능이 향상됐고, 이를 통해 SOTA를 달성할 수 있었다.&lt;/p&gt;

&lt;h1 id=&quot;analysis&quot;&gt;Analysis&lt;/h1&gt;

&lt;h2 id=&quot;alternate-layer-weighting-schemes&quot;&gt;Alternate layer weighting schemes&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-02-Deep-contextualized-word-representations/02.jpg&quot; alt=&quot;02.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ELMo representation을 사용하지 않고 단순하게 LSTM의 마지막 layer의 output (\(h_{k,L}\))을 사용하는 방법도 있다. 이러한 방식은 biLM, CoVe 등 기존의 많은 연구에서 시도되었는데 이와 ELMo representation을 사용한 경우를 비교해본다. Table 2의 Last Only는 마지막 LSTM layer의 output만을 word embedding으로 사용하는 경우이다.&lt;/p&gt;

&lt;p&gt;\(\lambda\)는 softmax-normalized weights \(s_j\)에 대한 regularization parameter인데, 0~1 사이의 값을 갖는다. \(\lambda\)가 1에 가까울수록 각 layer에서의 output들을 평균에 가깝게 계산해 최종 vector를 생성해내고 (\(s_j\)가 모두 유사한 값), \(\lambda\)가 0에 가까울수록 각 layer에서의 output들에 다양한 값들이 곱해서 더해진다.&lt;/p&gt;

&lt;p&gt;Table 2에서는 task에 관계 없이 동일한 경향성을 보이는데, baseline model보다 CoVe와 같은 마지막 LSTM layer의 output을 word embedding으로 사용한 model이 더 좋은 성능을 보였다. 또한 CoVe보다 ELMo가 더 좋은 성능을 보였는데, 이 중 \(\lambda\)가 낮은 경우가 더 좋은 결과를 보였다.&lt;/p&gt;

&lt;h2 id=&quot;where-to-include-elmo&quot;&gt;Where to include ELMo?&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-02-Deep-contextualized-word-representations/03.jpg&quot; alt=&quot;03.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위에서 언급했듯 supervised NLP model에 ELMo를 적용할 때에는 input layer의 직후에 ELMo를 삽입했다. SQuAD, SNLI, SRL의 baseline model은 모두 biRNN model인데, ELMo를 biRNN 직후에도 삽입을 한 뒤 성능을 비교했다. SQuAD와 SNLI에 있어서는 ELMo를 biRNN 이후에도 추가하는 것이 더 좋은 성능을 보여줬는데, 이는 SNLI와 SQuAD는 biRNN 직후 attention layer가 있는데, biRNN과 attention layer 사이에 ELMo를 추가함으로써 ELMo representation에 attention이 직접적으로 반영됐기 때문이라고 유추해 볼 수 있다.&lt;/p&gt;

&lt;h2 id=&quot;what-information-is-captured-by-the-bilms-representations&quot;&gt;What information is captured by the biLM’s representations?&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-02-Deep-contextualized-word-representations/04.jpg&quot; alt=&quot;04.jpg&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;word-sense-disambiguation&quot;&gt;Word sense disambiguation&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-02-Deep-contextualized-word-representations/05.jpg&quot; alt=&quot;05.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;WSD는 다의어의 의미를 구분짓는 task로 embedding이 얼마나 semantic 정보를 잘 담고 있는지에 대한 지표이다. ELMo는 WSD-specific한 model과 동등한 수치를, CoVe보다는 월등히 높은 수치를 달성했다. 주목할만한 점은 ELMo의 first LSTM layer의 output보다는 second layer (top layer)의 output이 WSD에서 좋은 성능을 보였다는 점이다.&lt;/p&gt;

&lt;h3 id=&quot;pos-tagging&quot;&gt;POS tagging&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-02-Deep-contextualized-word-representations/06.jpg&quot; alt=&quot;06.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;POS tagging은 word의 품사를 tagging하는 task로 embedding이 얼마나 syntax 정보를 잘 담고 있는지에 대한 지표이다. 여기서도 ELMo는 POS tagging-specific model과 동둥한 수준의 성능을, CoVe보다는 월등히 높은 성능을 보여줬다. WSD와는 다르게 오히려 first LSTM layer의 output이 top layer의 output보다 POS tagging에서 더 좋은 성능을 보였다는 점이 주목할 만하다.&lt;/p&gt;

&lt;h3 id=&quot;implications-for-supervised-tasks&quot;&gt;Implications for supervised tasks&lt;/h3&gt;

&lt;p&gt;결론적으로 ELMo에서 각 layer는 담고 있는 정보의 종류가 다르다고 할 수 있는데, 층이 낮은 layer(input layer에 가까운 layer)일수록 syntax 정보를, 층이 높은 layer(output layer에 가까운 layer)일수록 semantic 정보를  저장한다.&lt;/p&gt;

&lt;h2 id=&quot;sample-efficiency&quot;&gt;Sample efficiency&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-02-Deep-contextualized-word-representations/07.jpg&quot; alt=&quot;07.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ELMo의 사용은 일정 수준 이상의 성능 달성에 필요한 parameter update 횟수 및 전체 training set size를 획기적으로 줄여준다. SRL task에 있어서 ELMo 사용 이전 baseline model의 경우에는 486 epoch가 지나서야 score가 수렴했는데, ELMo를 추가하고 난 뒤에는 10 epoch만에 baseline model의 score를 능가했다.&lt;/p&gt;

&lt;p&gt;Figure 1에서는 같은 크기의 dataset에서 ELMo를 사용하는 경우가 훨씬 더 좋은 성능을 낸다는 것을 보여준다. 심지어 SRL task에서는 ELMo를 사용한 model이 training dataset의 단 1%을 학습했을 때 달성한 수치와 baseline model이 training dataset의 10%를 학습했을 때의 수치가 동일하다는 것을 확인할 수 있다.&lt;/p&gt;

&lt;h2 id=&quot;visualization-of-learned-weights&quot;&gt;Visualization of learned weights&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2021-01-02-Deep-contextualized-word-representations/08.jpg&quot; alt=&quot;08.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;softmax-normalized parameter \(s_j\)를 시각화한 것이다. ELMo를 biRNN의 input과 output에 사용했을 때를 각각 나눠 비교했다. ELMo가 input에 사용된 경우에는 대개 first LSTM layer가 선호되는 경향을 보였다. 특히나 SQuAD에서 이러한 경향성이 가장 두드러지게 나타났다. 반면 ELMO가 output에 사용된 경우에는 weight가 균형있게 분배되었지만 낮은 layer가 조금 더 높은 선호를 보였다.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;biLM을 사용해 높은 수준의 context를 학습하는 ELMo model을 제안했다. ELMo model을 사용하면 대부분의 NLP task에서 성능이 향상되었다. 또한 layer의 층이 올라갈수록 syntax보다는 semantic한 정보를 담아낸다는 사실도 발견해냈다. 때문에 어느 한 layer를 사용하는 것보다는 모든 layer의 representation을 결합해 사용하는 것이 전반적인 성능 향상에 도움이 된다는 결론을 내릴 수 있다.&lt;/p&gt;
</description>
        <pubDate>Fri, 01 Jan 2021 18:00:00 -0600</pubDate>
        <link>http://0.0.0.0:4000/machine%20learning/paper%20review/Deep-contextualized-word-representations/</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/machine%20learning/paper%20review/Deep-contextualized-word-representations/</guid>
        
        <category>NLP</category>
        
        
        <category>Machine Learning</category>
        
        <category>Paper Review</category>
        
      </item>
    
      <item>
        <title>[운영체제] Disk</title>
        <description>&lt;p&gt;숭실대학교 컴퓨터학부 홍지만 교수님의 2020-2학기 운영체제 강의를 정리 및 재구성했다.&lt;/p&gt;

&lt;h1 id=&quot;hard-disk&quot;&gt;Hard Disk&lt;/h1&gt;

&lt;p&gt;hard disk는 가장 범용적으로 사용되는 저장 장치이다. main memory와 다르게 영속적(persistent)으로 data를 저장할 수 있다. hard disk는 물리적으로 회전(rotation)하면서 data를 저장할 장소를 찾는다. 전체 구성 요소는 다음과 같다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-12-18-Disk/01.png&quot; alt=&quot;01.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;hard disk는 여러 층으로 이루어져 있다. 각 층은 platter라는 하나의 원판으로 구성되고, 모든 원판의 중심을 통과하는 spindle이 있다. spindle은 실제로 rotation을 수행하는 장치이다. 각 platter는 여러 track으로 구성되는데, 원하는 track을 선택하기 위해 disk arm이 이동하게 된다. disk arm의 끝에 달려있는 disk head가 실제로 data를 읽게 된다. 각 track은 sector로 구분되는데, sector는 가상 memory에서의 frame 또는 block과 같이 data가 한번에 읽고 씌여지는 단위이다. disk arm은 원하는 track으로 이동할 때 사용하고, 원하는 sector로 이동하기 위해서는 spindle이 flatter를 rotation시켜야 한다. 따라서 disk I/O의 단계는 다음과 같이 정리된다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;disk arm을 해당 track으로 이동 (seek time)&lt;/li&gt;
  &lt;li&gt;해당 sector로 roatation (rotational delay)&lt;/li&gt;
  &lt;li&gt;실제로 data를 read (transfer time)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;disk I/O에 소요되는 시간은 seek time + rotational delay + transfer time이 된다. 이 중 rotational delay, transfer time은 HW의 성능에 의해 좌우되기 때문에 실제로 disk scheduling을 통해 향상시키고자 하는 것은 seek time이다.&lt;/p&gt;

&lt;h1 id=&quot;disk-scheduling&quot;&gt;Disk Scheduling&lt;/h1&gt;

&lt;p&gt;disk의 seek time을 줄일 수 있는 disk scheduling algorithm들에 대해 알아보자. queue에 읽어야 할 track number가 다음과 같은 순서로 들어온다고 가정한다. 또한 disk arm의 시작 위치는 15라고 가정한다.&lt;/p&gt;

\[15, 8, 17, 11, 3, 23, 19, 14, 20\]

&lt;h2 id=&quot;fcfsfirst-come-first-service&quot;&gt;FCFS(First Come First Service)&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-12-18-Disk/02.png&quot; alt=&quot;02.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;queue에 들어온 순서대로 track으로 이동하는 것이다. 즉, FIFO와 동일한 방식이다. 모든 track에 대해 공정하게 접근하지만, 비효율적인 이동이 다수 발생할 수 있다는 단점이 있다.&lt;/p&gt;

&lt;h2 id=&quot;sstf-shortest-seek-time-first&quot;&gt;SSTF (Shortest Seek Time First)&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-12-18-Disk/03.png&quot; alt=&quot;03.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;현재 disk arm의 위치에서 가장 적게 움직여 도달할 수 있는 track으로 먼저 이동하는 방식이다. FIFO에 비해 효율적이지만, 양극단(0, 24)에 가까운 track에 대해서는 starvation이 발생할 수 있다. 또한 하나의 track만이 계속 사용될 경우 arm이 움직이지 않고 고정되는 arm stickiness 현상이 발생할 수도 있다.&lt;/p&gt;

&lt;h2 id=&quot;scan&quot;&gt;SCAN&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-12-18-Disk/04.png&quot; alt=&quot;04.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;한 방향으로의 관성을 갖고 움직여 양극단에 도달했을 경우에 반대 방향으로 전환해 이동하는 방식이다. 위의 경우는 track number가 감소하는 방향으로 시작한다고 가정한 것이다. 5시점까지 수행해 track 3으로 이동한 후, 최저점(0)에 도달한 뒤에 방향을 전환하게 된다. 마지막 9시점에서 23까지 이동하고 난 후 최고점(24)에 도달하게 된다. 이후 다시 방향을 전환하게 될 것이다. SSTF와 동일하게 starvation, arm stickiness가 발생할 수 있다.&lt;/p&gt;

&lt;h2 id=&quot;c-scan&quot;&gt;C-SCAN&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-12-18-Disk/05.png&quot; alt=&quot;05.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;SCAN과 유사하지만 한 방향으로만 track을 읽어들이는 정책이다. 1~5시점까지는 track number를 감소시키면서 track을 읽어들이고, 5시점 이후에 최저점(0)에 도달한 뒤 최고점(24)로 이동해 다시 track number를 감소시키며 track을 읽어들인다. 이 역시 starvation, arm stickiness가 발생할 수 있다.&lt;/p&gt;

&lt;h2 id=&quot;c-look&quot;&gt;C-LOOK&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-12-18-Disk/06.png&quot; alt=&quot;06.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;C-SCAN과 유사하지만 양극단(0, 24)까지 이동하지 않고 queue에서 가장 작은 track number(3), 가장 큰 track number(23)까지만 이동한다. 여전히 starvation, arm stickiness가 발생할 수 있다.&lt;/p&gt;

&lt;h2 id=&quot;n-step-scan--fscan&quot;&gt;N-step-SCAN / FSCAN&lt;/h2&gt;

&lt;p&gt;queue를 길이가 N인 하위 queue로 분할한 뒤, 각 queue에 대해 개별적으로 SCAN 정책을 사용하는 방식이다. 하나의 queue가 처리되는 동안 들어오는 신규 요청들은 모두 다른 queue에 추가된다. 따라서 starvation, arm stickiness가 발생하지 않는다. N의 값에 따라 성능이 달라지는데, N의 값이 커질 경우 SCAN과 유사한 성능을 보이고, N=1일 경우에는 FCFS와 완전히 동일한 정책이다. 이 중 FSCAN은 N=2인 경우를 지칭한다.&lt;/p&gt;

&lt;h1 id=&quot;raid&quot;&gt;RAID&lt;/h1&gt;

&lt;p&gt;disk의 성능 지표로는 총 3가지가 있다. 성능(performance), 용량(capacity), 신뢰성(reliability)이다. performance는 얼마나 I/O time이 짧은가에 대한 지표이고, capacity는 얼마나 많은 data를 저장할 수 있는가에 대한 지표,reliability는 data가 얼마나 안전하게 저장되는가에 대한 지표이다. RAID는 여러 disk가 있을 때 사용하는 disk 가상화 기술로, 위의 지표 중 capacity를 다소 희생해 reliability를 향상시키는 정책이다.&lt;/p&gt;

&lt;h2 id=&quot;raid-0&quot;&gt;RAID 0&lt;/h2&gt;

&lt;p&gt;RAID 0은 capacity를 희생시키지 않고 performance를 향상시키는 정책이다. 큰 size의 data를 disk에 저장해야 할 때 여러 disk block을 사용해야 할 것이다. 이 때 대개 하나의 disk에 존재하는 block을 사용하게 되는데, RAID 0은 여러 disk의 block을 균등하게 사용하는 것이다. 이를 striping이라고 한다. 이로 인해 얻을 수 있는 이점은 performance인데, 하나의 disk에서 disk arm을 여러 번 이동시켜 여러 track을 읽어들일 필요 없이, 병렬적으로 (동시에) 각 disk에서 disk arm을 이동시켜 전체 seek time을 줄일 수 있다.&lt;/p&gt;

&lt;p&gt;RAID 0은 capacity를 희생시키지 않기 때문에 N개의 disk가 있을 경우 N개의 disk 전부를 온전히 data 저장 용도로 사용할 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-12-18-Disk/07.png&quot; alt=&quot;07.png&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;raid-1&quot;&gt;RAID 1&lt;/h2&gt;

&lt;p&gt;RAID 1은 mirroring 기법을 사용한다. mirroring이란 같은 disk를 그대로 또다른 disk에 복사해 저장하는 것이다. 이를 통해 둘 중 하나의 disk가 crash된다고 하더라도 다른 mirroring disk의 data를 통해 복구할 수 있다.&lt;/p&gt;

&lt;p&gt;RAID 1은 N개의 disk가 있을 경우 N/2개의 disk의 용량만큼 data를 저장할 수 있다. 대신 fault tolerance가 증가하게 된다. 즉, capacity를 희생해 reliability를 증가시킨 것이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-12-18-Disk/08.png&quot; alt=&quot;08.png&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;raid-10--raid-01&quot;&gt;RAID 10 / RAID 01&lt;/h2&gt;

&lt;p&gt;RAID 1에 대해 RAID 0을 수행하는 정책이다. capacity는 N/2가 된다. reliability와 performance가 모두 향상된다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-12-18-Disk/09.png&quot; alt=&quot;09.png&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;raid-01&quot;&gt;RAID 01&lt;/h2&gt;

&lt;p&gt;RAID 0에 대해 RAID 1을 수행하는 정책이다. capacity는 N/2가 된다. reliability와 performance가 모두 향상된다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-12-18-Disk/10.png&quot; alt=&quot;10.png&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;raid-234&quot;&gt;RAID 2/3/4&lt;/h2&gt;

&lt;p&gt;RAID 2/3/4는 parity disk를 사용하는 방식이다. 이중 RAID 2는 다수의 parity disk를 사용하는 것이고, RAID 3,4는 하나의 parity disk를 사용한다. parity disk란 data 복구를 위해 존재하는 disk로 다른 disk들의 data를 검사할 수 있는 값을 저장하게 된다. 이 때 사용하는 단위는 RAID 3은 bit 단위, RAID 4는 block 단위이다. RAID 3/4는 최대 1개의 disk에 대한 crash를 복원할 수 있는데, 만약 data 저장 disk가 crash됐을 경우 parity disk의 값을 사용해 복원할 수 있고, 만약 parity disk가 crash됐을 경우 다른 disk들의 값을 사용해 parity data를 다시 생성한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-12-18-Disk/11.png&quot; alt=&quot;11.png&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;raid-5&quot;&gt;RAID 5&lt;/h2&gt;

&lt;p&gt;RAID 4에서 parity disk를 별도로 운용하지 않고 여러 disk에 나눠 저장하는 것이다. 이는 parity disk의 I/O 횟수가 일반 data 저장 disk에 비해 많아 parity disk의 수명이 짧아지는 단점을 보완한 것이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-12-18-Disk/12.png&quot; alt=&quot;12.png&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;raid-6&quot;&gt;RAID 6&lt;/h2&gt;

&lt;p&gt;서로 다른 parity 연산을 수행해 2개의 disk를 추가로 사용하는 방식이다. 최대 2개의 disk의 crash까지 복구 가능하다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-12-18-Disk/13.png&quot; alt=&quot;13.png&quot; /&gt;&lt;/p&gt;
</description>
        <pubDate>Thu, 17 Dec 2020 18:00:00 -0600</pubDate>
        <link>http://0.0.0.0:4000/operating%20system/Disk/</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/operating%20system/Disk/</guid>
        
        <category>Operating System</category>
        
        
        <category>Operating System</category>
        
      </item>
    
      <item>
        <title>[NLP 논문 리뷰] Efficient Estimation Of Word Representations In Vector Space (Word2Vec)</title>
        <description>&lt;h2 id=&quot;paper-info&quot;&gt;Paper Info&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1301.3781&quot;&gt;Archive Link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1301.3781.pdf&quot;&gt;Paper Link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Submit Date: Jan 16, 2013&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;one-hot encoding 방식은 word를 단순하게 표현하는 방법이다. word 자체가 갖는 정보를 담고 있지 않고 단순하게 index만을 담고 있는데, index 역시 word에 내재된 어떤 정보와도 관련이 없다. 본 논문에서는 word vector에 word 자체가 담고 있는 의미를 확실하게 담아내고자 했다. 구체적으로, 단순하게 유사한 단어들이 vector 공간에서 가까운 거리를 갖는 것에 그치지 않고 syntax, semantic 관점에서의 다양한 similarity를 반영하고자 했다. 동시에 one-hot encoding의 단점인 sparse vector problem을 해결해 dimension이 작으면서도 distribute한 word vector를 생성해냈다. 그 결과 sentence에서 단어가 등장하는 위치가 비슷한 word들의 vector가 가깝게 위치하는 것은 물론(syntax), “King” - “Man” + “Woman” = “Queen” 과 같은 vector 연산까지 정확하게 수행해낼 수 있었다(semantic).&lt;/p&gt;

&lt;h1 id=&quot;model-architectures&quot;&gt;Model Architectures&lt;/h1&gt;

&lt;p&gt;여러 model들을 비교하기 위해서 우선 computational complexity를 정의한다. model의 computational complexity는 #parameters로 정의한다. training complexity는 \(E \times T \times Q\)로 정의하는데, \(E\)는 #epochs이고, \(T\)는 len(training dataset), \(Q\)는 model specific하게 정의된 value이다.&lt;/p&gt;

&lt;h2 id=&quot;feedforward-neural-net-language-model-nnlm&quot;&gt;Feedforward Neural Net Language Model (NNLM)&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;example: “what will the fat cat sit on”, \(N=4\)&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/assets/images/2020-12-12-Efficient-Estimation-of-Word-Representations-in-Vector-Space/01.jpg&quot; alt=&quot;01.jpg&quot; /&gt;&lt;/p&gt;

    &lt;p&gt;출처: &lt;a href=&quot;https://wikidocs.net/45609&quot;&gt;https://wikidocs.net/45609&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;\(N\): input word 개수&lt;/p&gt;

&lt;p&gt;\(V\): vocabulary size&lt;/p&gt;

&lt;p&gt;\(D\): word representation dimenstion&lt;/p&gt;

&lt;p&gt;\(H\): hidden layer size&lt;/p&gt;

&lt;p&gt;일반적인 feed forward neural network를 사용한 language model이다.&lt;/p&gt;

&lt;p&gt;이전의 단어들 중 \(N\)개의 word에 대한 one-hot encoding을 input으로 받는다. 이 때 \(N\)은 n-gram에서의 \(N\)과 유사한 의미이며, hyper-parameter이다.&lt;/p&gt;

&lt;p&gt;각각의 input word에 대해 weight matrix \(W_P\)(\(V \times D\))를 곱한다. one-hot encoding과 \(W_P\)를 곱하는 것은 one-hot encoding에서 1인 index를 사용해 \(W_P\)에서 해당 row만 뽑아내는 것과 동일하다. 즉, \(W_P\)를 lookup table로 사용하는 것이다.&lt;/p&gt;

&lt;p&gt;이렇게 \(D\) 차원의 word vector를 얻어내는데, input에 대한 word vector들을 모두 concatenate해 projection layer \(P\)(\(N \times D)\)를 만들어낸다. 이는 여러 word에 대한 input을 하나의 matrix로 표현한 \(X\)(N \times V\()와\)W_P$$를 곱하는 연산과 동일하다. 이러한 연산의 의미를 직관적으로 이해해보자면 one-hot word vector가 아닌 embedding word vector를 얻는 과정이라고 볼 수 있다.&lt;/p&gt;

&lt;p&gt;이후 projection layer \(P\)에 새로운 weight matrix \(W_H\)(\(D \times H\))를 곱한 뒤 activation function에 넣어 hidden layer를 생성해낸다. 이는 여러 word embedding vector를 하나의 vector로 축약시키는 과정이다.&lt;/p&gt;

&lt;p&gt;hidden layer에서는 softmax와 cross entropy loss를 사용해 output에 대한 one-hot vector를 만들어낸다.&lt;/p&gt;

&lt;p&gt;전체 과정의 computational complexity는 다음과 같다.&lt;/p&gt;

\[Q = N \times D + N \times D \times H + H \times V\]

&lt;p&gt;위 수식에서 가장 부하가 큰 연산은 hidden layer에서 output을 만들어내는 연산인 \(H \times V\)이지만 hierarchical softmax를 사용하게 되면 \(H \times \log_2{V}\)로 연산량을 줄일 수 있다. 이 경우에는 가장 부하가 큰 연산은 projection layer에서 hidden layer를 만들어내는 연산인 \(N \times D \times H\)가 된다.&lt;/p&gt;

&lt;h2 id=&quot;recurrent-neural-net-language-model-rnnlm&quot;&gt;Recurrent Neural Net Language Model (RNNLM)&lt;/h2&gt;

&lt;p&gt;RNN을 사용한 language Model이다. RNN의 경우에는 projection layer를 제거한다. 또한 고정된 개수의 word만을 input으로 받는 NNLM과 달리 이전의 모든 word들에 대한 정보를 recurrent하게 hidden layer에 담게 된다.&lt;/p&gt;

&lt;p&gt;전체 과정의 computational complexitiy는 다음과 같다.&lt;/p&gt;

\[Q=H \times H + H \times V\]

&lt;p&gt;\(D\)를 \(H\)와 동일하게 만들었기 떄문에 위와 같은 수식이 된다. 역시나 동일하게 hierarchical softmax를 사용하면 \(H \times V\)를 \(H \times \log_2{V}\)로 줄일 수 있다. 이 경우네는 가장 부하가 큰 연산은 \(H \times H\)가 된다.&lt;/p&gt;

&lt;h1 id=&quot;new-log-linear-models&quot;&gt;New Log-linear Models&lt;/h1&gt;

&lt;p&gt;computational complexity를 줄이기 위해 2가지 단계를 제안한다.  continuous bag-of-words model(CBOW)을 사용하는 단계와 continuous skip-gram model(Skip-gram)을 사용하는 단계이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-12-12-Efficient-Estimation-of-Word-Representations-in-Vector-Space/02.jpg&quot; alt=&quot;02.jpg&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;continuous-bag-of-words-model&quot;&gt;Continuous Bag-of-Words Model&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-12-12-Efficient-Estimation-of-Word-Representations-in-Vector-Space/03.jpg&quot; alt=&quot;03.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;출처: &lt;a href=&quot;https://wikidocs.net/22660&quot;&gt;https://wikidocs.net/22660&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;NNLM에서 hidden layer를 제거한것이다. CBOW의 목표는 NNLM과 동일하게 word vector 1개를 예측하는 model이다. 다만 NNLM에서는 이전 word들만을 사용해 다음 word를 예측했다면, CBOW에서는 양방향(이전/이후)의 word 총 \(N\)개를 사용해 예측을 진행한다.&lt;/p&gt;

&lt;p&gt;CBOW는 NNLM에 비해 computational complexity를 감소시켰는데, NNLM에서의 projection layer는 activation function을 사용하지 않는 linear layer였다. 반면 hidden layer는 activation function을 사용하는 non-linear layer였다. hierarichial softmax를 사용한다는 가정 하에 가장 연산량이 많이 소요되는 layer가 hidden layer였으므로 이를 제거해 전체 연산량을 줄인 것이다. NNLM에서 hidden layer의 존재 의미는 여러 word embedding vector를 하나의 vector로 압축하는 것이었다면, CBOW에서는 이를 non-linear layer를 거치지 않고 단순하게 평균을 내게 된다. 따라서 CBOW의 projection layer는 word embedding vector(\(W_p\)에서의 row)들의 평균이다.&lt;/p&gt;

&lt;p&gt;computational complexity는 다음과 같다.&lt;/p&gt;

\[Q = N \times D + D \times \log_2{V}\]

&lt;h2 id=&quot;continuous-skip-gram-model&quot;&gt;Continuous Skip-gram Model&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-12-12-Efficient-Estimation-of-Word-Representations-in-Vector-Space/04.jpg&quot; alt=&quot;04.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;출처: &lt;a href=&quot;https://wikidocs.net/22660&quot;&gt;https://wikidocs.net/22660&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;CBOW와 유사하지만 input/output이 서로 뒤바뀐경우이다. 현재 word를 통해 이전, 이후의 word를 예측하는 model이다. 여러 word에 대해 prediction을 수행하기 때문에 당연하게도 연산량은 CBOW에 비해 많다. 하지만 skip-gram은 input word vector를 평균내지 않고 온전히 사용하기 때문에 등장 빈도가 낮은 word들에 대해 CBOW 대비 train 효과가 크다는 장점이 있다. CBOW에서는 각 word vector들을 평균내서 사용하기 때문에 등장 빈도가 낮은 word들은 제대로 된 학습을 기대하기 힘들다.&lt;/p&gt;

&lt;p&gt;computational complexity는 다음과 같다.&lt;/p&gt;

\[Q = C \times ( D + D \times \log_2{V})\]

&lt;p&gt;새로운 variable \(C\)가 등장하는데 \(C\)는 predict할 word의 개수와 관련된 값이다. 구체적으로, \(C\)는 predict할 word와 현재 word의 maximum distance이다. \([1,C)\)의 범위에서 random하게 value \(R\)을 뽑고, 현재 word 이전 \(R\)개, 이후 \(R\)개의 word에 대해서 predict를 수행한다. \(R\)의 기댓값은 \(\frac{1+(C-1)}{2}=\frac{C}{2}\)이고, predict 수행 횟수는 \(2R=2\times\frac{C}{2}=C\)이므로 전체 computational complexity는 1회 수행할 때의 값에 \(C\)를 곱한 것이다.&lt;/p&gt;

&lt;h1 id=&quot;results&quot;&gt;Results&lt;/h1&gt;

&lt;p&gt;word embedding의 성능을 측정하던 기존의 방식들은 유사한 word를 찾아내는 것이 대부분이었다. 예를 들면 “France”와 “Italy”가 유사한지를 측정하는 것 등이다. 본 논문에서는 이러한 방식에서 한 발 더 나아가 word 사이의 상관 관계를 뽑아내 다른 word에 적용시키는 방식을 도입했다. 예를 들면 “big”-“biggest”와 유사한 상관 관계를 갖는 word를 “small”에 대해 찾아 “smallest”를 맞추는 것이다. 본 논문에서 제시한 model로 학습한 word vector는 단순한 algebraic operation으로 이러한 task를 해결할 수 있다. X = vector(“biggest”) - vector(“big”) + vector(“small”)을 수행하면 X=vector(“smallest”)가 나올 것이다.&lt;/p&gt;

&lt;p&gt;이와 같은 semantic한 의미까지 제대로 담은 word vector를 활용할 경우 수많은 NLP task에서 뛰어난 성능 향상을 보이게 된다.&lt;/p&gt;

&lt;h2 id=&quot;task-description&quot;&gt;Task Description&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-12-12-Efficient-Estimation-of-Word-Representations-in-Vector-Space/05.jpg&quot; alt=&quot;05.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위와 같은 semantic, syntax 관계들을 목록화시켰다. 각 관계들에 대해 직접 word pair들을 수집하고, 각 word pair를 모두 섞어 random한 pair들을 만들어낸다. 이렇게 생성해낸 dataset으로 test를 수행하는 것이다. 이 때 정답과 완전히 동일한 word를 예측한 경우에만 정답으로 간주한다. 동의어나 유사어에 대해서도 오답 처리를 하기 때문에 사실상 100% accuracy는 불가능한 task이다.&lt;/p&gt;

&lt;h2 id=&quot;maximization-of-accuracy&quot;&gt;Maximization of Accuracy&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-12-12-Efficient-Estimation-of-Word-Representations-in-Vector-Space/06.jpg&quot; alt=&quot;06.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;word vector의 dimension과 #training words를 통해 실험을 진행했다. dimensionality나 #training words 중 하나를 고정시킨 뒤 다른 하나만을 증가시킬 경우 일정한 수준 이상으로 accuracy가 증가하지 않는 현상을 보였다. 기존의 많은 연구에서 단순히 training dataset의 크기만을 늘려가며 성능을 높이려 했지만, 많은 word가 train된다면 이에 대한 정보들을 담을 수 있는 충분한 dimension이 확보되어야 한다는 사실을 알 수 있다.&lt;/p&gt;

&lt;h2 id=&quot;comparison-of-model-architectures&quot;&gt;Comparison of Model Architectures&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-12-12-Efficient-Estimation-of-Word-Representations-in-Vector-Space/07.jpg&quot; alt=&quot;07.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;RNNLM이 가장 낮은 성능을 보였다. CBOW와 Skip-gram은 semantic, syntactic, relatedness에서 모두 NNLM을 능가했다. 특히나 Skip-gram은 Semantic Accuracy에서 다른 model들을 압도했다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-12-12-Efficient-Estimation-of-Word-Representations-in-Vector-Space/08.jpg&quot; alt=&quot;08.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;다른 여러 NNLM과 비교했을 때에도 CBOW와 skip-gram은 훨씬 더 좋은 성능을 보여준다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-12-12-Efficient-Estimation-of-Word-Representations-in-Vector-Space/09.jpg&quot; alt=&quot;09.jpg&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;microsoft-research-sentence-completion-challenge&quot;&gt;Microsoft Research Sentence Completion Challenge&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-12-12-Efficient-Estimation-of-Word-Representations-in-Vector-Space/10.jpg&quot; alt=&quot;10.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Microsoft Research Sentence Completion Challenge는 1040개의 sentence가 주어지는게, 각 sentence는 1개의 word가 빠져 있다. 각 sentence에서 빠진 word를 predict하는 task이다. 이 task에서 skip-gram 단독으로는 기존의 model들에 비해 다소 낮은 수치를 보였지만, RNNLM과 결합한 뒤에는 SOTA를 달성했다.&lt;/p&gt;

&lt;h1 id=&quot;examples-of-the-learned-relationships&quot;&gt;Examples of the Learned Relationships&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-12-12-Efficient-Estimation-of-Word-Representations-in-Vector-Space/11.jpg&quot; alt=&quot;11.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;단어 사이의 상관관계를 분석해 다른 단어에 대해 유사한 관계를 갖는 단어를 예측하는 task에서 본 논문의 model은 60%의 정확도를 달성했다. 더 높은 정확도를 달성하기 위해서는 더 많은 dataset을 사용하고, 또 각 단어 사이의 상관관계 vector를 여러 단어쌍 사이의 subtract vector의 평균으로 만들어내면 될 것이다.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;CBOW와 skip-gram이라는 새로운 word embedding 학습 방법을 제안했다. 기존의 여러 model들에 비해 연산량이 현저히 적고, 간단한 model임에도 매우 높은 성능을 보였다. 또한 word embedding vector의 syntax, semantic 성능을 측정할 수 있는 새로운 dataset을 제시했다.&lt;/p&gt;
</description>
        <pubDate>Fri, 11 Dec 2020 18:00:00 -0600</pubDate>
        <link>http://0.0.0.0:4000/machine%20learning/paper%20review/Efficient-Estimation-of-Word-Representations-in-Vector-Space/</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/machine%20learning/paper%20review/Efficient-Estimation-of-Word-Representations-in-Vector-Space/</guid>
        
        <category>NLP</category>
        
        
        <category>Machine Learning</category>
        
        <category>Paper Review</category>
        
      </item>
    
      <item>
        <title>[운영체제] File System</title>
        <description>&lt;p&gt;숭실대학교 컴퓨터학부 홍지만 교수님의 2020-2학기 운영체제 강의를 정리 및 재구성했다.&lt;/p&gt;

&lt;h2 id=&quot;block&quot;&gt;Block&lt;/h2&gt;

&lt;p&gt;OS는 disk를 일정한 크기의 block으로 나누어 저장한다. 대개 block의 크기는 4KB이다. 각 block은 그 목적에 따라 아래의 4가지로 구분지을 수 있다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Super block&lt;/p&gt;

    &lt;p&gt;file system의 global한 정보들을 담는 block으로 하나의 file system에 1개만 존재한다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Allocation structure block&lt;/p&gt;

    &lt;p&gt;bitmap, linked list 등의 방법으로 inode struct와 data에 대해 used/unused 정보가 저장된다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Key meta data block&lt;/p&gt;

    &lt;p&gt;inode struct의 table이 저장된다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;User data block&lt;/p&gt;

    &lt;p&gt;실제 data들이 저장된다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;아래는 2개의 allocation structure block이 각각 inode bitmap, data bitmap으로 운용되고, 최대 80개의 inode struct가 5개의 key meta data block에 저장되는 경우의 전체 disk 구조이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-12-01-File-System/01.png&quot; alt=&quot;01.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-12-01-File-System/02.png&quot; alt=&quot;02.png&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;inode-struct&quot;&gt;inode struct&lt;/h2&gt;

&lt;p&gt;inode struct에는 file에 대한 meta data가 저장된다. 각 file마다 하나의 inode struct가 부여된다. file의 size, mode, permission, 소유자, 각종 시각 등이 저장된다. unix의 ls 명령어로 출력되는 정보들,  unistd.h의 stat() 함수에서 얻을 수 있는 struct stat 에 담긴 정보들은 모두 inode struct에서 가져온 것이다.&lt;/p&gt;

&lt;p&gt;inode struct에서 가장 중요한 정보는 실제 data가 저장된 user data block의 pointer이다. file의 크기가 block의 size보다 클 경우에는 여러 block을 사용해야 하기 때문에 data block을 가리키는 여러 pointer 변수들이 inode struct에 존재하게 된다.&lt;/p&gt;

&lt;p&gt;inode struct에서 pointer로 data block을 가리키는 방법에는 총 2가지가 있는데, direct pointer로 data block을 직접 가리키는 방법, indirect pointer로 disk block을 가리키는 pointer들이 저장된 block을 가리키는 방법이다. direct pointer만을 사용할 경우 한 file이 가질 수 있는 최대 size에 제약이 생기게 된다. 예를 들어 block size가 4KB이고, inode struct가 direct pointer 10개를 운용한다고 할 경우에는 한 file의 최대 size는 10&lt;em&gt;4KB=40KB가 된다. 반면 single indirect pointer 10개를 운용한다 할 경우 각 indirect pointer가 최대 저장할 수 있는 pointer의 개수는 block size / sizeof(pointer)이므로 4KB/4B = 1K이다. 각 pointer는 data block을 가리킬 수 있으므로 10&lt;/em&gt;1K*4KB = 40MB가 된다.&lt;/p&gt;

&lt;h2 id=&quot;directory&quot;&gt;directory&lt;/h2&gt;

&lt;p&gt;directory는 file의 한 종류이다. 그렇다면 directory의 inode struct는 어떻게 구성되어 있을까? inode struct의 일반적인 구성과 동일하다. directory의 data block에서의 data가 다른데, directory 하위 항목들에 대한 linked list를 저장된다. linked list의 각 node는 inode number와 name을 구성 요소로 갖는다. 이 때 inode struct pointer를 직접 저장하지 않고 단순 index 번호만 저장함으로써 공간을 절약한다. directory마다 단순 선형 linked list를 운용하게 될 경우 깊은 계층 구조를 갖는 directory에서 성능이 많이 하락하기 때문에 B-tree와 같은 자료구조를 사용해 성능을 향상시키기도 한다.&lt;/p&gt;

&lt;h2 id=&quot;file-descriptor--inode&quot;&gt;file descriptor &amp;amp; inode&lt;/h2&gt;

&lt;p&gt;각 process는 고유한 file descriptor table을 운용한다. 그 중 0번은 stdin, 1번은 stdout, 2번은 stderr file로 미리 예약되어 있다. file descriptor란 해당 process가 어떤 file을 open했을 때 return되는 값인데, 한 process가 한 file을 여러 번 open할 수도 있다. 이 때마다 file descriptor는 새로 할당되게 된다. 즉, 같은 file에 대해 다른 file descriptor를 동시에 가질 수도 있는 것이다. 각 file descriptor는 open file table을 가리킨다. open file table의 각 항목은 status(read/write 등), offset, inode sturct pointer 등을 저장한다. open file table에서 inode struct를 가리키고, inode struct의 block pointer가 실제로 data가 저장된 data block을 가리키는 것이다.&lt;/p&gt;

&lt;p&gt;정리하자면, file descriptor table은 process마다 별개로 부여되는 local 구조이고, open file table, inode table은 전체 file system에서 하나를 운용하는 global 구조이다. 각 항목이 가리키는 방향은 file descriptor table → open file table → inode table → data block이다.&lt;/p&gt;

&lt;h2 id=&quot;reading-a-file-from-disk&quot;&gt;Reading a File from Disk&lt;/h2&gt;

&lt;p&gt;disk에서 실제 file을 읽어들이는 과정을 따라가보자. super block만이 memory에 올라와 있고, bitmap이 담긴 allocation structure block은 disk에 남아있는 상태라고 가정해보자. 다음의 순서를 따른다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-12-01-File-System/03.png&quot; alt=&quot;03.png&quot; /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;root directory (“ / “) read
    &lt;ol&gt;
      &lt;li&gt;root의 inode struct read&lt;/li&gt;
      &lt;li&gt;root의 block pointer 획득&lt;/li&gt;
      &lt;li&gt;root의 data block read&lt;/li&gt;
      &lt;li&gt;root의 하위 항목들에 대한 linked list 획득&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;foo directory (“ /foo “) read
    &lt;ol&gt;
      &lt;li&gt;root의 하위 항목들에 대한 linked list에서 이름이 “foo”인 항목의 inode number 획득&lt;/li&gt;
      &lt;li&gt;inode number를 통해 inode table에서의 주소 계산&lt;/li&gt;
      &lt;li&gt;foo의 inode sturct read&lt;/li&gt;
      &lt;li&gt;foo의 block pointer 획득&lt;/li&gt;
      &lt;li&gt;foo의 하위 항목들에 대한 linked list 획득&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;bar file (“ /foo/bar “) read
    &lt;ol&gt;
      &lt;li&gt;bar의 하위 항목들에 대한 linked list에서 이름이 “bar”인 항목의 inode number 획득&lt;/li&gt;
      &lt;li&gt;inode number를 통해 inode table에서의 주소 계산&lt;/li&gt;
      &lt;li&gt;bar의 inode struct read&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;bar[0] read
    &lt;ol&gt;
      &lt;li&gt;bar의 inode struct에서 첫번째 data block pointer 획득&lt;/li&gt;
      &lt;li&gt;data block read&lt;/li&gt;
      &lt;li&gt;bar inode struct write(access time 등 갱신 위함)&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;bar[1] read
    &lt;ol&gt;
      &lt;li&gt;bar의 inode struct에서 두번째 data block pointer 획득&lt;/li&gt;
      &lt;li&gt;data block read&lt;/li&gt;
      &lt;li&gt;bar inode struct write(access time 등 갱신 위함)&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;bar[2] read
    &lt;ol&gt;
      &lt;li&gt;bar의 inode struct에서 세번째 data block pointer 획득&lt;/li&gt;
      &lt;li&gt;data block read&lt;/li&gt;
      &lt;li&gt;bar inode struct write(access time 등 갱신 위함)&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;creating--writing-a-file-from-disk&quot;&gt;Creating &amp;amp; Writing a File from Disk&lt;/h2&gt;

&lt;p&gt;disk에서 실제 file을 생성하고 write하는 과정을 따라가보자. 역시나 super block만이 memory에 올라와 있고, bitmap이 담긴 allocation structure block은 disk에 남아있는 상태라고 가정한다. 다음의 순서를 따른다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-12-01-File-System/04.png&quot; alt=&quot;04.png&quot; /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;root directory (“ / “) read
    &lt;ol&gt;
      &lt;li&gt;root의 inode struct read&lt;/li&gt;
      &lt;li&gt;root의 block pointer 획득&lt;/li&gt;
      &lt;li&gt;root의 data block read&lt;/li&gt;
      &lt;li&gt;root의 하위 항목들에 대한 linked list 획득&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;foo directory (“ /foo “) read
    &lt;ol&gt;
      &lt;li&gt;root의 하위 항목들에 대한 linked list에서 이름이 “foo”인 항목의 inode number 획득&lt;/li&gt;
      &lt;li&gt;inode number를 통해 inode table에서의 주소 계산&lt;/li&gt;
      &lt;li&gt;foo의 inode sturct read&lt;/li&gt;
      &lt;li&gt;foo의 block pointer 획득&lt;/li&gt;
      &lt;li&gt;foo의 하위 항목들에 대한 linked list 획득&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;bar file (“ /foo/bar “) create
    &lt;ol&gt;
      &lt;li&gt;현재 사용 중인 inode number을 확인하기 위해 inode bitmap read&lt;/li&gt;
      &lt;li&gt;미사용 중인 inode number 선택 후 사용 중으로 변경하기 위해 inode bitmap write&lt;/li&gt;
      &lt;li&gt;foo의 하위 항목들에 대한 linked list에 획득한 inode number와 “bar” 명칭으로 항목 추가하기 위해 bar data block write&lt;/li&gt;
      &lt;li&gt;bar inode struct read (inode struct 초기화 위함)&lt;/li&gt;
      &lt;li&gt;bar inode struct write (inode struct 초기화 위함)&lt;/li&gt;
      &lt;li&gt;foo inode struct write (access time 등 갱신 위함)&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;bar file (“ /foo/bar “) write
    &lt;ol&gt;
      &lt;li&gt;write 가능한 여유 있는 data block 존재 여부 확인하기 위해 bar inode struct read&lt;/li&gt;
      &lt;li&gt;현재 사용 중인 data block number 확인하기 위해 data bitmap read&lt;/li&gt;
      &lt;li&gt;미사용 중인 data block number 선택 후 사용 중으로 변경하기 위해 data bitmap write&lt;/li&gt;
      &lt;li&gt;bar data block write&lt;/li&gt;
      &lt;li&gt;bar inode write (access time 등 갱신 위함)&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;ffs-fast-file-system&quot;&gt;FFS (Fast File System)&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-12-01-File-System/05.png&quot; alt=&quot;05.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;fast file system은 기존의 unix file system에서 성능을 더 향상시킨 file system이다. 기존의 file system은 disk 전체에 super block, inode bitmap, data bitmap이 disk에 오직 1개만 존재했다. 또한 inode 역시 disk의 한 영역에 몰려서 저장되어 있어 실제 data block들과의 disk상에서의 물리적 거리가 멀 수 밖에 없었다. FFS는 이러한 단점을 해결하고자 disk 전체를 여러 group으로 나누고, 각 group마다 super block, bitmaps, inodes, data block들을 부여한다. 이를 통해 inode에서 참조하는 data block과 실제 inode가 저장된 block 사이의 물리적 거리가 줄어들어 seek time이 감소한다.&lt;/p&gt;

&lt;p&gt;또한 FFS는 directory 구조 역시 개선했는데, 기존의 file system은 단순한 계층 구조였기에 하위 file들의 data block이 부모 directory의 data block과 멀리 떨어져 있을 가능성이 농후했다. FFS는 동일한 directory에 있는 file에 접근할 확률이 40%나 된다는 통계에 기반해 (Name-based Locality) directory와 그 하위 file들을 disk 내에서 같은 group 안에 배치하도록 했다. 이를 통해 seek time을 감소시킬 수 있었다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-12-01-File-System/06.png&quot; alt=&quot;06.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;마지막으로, FFS는 disk layout에 대해서 최적화를 수행했다. 초기 hard disk는 HW의 성능이 떨어져 rotation 속도가 많이 느렸다. 하지만 점차 HW가 발전함에 따라 rotation 속도가 비약적으로 상승했고, 연속된 sector를 읽어들이기에는 이미 head가 다음 sector를 지나쳐버리는 증상이 발생하게 되었다. 이를 해결하기 위해 FFS는 다음 sector를 연속적으로 배치하지 않고, 1칸 뒤에 배치하는 식으로 sector 배치를 변경했다.&lt;/p&gt;

&lt;p&gt;이 외에도 FFS는 block의 크기를 줄여 내부 단편화 현상을 감소시키고, symbolic link를 도입하는 등의 여러 변화를 채택했다.&lt;/p&gt;

&lt;h1 id=&quot;crash-consistency&quot;&gt;Crash Consistency&lt;/h1&gt;

&lt;h2 id=&quot;disk-crash-scenario&quot;&gt;Disk Crash Scenario&lt;/h2&gt;

&lt;p&gt;disk I/O 과정에서 crash가 발생하는 경우에 대해서 살펴보자. 이미 존재하는 file에 대해 새로운 data를 append하는 경우에는 data bitmap, inodes, data block을 갱신해야 한다. 위의 3가지 block들에 대한 갱신은 atomic하게 이루어져야 한다. 이 과정에서 발생할 수 있는 crash scenario는 다음의 6가지이다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;data block만 정상 갱신, data bitmap, inodes는 crash되는 경우&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/assets/images/2020-12-01-File-System/07.png&quot; alt=&quot;07.png&quot; /&gt;&lt;/p&gt;

    &lt;p&gt;data bitmap과 inodes가 모두 crash되었기 때문에 bitmap과 inodes 사이의 불일치(inconsistent)는 없다. 따라서 consistent한 상황이다. 대신 data block은 갱신이 되었는데, 해당 block은 data bitmap에서도 unused로 표시가 되어 있고, inodes에도 data block 포인터가 연결이 되어 있지 않기 떄문에 garbage data이다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;inodes만 정상 갱신, data bitmap, data block은 crash되는 경우&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/assets/images/2020-12-01-File-System/08.png&quot; alt=&quot;08.png&quot; /&gt;&lt;/p&gt;

    &lt;p&gt;data bitmap은 crash, inodes는 정상 갱신되었기 때문에 inconsistent한 상황이다. inodes는 이미 data block을 가리키는데 해당 data block에는 data가 쓰여 있지 않고, data bitmap에서도 해당 data block은 unused로 표시가 되어 있다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;data bitmap만 정상 갱신, inodes, data block은 crash되는 경우&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/assets/images/2020-12-01-File-System/09.png&quot; alt=&quot;09.png&quot; /&gt;&lt;/p&gt;

    &lt;p&gt;data bitmap은 정상 갱신되었지만, inodes는 crash되었기 때문에 inconsistent한 상황이다. data block에는 write가 되지 않았고, inodes에서도 해당 data block을 가리키지 않는데 datat bitmap에서는 used로 표시가 되어있는 경우이다. 이후 해당 data block은 사용되지 못하고 낭비될 것이다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;data bitmap, inodes는 정상 갱신, data block만 crash되는 경우&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/assets/images/2020-12-01-File-System/10.png&quot; alt=&quot;10.png&quot; /&gt;&lt;/p&gt;

    &lt;p&gt;data bitmap과 inodes가 모두 정상갱신 되었기 때문에 consistent한 상황이다. data block에 write만 되지 않은 것이기 때문에 garbage data가 저장된 상태이다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;inodes, data block은 정상 갱신, data bitmap만 crash되는 경우&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/assets/images/2020-12-01-File-System/11.png&quot; alt=&quot;11.png&quot; /&gt;&lt;/p&gt;

    &lt;p&gt;data bitmap은 crash되고, inodes는 정상 갱신되었기 때문에 inconsistent한 상황이다. 이 경우 inode에서 data block을 가리키고, 해당 data block에는 정상적인 data가 쓰여져 있음에도 data bitmap에서 unused로 표시가 되어 있기 때문에 언제든 덮어씌워질 수 있고, 다른 inode가 동일한 data block을 가리킬 수도 있다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;data bitmap, data block은 정상 갱신, inodes만 crash되는 경우&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/assets/images/2020-12-01-File-System/12.png&quot; alt=&quot;12.png&quot; /&gt;&lt;/p&gt;

    &lt;p&gt;data bitmap은 정상 갱신, inodes는 crash되었기 때문에 inconsistent한 상황이다. 이 경우 data block에도 정상적인 data가 쓰여져 있고 data bitmap에서도 used로 표시가 되었지만 inodes에서 해당 data block을 가리키지 않기 때문에 해당 data block은 어떤 file에도 연결되지 못한다. 이를 orphan data block이라고 한다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;fsck&quot;&gt;FSCK&lt;/h2&gt;

&lt;p&gt;FSCK는 unix에서 file system에서의 crash inconsistency를 찾아내 해결하는 도구이다. 초기의 file system은 crash inconsistency를 발견하더라도 무시했다가 rebooting 과정에서 이를 해결했다. FSCK는 rebooting 없이도 이를 해결하고자 개발되었다. FSCK 동작 중에는 file system이 어떠한 다른 동작도 수행하지 않는다고 가정한다.&lt;/p&gt;

&lt;p&gt;FSCK는 inodes 정보를 기반으로 bitmap을 재갱신한다. 개념적으로 간단하고 별도의 write overhead가 없다는 장점이 있지만, 과도하게 많은 연산을 수행하고, consistent한 경우에 대해서는 해결이 불가능하다는 단점이 있다.&lt;/p&gt;

&lt;h2 id=&quot;journaling-wal-write-ahead-logging&quot;&gt;Journaling (WAL: Write-Ahead Logging)&lt;/h2&gt;

&lt;p&gt;Journaling은 disk I/O, 특히 write 연산 시에 log를 기록해 저장했다가 이를 crash inconsistency를 해결하는데 사용하는 방식이다. write 요청이 올 경우 disk는 이를 즉시 갱신하지 않고 write 작업 중 수행할 연산들에 대한  log를 미리 작성한다. 이후 만약 write 도중 disk crash가 발생할 경우, 해당 log를 확인해 다시 write를 수행한다. 이러한 log를 저장하는 용도로 disk에 journal block을 새로 추가한다. log의 저장 단위는 transaction이다.&lt;/p&gt;
</description>
        <pubDate>Mon, 30 Nov 2020 18:00:00 -0600</pubDate>
        <link>http://0.0.0.0:4000/operating%20system/File-System/</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/operating%20system/File-System/</guid>
        
        <category>Operating System</category>
        
        
        <category>Operating System</category>
        
      </item>
    
      <item>
        <title>[운영체제] Semaphore</title>
        <description>&lt;p&gt;숭실대학교 컴퓨터학부 홍지만 교수님의 2020-2학기 운영체제 강의를 정리 및 재구성했다.&lt;/p&gt;

&lt;h1 id=&quot;semaphore&quot;&gt;Semaphore&lt;/h1&gt;

&lt;p&gt;semaphore는 다수의 thread 사이의 병행성 유지를 위해 OS 단위에서 제공되는 기법이다. 기본적인 작동 원리는 특정 thread가 특정 signal을 수신할 때까지 정해진 위치에서 wait하도록 강제하는 것이다.&lt;/p&gt;

&lt;h2 id=&quot;counting-semaphore&quot;&gt;counting semaphore&lt;/h2&gt;

&lt;p&gt;counting semahpore는 정수값을 갖는 counting 변수와 3가지 연산으로 구성된다. 범용 semaphore라고도 불리운다. 3가지 연산은 아래와 같다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;semInit(): semaphore 변수를 음이 아닌 값(대개 1)으로 초기화를 수행한다.&lt;/li&gt;
  &lt;li&gt;semWait(): semaphore 변수 값을 감소시킨다. 만약 값이 음수가 되면 semWait()을 호출한 thread는 block된다. 그 외에는 해당 thread는 정상적으로 계속 수행한다.&lt;/li&gt;
  &lt;li&gt;semSignal(): semaphore 변수 값을 증가시킨다. 만약 값이 양수가 아니면 semWait()에 의해 block된 thread 중 하나를 깨운다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;기본적인 pseudo code는 다음과 같다.&lt;/p&gt;

&lt;div class=&quot;language-c highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;typedef&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;struct&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
	&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;count&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;queue&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;waitQueue&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;semaphore&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;semInit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;semaphore&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;count&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;semWait&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;semaphore&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;count&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;count&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
		&lt;span class=&quot;c1&quot;&gt;//요청한 thread를 s.waitQueue에 push&lt;/span&gt;
		&lt;span class=&quot;c1&quot;&gt;//요청한 thread의 상태를 block으로 변경&lt;/span&gt;
	&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;semSignal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;semaphore&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;count&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;count&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
		&lt;span class=&quot;c1&quot;&gt;//s.waitQueue에서 thread 1개를 pop&lt;/span&gt;
		&lt;span class=&quot;c1&quot;&gt;//pop한 thread의 상태를 runnable로 변경 후 OS의 readyQueue에 push&lt;/span&gt;
	&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;count 변수는 값이 음수인 경우에 그 절대값은 대기 queue의 길이를 의미한다.&lt;/p&gt;

&lt;h2 id=&quot;binary-semaphore-mutex&quot;&gt;binary semaphore (mutex)&lt;/h2&gt;

&lt;p&gt;mutex는 semaphore 변수가 0 또는 1의 binary 값만 갖는 semaphore를 뜻한다. 동일하게 3가지 연산으로 구성된다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;semInitB(): semaphore 변수를 0 또는 1로 초기화한다.&lt;/li&gt;
  &lt;li&gt;semWaitB(): semaphore 변수 값을 확인해 0일 경우 semWaitB()를 호출한 thread는 block되고, 1일 경우 값을 0으로 변경시킨 뒤 thread는 계속 수행한다.&lt;/li&gt;
  &lt;li&gt;semSignalB(): block된 thread가 있는지 확인한 후, 만약 있을 경우 해당 thread들 중 하나를 깨우고, 없을 경우 semaphore 변수 값을 1로 설정한다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;pseudo code는 다음과 같다.&lt;/p&gt;

&lt;div class=&quot;language-c highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;typedef&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;struct&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
	&lt;span class=&quot;kt&quot;&gt;_Bool&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;queue&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;waitQueue&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;binary_semaphore&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;semInitB&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;binary_semaphore&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;semWaitB&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;binary_semaphore&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
		&lt;span class=&quot;c1&quot;&gt;//요청한 thread를 s.waitQueue에 push&lt;/span&gt;
		&lt;span class=&quot;c1&quot;&gt;//요청한 thread의 상태를 block으로 변경&lt;/span&gt;
	&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;semSignalB&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;binary_semaphore&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;waitQueue&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;empty&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
		&lt;span class=&quot;c1&quot;&gt;//s.waitQueue에서 thread 1개를 pop&lt;/span&gt;
		&lt;span class=&quot;c1&quot;&gt;//pop한 thread의 상태를 runnable로 변경 후 OS의 readyQueue에 push&lt;/span&gt;
	&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;binary semahpore는 일반 범용 semaphore에 비해 구현이 간단하다는 장점이 있다. 둘 모두 waitQueue를 운용한다는 점에서 공통적이다.&lt;/p&gt;

&lt;h2 id=&quot;strong--weak-semaphore&quot;&gt;strong / weak semaphore&lt;/h2&gt;

&lt;p&gt;queue에서 FIFO 방식을 사용하는 semaphore를 강성(strong) semaphore라고 하고, 특별히 queue의 순서를 명시하지 않은 semaphore를 약성(weak) semaphore라고 한다. 하지만 실제로 대부분의 OS에서는 강성 semaphore를 사용한다. starvation이 없고, 직관적이며 구현하기도 용이하기 때문이다.&lt;/p&gt;

&lt;p&gt;아래는 강성 semaphore의 예시이다. D thread는 생산자,와 A, B, C thread는 소비자인 문제이다. 초기 semaphore 변수 s가 값이 1로 시작된다.  s의 값이 음수일 때에는 그 절댓값이 기다리는 thread의 개수(waitQueue 내 thread의 개수)를 뜻하고, s의 값이 음수가 아닐 때에는 생산자가 생성한 자원의 여분 개수를 뜻한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-11-24-Semaphore/01.png&quot; alt=&quot;01.png&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;mutual-exclusion-problem&quot;&gt;mutual exclusion problem&lt;/h2&gt;

&lt;p&gt;범용 semaphore를 사용해 상호 배제 문제를 해결해보자. 상호 배제 문제란 동일한 자원에 접근하려는 n개의 thread의 병행성을 처리하는 문제이다. semInit()에서 count 변수를 0이 아닌 변수로 초기화한다. count 변수의 초기값은 자원의 개수를 의미한다. 따라서 count 변수는 0으로 초기화 되어서는 안된다. 모든 thread가 무한히 block될 것이기 때문이다. 각 thread에서 critical section(임계 영역)을 생성하게 되는데, critical section이란 한 번에 최대 1개의 thread만이 접근할 수 있는 영역이다. semWait()~semSignal() 사이의 영역이 된다. pseudo code는 다음과 같다.&lt;/p&gt;

&lt;div class=&quot;language-c highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;semaphore&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;thread_execute&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;thread_no&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;semWait&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
		&lt;span class=&quot;c1&quot;&gt;//임계 영역&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;semSignal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
		&lt;span class=&quot;c1&quot;&gt;//임계 영역 이후&lt;/span&gt;
	&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;semInit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_of_threads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;thread_start&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;thread가 1, 2, 3 순서대로 실행된다고 가정했을 때 각 thread는 아래와 같은 형태로 실행된다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-11-24-Semaphore/02.png&quot; alt=&quot;02.png&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;producer-consumer-problem&quot;&gt;producer-consumer problem&lt;/h2&gt;

&lt;p&gt;생산자-소비자 문제를 mutex를 이용해 해결해보자. 생산자-소비자 문제는 다수의 생산자 thread가 각자 자원을 생성해 공용 buffer에 저장하고, 다수의 소비자 thread가 공용 buffer에서 자원을 1개씩 소비하는 상황의 병행성을 처리하는 문제이다. 공용 buffer에는 한 번에 1개의 thread만 접근 가능하다(critical section)는 조건이 있다. 우선 공용 buffer가 무한한 크기를 갖는다고 가정한다. 이 때 in과 out이라는 pointer 변수를 사용하는데, in은 다음에 생산자가 생성한 자원이 저장될 buffer에서의 위치이며, out은 다음에 소비자가 소비할 자원이 저장된 buffer에서의 위치이다. 따라서 out&amp;lt;in인 경우에만 소비자가 소비할 자원이 있는 것이다. 전체 pseudo code는 다음과 같다.&lt;/p&gt;

&lt;div class=&quot;language-c highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;                     &lt;span class=&quot;c1&quot;&gt;//in-out의 값&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;binary_semaphore&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;        &lt;span class=&quot;c1&quot;&gt;//buffer의 접근을 제어하는 mutex&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;binary_semaphore&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;delay&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;//buffer가 비었는지를 확인해 소비를 제어하는 mutex&lt;/span&gt;

&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;producer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;produce&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;       &lt;span class=&quot;c1&quot;&gt;//자원 생산&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;semWaitB&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
		&lt;span class=&quot;cm&quot;&gt;/*
			                     critical section start
		*/&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;           &lt;span class=&quot;c1&quot;&gt;//buffer에 push&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;            &lt;span class=&quot;c1&quot;&gt;//buffer.empty()==false가 된 상황&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;semSignalB&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;delay&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;   &lt;span class=&quot;c1&quot;&gt;//consumer 중 1개 block 해제&lt;/span&gt;
		&lt;span class=&quot;cm&quot;&gt;/*
		                       critical section end
		*/&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;semSignalB&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
		&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
	&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;err&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;consumer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
	&lt;span class=&quot;c1&quot;&gt;//consumer가 producer보다 먼저 실행되는 상황(buffer.empty()==true)를 막기 위해 block&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;semWaitB&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;delay&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;semWaitB&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
		&lt;span class=&quot;cm&quot;&gt;/*
		                     critical section start
		*/&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;take&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;        &lt;span class=&quot;c1&quot;&gt;//buffer에서 pop&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
		&lt;span class=&quot;cm&quot;&gt;/*
			                	critical section end
		*/&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;semSignalB&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;consume&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;       &lt;span class=&quot;c1&quot;&gt;//자원 소비&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;          &lt;span class=&quot;c1&quot;&gt;//buffer.empty()==true가 된 상황&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;semWaitB&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;delay&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;//thread block&lt;/span&gt;
	&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;semInitB&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;semInitB&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;delay&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_of_producers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;thread_start&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_of_consumers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;thread_start&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;위의 code는 producer와 consumer 내의 while문이 매 번 atomic하게 전체가 함께 실행되면 정상적으로 작동할 것이다. 하지만 while loop가 1번 도는 사이에 scheduling이 발생하지 않을 것이라는 보장이 없다. 만약 consumer에서 semSignalB(s)와 if(n==0) 사이에서 scheduling이 발생해 producer가 실행된다면 n은 0에서 1로 변경될 것이고, 그렇다면 다시 scheduling이 되어 consumer로 돌아왔을 때 if(n==0)을 만족하지 못하므로 semWaitB(delay)가 실행되지 않을 것이다. 이는 소비자가 한 개의 thread라면 큰 문제가 되지 않지만, 다수의 thread일 경우에는 문제 상황이 된다. empty임에도 여러 소비자 thread 모두 block되지 않을 수 있기 때문이다.&lt;/p&gt;

&lt;h3 id=&quot;solution-1-보조-변수-사용&quot;&gt;solution 1: 보조 변수 사용&lt;/h3&gt;

&lt;div class=&quot;language-c highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;                     &lt;span class=&quot;c1&quot;&gt;//in-out의 값&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;binary_semaphore&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;        &lt;span class=&quot;c1&quot;&gt;//buffer의 접근을 제어하는 mutex&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;binary_semaphore&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;delay&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;//buffer가 비었는지를 확인해 소비를 제어하는 mutex&lt;/span&gt;

&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;producer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;produce&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;       &lt;span class=&quot;c1&quot;&gt;//자원 생산&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;semWaitB&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
		&lt;span class=&quot;cm&quot;&gt;/*
			                     critical section start
		*/&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;           &lt;span class=&quot;c1&quot;&gt;//buffer에 push&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;            &lt;span class=&quot;c1&quot;&gt;//buffer.empty()==false가 된 상황&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;semSignalB&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;delay&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;   &lt;span class=&quot;c1&quot;&gt;//consumer 중 1개 block 해제&lt;/span&gt;
		&lt;span class=&quot;cm&quot;&gt;/*
		                       critical section end
		*/&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;semSignalB&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
		&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
	&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;err&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;consumer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
	&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;
	&lt;span class=&quot;c1&quot;&gt;//consumer가 producer보다 먼저 실행되는 상황(buffer.empty()==true)를 막기 위해 block&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;semWaitB&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;delay&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;semWaitB&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
		&lt;span class=&quot;cm&quot;&gt;/*
		                     critical section start
		*/&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;take&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;        &lt;span class=&quot;c1&quot;&gt;//buffer에서 pop&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
		&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;
		&lt;span class=&quot;cm&quot;&gt;/*
			                	critical section end
		*/&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;semSignalB&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;consume&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;       &lt;span class=&quot;c1&quot;&gt;//자원 소비&lt;/span&gt;
		&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;          &lt;span class=&quot;c1&quot;&gt;//buffer.empty()==true가 된 상황&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;semWaitB&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;delay&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;//thread block&lt;/span&gt;
	&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;semInitB&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;semInitB&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;delay&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_of_producers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;thread_start&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_of_consumers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;thread_start&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;n의 값이 변경되는 것을 막기 위해 critical section 내에서 보조 변수 m에 현재 n의 값을 임시로 저장한다. 이후 critical section 밖의 if문에서 n 대신 m이 0인지를 확인하게 된다.&lt;/p&gt;

&lt;h3 id=&quot;solution-2-범용-semaphore-사용&quot;&gt;solution 2: 범용 semaphore 사용&lt;/h3&gt;

&lt;p&gt;binary semaphore가 아닌 범용 semaphore를 사용하면 애초에 위의 문제 상황이 발생하지 않는다.&lt;/p&gt;

&lt;div class=&quot;language-c highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;semaphore&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;               &lt;span class=&quot;c1&quot;&gt;//buffer의 접근을 제어하는 semaphore&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;semaphore&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;               &lt;span class=&quot;c1&quot;&gt;//buffer에 들어있는 자원의 개수를 제어하는 semaphore&lt;/span&gt;

&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;producer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;produce&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;       &lt;span class=&quot;c1&quot;&gt;//자원 생산&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;semWait&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
		&lt;span class=&quot;cm&quot;&gt;/*
			                     critical section start
		*/&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;           &lt;span class=&quot;c1&quot;&gt;//buffer에 push&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;semSignal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;          &lt;span class=&quot;c1&quot;&gt;//consumer 중 1개 block 해제&lt;/span&gt;
		&lt;span class=&quot;cm&quot;&gt;/*
		                       critical section end
		*/&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;semSignal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
		&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
	&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;err&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;consumer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;semWait&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;          &lt;span class=&quot;c1&quot;&gt;//buffer.empty()==true일 때 실행되는 것을 방지하기 위해 block&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;semWait&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
		&lt;span class=&quot;cm&quot;&gt;/*
		                     critical section start
		*/&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;take&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;        &lt;span class=&quot;c1&quot;&gt;//buffer에서 pop&lt;/span&gt;
		&lt;span class=&quot;cm&quot;&gt;/*
			                	critical section end
		*/&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;semSignal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;consume&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;       &lt;span class=&quot;c1&quot;&gt;//자원 소비&lt;/span&gt;
	&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;semInit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;semInit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_of_producers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;thread_start&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_of_consumers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;thread_start&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;err&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;공용 변수 n과 delay를 통합해 하나의 범용 semaphore n으로 운용한다. n의 값에 따라 delay를 wait 또는 signal하지 않고 무조건적으로 producer에서는 semSignal(n), consumer에서는 semWait(n)하게 된다. n은 buffer에 들어가 있는 자원의 개수(음수일 경우 그 절댓값은 waitQueue에 들어있는 thread의 수)임과 동시에 thread의 실행 순서를 제어하는 역할을 하게 된다. consumer thread들은 매 번 실행될 때마다 semWait(n)을 하게 된다. 따라서 producer에서 semSignal(n)과 semSignal(s)가 서로 순서가 바뀌어 semSignal(n)이 critical section 밖에서 수행된다고 하더라도 동일하게 실행된다. 왜냐하면 어차피 consumer들은 semWait(n)을 통해 block된 상태이기에 semSignal(n)이 호출되어야 수행될 수 있기 때문이다.&lt;/p&gt;

&lt;h3 id=&quot;유한-buffer-사용&quot;&gt;유한 buffer 사용&lt;/h3&gt;

&lt;p&gt;위의 모든 solution은 무한한 buffer를 사용한다는 가정 하에서 이루어졌다. 하지만 실제로 무한한 buffer는 존재하지 않으므로 유한한 buffer를 사용하게 된다. 대개 circular queue를 사용하게 된다. pseudo code는 다음과 같다.&lt;/p&gt;

&lt;div class=&quot;language-c highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;n&quot;&gt;semaphore&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;               &lt;span class=&quot;c1&quot;&gt;//buffer의 접근을 제어하는 semaphore&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;semaphore&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;               &lt;span class=&quot;c1&quot;&gt;//buffer에 들어있는 자원의 개수를 제어하는 semaphore&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;semaphore&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;               &lt;span class=&quot;c1&quot;&gt;//유한 buffer를 관리하는 semaphore&lt;/span&gt;

&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;producer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
	&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;produce&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;       &lt;span class=&quot;c1&quot;&gt;//자원 생산&lt;/span&gt;
		&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;semWait&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;            &lt;span class=&quot;c1&quot;&gt;//buffer.full()==true일 경우 block&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;semWait&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
		&lt;span class=&quot;cm&quot;&gt;/*
			                     critical section start
		*/&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;           &lt;span class=&quot;c1&quot;&gt;//buffer에 push&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;semSignal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;          &lt;span class=&quot;c1&quot;&gt;//consumer 중 1개 block 해제&lt;/span&gt;
		&lt;span class=&quot;cm&quot;&gt;/*
		                       critical section end
		*/&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;semSignal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
		&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
	&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;err&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;consumer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
	&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;semWait&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;          &lt;span class=&quot;c1&quot;&gt;//buffer.empty()==true일 때 실행되는 것을 방지하기 위해 block&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;semWait&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
		&lt;span class=&quot;cm&quot;&gt;/*
		                     critical section start
		*/&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;take&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;        &lt;span class=&quot;c1&quot;&gt;//buffer에서 pop&lt;/span&gt;
		&lt;span class=&quot;cm&quot;&gt;/*
			                	critical section end
		*/&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;semSignal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
		&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;semSignal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;       &lt;span class=&quot;c1&quot;&gt;//buffer.full()==false이므로 block된 producer 중 1개 unblock&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;consume&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;       &lt;span class=&quot;c1&quot;&gt;//자원 소비&lt;/span&gt;
	&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;semInit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;semInit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;semInit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BUFFER_SIZE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_of_producers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;thread_start&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_of_consumers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;thread_start&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;err&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
</description>
        <pubDate>Mon, 23 Nov 2020 18:00:00 -0600</pubDate>
        <link>http://0.0.0.0:4000/operating%20system/Semaphore/</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/operating%20system/Semaphore/</guid>
        
        <category>Operating System</category>
        
        
        <category>Operating System</category>
        
      </item>
    
      <item>
        <title>[운영체제] Concurrency</title>
        <description>&lt;p&gt;숭실대학교 컴퓨터학부 홍지만 교수님의 2020-2학기 운영체제 강의를 정리 및 재구성했다.&lt;/p&gt;

&lt;h1 id=&quot;thread&quot;&gt;Thread&lt;/h1&gt;

&lt;h2 id=&quot;process&quot;&gt;Process&lt;/h2&gt;

&lt;p&gt;OS에서 process는 역할을 정리해보자. 우선 process는 자원 소유의 단위이다. 자원이라는 것은 main memory, I/O device, file system 등을 의미한다. 대표적인 예시로 process별로 main memory에 서로 다른 공간을 할당하는 것이 있다. 두번째로 process는 scheduling의 단위이다. context switching은 process 사이에 발생하면서 다음 실행될 process를 선택한다. 이러한 process의 2가지 역할은 서로 독립적이다. 따라서 os는 두 가지 역할을 모두 process라는 하나의 개념으로 수행하지 않고, 별개의 단위를 만들어냈다. 우선 자원 관리 역할은 process가 그대로 수행한다. 이 때의 process를 task라고 명명하기도 한다. 반면 scheduling의 단위는 thread 또는 경량(lightweight) process라고 새로 정의한다.&lt;/p&gt;

&lt;h2 id=&quot;multi-thread&quot;&gt;Multi-thread&lt;/h2&gt;

&lt;p&gt;os가 하나의 process 내에 여러 thread를 지원하는 것을 다중 쓰레딩(kernel-level multi thread)라고 한다. MS-DOS와 같은 단일 사용자 process의 경우에는 오직 하나의 process만 동시에 실행될 수 있으며, 해당 process 내에 하나의 thread만이 존재한다. 즉, thread라는 개념이 없는 것과 마찬가지이다. 초기의 UNIX와 같은 다중 사용자 process는 여러 process가 동시에 실행될 수 있지만, 각 process 내에 하나의 thread만이 존재한다. Windows, Mac OS, BSD와 같은 비교적 최신 운영체제는 모두 multi thread를 채택하고 있다. 여러 process가 동시에 실행될 수 있으면서, 각 process 내에 여러 thread가 함께 존재하는 것이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-11-17-Concurrency/01.png&quot; alt=&quot;01.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;다증 쓰레딩 환경에서 process는 자원 할당의 단위, 보호의 단위로써의 의미를 갖는다. process 별로 자원을 할당하고, 다른 process가 자신의 자원에 접근하지 못하도록 보호하는 것이다. 한편 dispatching(scheduling)의 단위는 process가 아닌 thread가 수행하게 된다. 각 thread는 context switching 수행을 위해 별개의 독립된 program counter를 보유한다. 또한 별개의 독립된 stack을 각자 보유한다. 반면 heap, data, bss, text와 같은 memory 영역은 process 내의 다른 thread들과 공유한다. 즉, process에게 할당된 stack memory 영역을 여러 thread들이 나누어 사용하고, 나머지 memory 영역은 process 단위로 공유하는 것이다. 따라서 기본적으로 memory 등의 모든 자원은 process 내의 모든 thread들이 공유한다고 볼 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-11-17-Concurrency/02.png&quot; alt=&quot;02.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그렇다면 multi-thread를 사용함으로써 얻는 이점은 어떤 것이 있을까? 우선 성능 향상에 큰 도움이 된다. process를 여러 thread 별로 할당해 I/O를 많이 하는 thread, CPU 연산을 많이 하는 thread를 분리한다면 I/O 때문에 대기하는 시간을 단일 process 방식보다 훨씬 줄일 수 있을 것이다. 또한 process를 생성하는 것에 비해 이미 존재하는 process 내에서 새로운 thread를 생성하는데 드는 비용이 더 적다는 장점도 있다. 이에 더해 context switching도 thread 간의 전환이 process 단위보다 더 빠르다. 마지막으로, process 간에는 자원을 공유할 수 없기 때문에 서로 통신하기 위해서는 kernel이 개입해야 하지만, thread는 kernel 호출 없이도 서로 원활하게 통신할 수 있다.&lt;/p&gt;

&lt;h2 id=&quot;processthread&quot;&gt;Process/Thread&lt;/h2&gt;

&lt;p&gt;process와 thread의 유사점/차이점을 정리해보자. 우선 context switching의 단위라는 점이 공통적이다. 또한 program counter를 보유한다는 점도 공통적이다. 반면 process는 PCB(process control block)으로 관리하지만, thread는 TCB(thread control block)으로 관리한다는 점이 차이가 있다. 또한 process는 다른 process의 자원(memory 공간 등)에 접근할 수 없는 반면, thread는 다른 thread의 자원에 접근 가능하다는 차이점이 있다. 마지막으로, process 단위의 context switching 후에는 memory 주소 공간이 달라지지만, thread 단위의 context switching 후에는 memory 주소 공간의 변화가 없다는 차이점이 있다.&lt;/p&gt;

&lt;h2 id=&quot;states-of-threads&quot;&gt;States of Threads&lt;/h2&gt;

&lt;p&gt;thread의 상태는 process의 상태와는 별개이다. process의 상태를 떠올려보면, suspend 상태가 존재했다. thread 상태에서는 suspend가 존재하지 않는다. suspend라는 것은 main memory가 아닌 disk의 swap 영역에 위치하는 상태인데, 이는 process 전체가 swap 영역으로 옮겨지는 것이기에 thread 단위의 작업이 아니다. 즉, process가 swap-out된다면 해당 process에 속한 모든 thread가 함께 swap-out되는 것이다. 즉, suspend는 thread의 state와는 아무런 연관이 없다. thread의 상태는 크게 다음의 4가지가 있다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;생성(spawn)&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;블록(block)&lt;/p&gt;

    &lt;p&gt;thread가 어떠한 사건을 기다리는 상태이다. 자신의 register, program counter, stack pointer를 저장한다. dispatcher는 같은 process나 다른 process 내의 다른 thread를 수행한다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;비블록(unblock)&lt;/p&gt;

    &lt;p&gt;사건이 발생해 thread가 준비 queue에 push되는 상태이다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;종료(finish)&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;ultuser-level-thread--kltkernel-level-thread&quot;&gt;ULT(User-level Thread) / KLT(Kernel-level Thread)&lt;/h2&gt;

&lt;p&gt;사용자는 kernel-level thread를 직접 제어하지 못한다. KLT는 오직 kernel만이 제어할 수 있는 thread이다. single-thread 운영체제일 경우에는 KLT가 구현되어 있지 않다. 사용자는 현재 환경이 KLT가 구현되어 있는지도 알지 못하고, KLT를 제어할 수도 없기 때문에 user-level thread를 사용하게 되는데, 대개 pthread와 같은 thread library를 활용한다. thread library는 실행 운영체제가 single-thread일 경우 여러 ULT를 하나의 process로 보내게 된다. 만약 KLT가 구현되어 있는 multi-thread 운영체제라면 알맞게 KLT와 mapping을 시켜 여러 process로 보내게 된다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-11-17-Concurrency/03.png&quot; alt=&quot;03.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ULT와 KLT를 비교해보자. 한 process 내의 thread 사이 dispatch를 수행할 때를 생각해보자. 우선 ULT는 각 thread가 모두 사용자 주소 공간에 위치하기 때문에 dispatch를 할 때에 kernel mode로 변경될 필요 없이 user mode에서 모두 수행 가능하다. 반면 KLT는 dispatch를 할 때마다 kernel mode로 변경되어야만 한다. 즉, mode 전환 여부에 있어서는 ULT가 KLT보다 유리하다. 또한, ULT는 운영 체제에 종속적이지 않고 어떠한 kernel의 변경 없이도 원활히 수행될 수 있는 반면, KLT는 운영 체제에 따라 존재하지 않을 수도 있다는 차이점도 있다. 한편, ULT의 경우에는 하나의 thread에서 system call을 호출할 경우 같은 process 내의 모든 thread들이 함께 block이 된다는 치명적인 단점이 있다. 즉, 순수한 ULT만으로는 다중 처리의 장점을 살리지 못하게 된다. 반면 KLT의 경우에는 한 thread가 block된다고 하더라도 다른 thread들은 자유롭다. 즉, 여러 dispatcher에 하나의 process에 속한 여러 thread를 동시에 scheduling이 가능하다. 진정한 의미의 다중 처리가 가능한 것이다.&lt;/p&gt;

&lt;h1 id=&quot;lock&quot;&gt;Lock&lt;/h1&gt;

&lt;p&gt;thread는 서로 memory를 공유한다. 따라서 공통으로 사용하는 공유 변수가 있다. 여러 thread가 모두 변수의 값을 read만 하는 경우에는 문제가 발생하지 않지만, 만약 특정 thread가 변수의 값을 write하게 된다면 동기화 문제가 발생한다. 이를 해결하기 위해서는 한 thread가 변수를 write할 때에는 다른 thread가 해당 변수에 접근할 수 없도록 lock을 걸어주는 과정이 필요하다. lock~unlock의 구간을 critical section(임계 영역)이라고 한다. critical section은 해당 영역 내의 다수의 명령어를 atomic(원자적)하게 실행되도록 보장한다. lock에 대한 정책은 모든 thread에게 공정해야만 한다. 구체적으로, 우선 모든 thread가 적절한 시간 내에 critical section에 들어갈 수 있어야만 하고, critical section에 들어가기 위해 대기 중인 모든 thread들의 요청은 언젠가는 허가가 되어야 한다. 마지막으로 starvation이 발생하지 않아야 한다. lock은 적용 범위에 따라 사용하는 일부분만 lock을 하는 coarse-grained lock, 사용하는 모든 영역을 lock하는 fine-grained lock으로 구분되기도 한다.&lt;/p&gt;

&lt;p&gt;lock에서 사용되는 용어들을 정리해보자.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;mutual exclusion (상호 배제)&lt;/p&gt;

    &lt;p&gt;critical section에는 어느 시점에서든 반드시 단 1개의 thread만 접근 가능해야 한다는 개념이다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;race condition (경쟁 조건)&lt;/p&gt;

    &lt;p&gt;다수의 thread가 공유 data를 read/write하는 상황이다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;busy waiting&lt;/p&gt;

    &lt;p&gt;모든 thread가 critical section 접근 조건을 만족하지 못해, 반복적으로 접근 조건만을 검사하며 함께 대기하는 상황이다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;deadlock (교착 상태)&lt;/p&gt;

    &lt;p&gt;다수의 thread가 다른 thread가 어떠한 일을 해 줄 때까지 대기하는 상태로, 모든 thread가 다른 thread의 변화를 기다리며 대기하는 상황이다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;livelock&lt;/p&gt;

    &lt;p&gt;다수의 thread가 단순히 자신의 상태를 변화시키는 작업만 반복적으로 수행하면서 대기하는 상황이다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;mutual-exclusion&quot;&gt;mutual exclusion&lt;/h2&gt;

&lt;p&gt;mutual exclusion을 위한 세부 요구 조건이 있는데, 우선 mutual exclusion은 선택 사항이 아니라 필수 사항이라는 점이다. 즉, mutual exclusion은 강제되어야만 한다. 두번째로, critical section의 밖에 있는 어떤 thread도 critical section 내의 thread에게 간섭해서는 안된다. 세번째로, deadlock 및 starvation이 발생하지 않아야 한다. 네번째로, critical section에 아무도 접근하지 않을 때에는 대기하던 thread 중 하나가 즉시 critical section에 접근할 수 있어야 한다. 마지막으로, 어떠한 thread도 critical section을 무한히 점유할 수는 없다.&lt;/p&gt;

&lt;p&gt;mutual exclusion을 구현하기 위한 여러 방법을 살펴보자. 단순화를 위해 binary mutual exclusion으로 가정한다. 즉, 2개의 process만이 존재하는 상황이다.&lt;/p&gt;

&lt;h3 id=&quot;turn-variable&quot;&gt;turn variable&lt;/h3&gt;

&lt;div class=&quot;language-c highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;turn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;// thread 0&lt;/span&gt;
&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;thread0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;turn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
		&lt;span class=&quot;cm&quot;&gt;/*
			critical section...
		*/&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;turn&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
	&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;// thread 1&lt;/span&gt;
&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;thread1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;!&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;turn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
		&lt;span class=&quot;cm&quot;&gt;/*
			critical section...
		*/&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;turn&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
	&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;turn이라는 boolean 변수를 활용해 turn==0일 때에는 thread0을, turn==1일 때에는 thread1을 실행한다.&lt;/p&gt;

&lt;p&gt;하지만 이러한 정책은 한 thread가 연속적으로 critical section을 점유하지 못한다는 문제점이 있다. 구체적인 예시로, thread0이 critical section을 점유한 뒤 나오게 되면, thread1이 critical section을 점유하기 전까지는 절대 critical section을 점유할 수 없다. 만약 thread1이 critical section에 접근할 필요가 없는 thread라면 thread0은 무한히 대기하게 될 것이다. 즉, deadlock이 발생한 것이다.&lt;/p&gt;

&lt;h3 id=&quot;flag&quot;&gt;flag&lt;/h3&gt;

&lt;div class=&quot;language-c highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;flag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;};&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;// thread 0&lt;/span&gt;
&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;thread0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]);&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;flag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
		&lt;span class=&quot;cm&quot;&gt;/*
			critical section...
		*/&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;flag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
	&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;// thread 1&lt;/span&gt;
&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;thread1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]);&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;flag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
		&lt;span class=&quot;cm&quot;&gt;/*
			critical section...
		*/&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;flag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
	&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;각 thread마다 하나의 boolean 변수를 사용해 자신이 critical section을 점유하고 있는지를 나타낸다. 따라서 turn variable 정책과는 달리 하나의 thread가 연속적으로 critical section을 점유할 수 있다. 상대방 thread가 critical section을 점유하고 있는지만 확인하기 때문이다. 이는 flag 변수의 개수를 늘리고 while문에서 확인할 flag 개수를 증가만 시킨다면 binary가 아닌 n개의 thread에 대해서도 적용 가능하도록 손쉽게 확장할 수 있다.&lt;/p&gt;

&lt;p&gt;하지만 이러한 정책은 busy waiting을 발생시킨다는 문제점이 있다. 만약 다른 thread의 flag를 검사하는while문과 자신의 flag를 1로 변화시키는 명령어 사이에 context switching이 발생하게 된다면 두 flag가 모두 1이 되어 두 thread 모두 critical section에 접근하게 된다. 이는 critical section의 정의에 부합하지 않는 상황이다. 이는 실제 lock을 수행하는 명령과 lock 수행을 돕는 변수의 값 변경 사이에 context switching이 발생한 것으로 lock step 사이에 scheduling이 발생했다고 볼 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;busy-waiting&quot;&gt;busy waiting&lt;/h3&gt;

&lt;div class=&quot;language-c highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;flag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;};&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;// thread 0&lt;/span&gt;
&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;thread0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;flag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]);&lt;/span&gt;
		&lt;span class=&quot;cm&quot;&gt;/*
			critical section...
		*/&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;flag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
	&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;// thread 1&lt;/span&gt;
&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;thread1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;flag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]);&lt;/span&gt;
		&lt;span class=&quot;cm&quot;&gt;/*
			critical section...
		*/&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;flag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
	&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;lock step 사이에 context switching이 발생하는 것을 막기 위해서 flag를 먼저 변경시킨 뒤 대기한다. 그러나 이 경우에도 flag의 변경과 while문 사이에 context switching이 발생하게 되면 모든 flag가 1이 되게 된다. 이러한 경우 모든 thread가 flag를 확인하는 while문을 무한히 수행하게 된다. busy waiting이 발생한 것이다.&lt;/p&gt;

&lt;h3 id=&quot;busy-flag-again&quot;&gt;busy flag again&lt;/h3&gt;

&lt;div class=&quot;language-c highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;flag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;};&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;// thread 0&lt;/span&gt;
&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;thread0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]){&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;flag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;delay&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;flag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
		&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
		&lt;span class=&quot;cm&quot;&gt;/*
			critical section...
		*/&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;flag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
	&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;// thread 1&lt;/span&gt;
&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;thread1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]){&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;flag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;delay&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;flag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
		&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
		&lt;span class=&quot;cm&quot;&gt;/*
			critical section...
		*/&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;flag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
	&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;다른 thread의 flag를 검사하는 while문 내에서 자신의 flag를 일정 시간 간격으로 toggle하는 것이다. 이러한 경우 delay 시각 내에 context switching이 발생하게 되면 다른 thread가 critical section에 접근할 수 있게 된다. 하지만 이 역시 두 thread의 delay가 동시에 발생하는 최악의 경우에는 livelock 상태에 빠지게 된다. 하지만 이는 현실에서는 발생하기 불가능에 가깝기 때문에 무시되고는 한다. SW 상으로 critical section 정책을 구현하는 것에 있어서는 위의 정책이 가장 최선이다.&lt;/p&gt;

&lt;h3 id=&quot;interrupt-disable&quot;&gt;interrupt disable&lt;/h3&gt;

&lt;div class=&quot;language-c highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;// thread 0&lt;/span&gt;
&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;thread0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;interrupt_disable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
		&lt;span class=&quot;cm&quot;&gt;/*
			critical section...
		*/&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;interrupt_enable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
	&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;// thread 1&lt;/span&gt;
&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;thread1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;interrupt_disable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
		&lt;span class=&quot;cm&quot;&gt;/*
			critical section...
		*/&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;interrupt_enable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
	&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;HW적으로 critical section 접근 이전에 interrupt를 disable한 뒤, critical section 이후에 interrupt를 enable함으로써 context switching이 발생하지 않도록 만드는 것이다. 그러나 real time OS에서 context switching을 금지한다는 것은 있을 수 없는 일이기에 현실성이 없다.&lt;/p&gt;

&lt;h3 id=&quot;atomic-instruction&quot;&gt;atomic instruction&lt;/h3&gt;

&lt;div class=&quot;language-c highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;flag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;};&lt;/span&gt;

&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;thread0_atomic_instruction&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]);&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;flag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;thread1_atomic_instruction&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]);&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;flag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;// thread 0&lt;/span&gt;
&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;thread0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;thread0_atomic_instruction&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
		&lt;span class=&quot;cm&quot;&gt;/*
			critical section...
		*/&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;flag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
	&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;// thread 1&lt;/span&gt;
&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;thread1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;thread1_atomic_instruction&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
		&lt;span class=&quot;cm&quot;&gt;/*
			critical section...
		*/&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;flag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
	&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;기존의 flag 정책에서 문제가 발생하는 상황은 다른 thread의 flag를 검사하는 while문과 자신 flag의 값을 변경하는 명령어 사이에 context switching이 발생하는 경우였다. 이를 막기 위해 HW 상에서 두 명령어를 묶어 atomic하게 실행되도록 하는 것이다. 이를 위해서 HW는 testset과 exchange라는 명령어를 제공하게 된다.&lt;/p&gt;

&lt;h1 id=&quot;deadlock&quot;&gt;Deadlock&lt;/h1&gt;

&lt;p&gt;deadlock은 어느 경우에 발생하는지, 어떻게 해결할 수 있는지에 대해 알아보자.&lt;/p&gt;

&lt;p&gt;deadlock은 아래의 4가지 조건이 모두 충족되었을 때 발생한다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;mutual exclusion (상호 배제)&lt;/p&gt;

    &lt;p&gt;critical section을 동시에 최대 1개의 thread만 점유할 수 있는 것이다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Hold-and-wait (점유 대기)&lt;/p&gt;

    &lt;p&gt;critical section을 점유할 수 없을 경우 critical section이 비워질 때까지 대기한다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Non preemption (비선점)&lt;/p&gt;

    &lt;p&gt;한 번 점유한 경우 다른 thread에 의해 강제로 점유를 뺏기지 않는다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Circular wait (환형 대기)&lt;/p&gt;

    &lt;p&gt;자원 할당 그래프 (Resource Allocation Graph)에서 cycle이 생성된 경우이다. 즉, 서로 다른 thread의 행동을 기다리면서 무한히 대기하는 상황이다.&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/assets/images/2020-11-17-Concurrency/04.png&quot; alt=&quot;04.png&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;1~3의 조건은 deadlock의 필요 조건이다. 즉, 조건 중 어느 하나라도 충족하지 않으면 deadlock은 발생하지 않는다. 하지만 1~3의 조건이 모두 충족되었다고 해서 무조건 deadlock이 발생하는 것은 아니다. 1~4의 조건이 모두 만족해야만 deadlock이 발생한다. 즉 1~4의 조건은 deadlock의 필요충분 조건이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-11-17-Concurrency/05.png&quot; alt=&quot;05.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위는 Process P와 Q가 자원 A와 B를 경쟁적으로 사용하는 상황에서의 deadlock 발생 가능성을 나타낸 것이다. 총 6개의 시나리오에 대해서 살펴보자.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Q가 B를 획득하고, A를 획득한다. Q는 모든 작업을 수행하고 B와 A를 순서대로 release한다. 이후에 P로 전환되어 자유롭게 실행된다.&lt;/li&gt;
  &lt;li&gt;Q가 B를 획득하고, A를 획득한다. P로 전환되지만 A를 획득할 수 없어 block된다. Q가 마저 실행되고 B와 A를 release한 뒤에 P가 실행된다.&lt;/li&gt;
  &lt;li&gt;Q가 B를 획득한 뒤 P로 전환되어 P가 A를 획득한다. 이후에 Q로 전환될 경우 Q가 A를 획득할 수 없어 block되고, P가 계속 실행될 경우 B를 획득할 수 없어 block된다. &lt;strong&gt;deadlock&lt;/strong&gt;이다.&lt;/li&gt;
  &lt;li&gt;P가 A를 획득한 뒤 Q로 전환되어 Q가 B를 획득한다. 이후에 P로 전환될 경우 P가 B를 획득할 수 없어 block되고, Q가 계속 실행될 경우 A를 획득할 수 없어 block된다. &lt;strong&gt;deadlock&lt;/strong&gt;이다.&lt;/li&gt;
  &lt;li&gt;P가 A를 획득하고, B를 획득한다. Q로 전환되지만 B를 획득할 수 없어 block된다. P가 마저 실행되고 A와 B를 release한 뒤에 Q가 실행된다.&lt;/li&gt;
  &lt;li&gt;P가 A를 획득하고, B를 획득한다. P는 모든 작업을 수행하고 A와 B를 순서대로 release한다. 이후에 Q로 전환되어 자유롭게 실행된다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-11-17-Concurrency/06.png&quot; alt=&quot;06.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위와 같이 한 process가 두 자원은 동시에 점유하지 않을 경우에는 deadlock이 발생하지 않게 된다. 위의 deadlock 발생 상황과의 차이점은 한 process가 동시에 두 자원을 점유하지 않는다는 것이다.&lt;/p&gt;

&lt;h2 id=&quot;deadlock-prevention&quot;&gt;Deadlock Prevention&lt;/h2&gt;

&lt;p&gt;deadlock을 해결하는 방법은 크게 2가지가 있다. deadlock이 발생할 가능성이 생기면 이를 예방하는 것이 그 중 하나이다. deadlock의 필요 조건(상호 배제, 점유 대기, 비선점)은 고려하지 않고 circular wait이 발생하지 않도록만 하는 것이다. 하지만 process가 사용할 모든 자원을 미리 알고 있어야 circular wait이 발생하는지를 예측할 수 있기 때문에 현실적으로 구현이 불가능에 가깝다. deadlock prevention의 방법으로는 process 시작 거부와 자원 할당 거부가 있다.&lt;/p&gt;

&lt;h3 id=&quot;process-시작-거부&quot;&gt;Process 시작 거부&lt;/h3&gt;

&lt;p&gt;자원에 대한 vector와 matrix를 정의해 계산하고, 이를 이용해 deadlock 발생을 예측해 회피한다. OS는 process 수행 이전에 아래의 정보들을 모두 알고 있어야만 한다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;자원: system에 존재하는 자원의 전체 개수&lt;/p&gt;

\[R = (R_1,R_2, ...,R_m)\]
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;가용: system에 존재하는 자원 중 현재 사용 가능한 자원의 개수&lt;/p&gt;

\[V=(V_1,V_2,...,V_m)\]
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;요청: process가 요청하고 있는 자원의 개수&lt;/p&gt;

\[C=\begin{pmatrix}C_{11}&amp;amp;...&amp;amp;C_{1m}\\&amp;amp;...&amp;amp;\\C_{n1}&amp;amp;...&amp;amp;C_{nm}\end{pmatrix}\]

    &lt;p&gt;\(C_{ij}\): process \(i\)가 자원 \(j\)를 \(C_{ij}\)만큼 요청&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;할당: process가 할당받고 있는 자원의 개수&lt;/p&gt;

\[A=\begin{pmatrix}A_{11}&amp;amp;...&amp;amp;A_{1m}\\&amp;amp;...&amp;amp;\\A_{n1}&amp;amp;...&amp;amp;A_{nm}\end{pmatrix}\]

    &lt;p&gt;\(A_{ij}\): process \(i\)가 자원 \(j\)를 \(A_{ij}\)만큼 할당받음&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;위의 vector와 matrix는 정의에 따라 아래와 같은 수식들이 성립된다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;전체 자원의 개수는 가용 가능한 자원과 전체 process들에게서 사용중인 자원의 합이다.&lt;/p&gt;

\[R_j=V_j+\sum_{j=1}^m{A_{ij}}\]
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;요청 자원은 전체 자원의 양보다 많을 수 없다.&lt;/p&gt;

\[C_{ij}\le R_j\]
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;할당 자원은 요청 자원보다 많을 수 없다.&lt;/p&gt;

\[A_{ij}\le C_{ij}\]
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;process 시작 거부 방식은 모든 자원들에 대해 아래의 수식을 만족할 때에만 해당 process를 시작한다.&lt;/p&gt;

\[R_j\ge C_{(n+1)j}+\sum_{i=1}^n{C_{ij}}\ \ \ for\ all\ j\]

&lt;p&gt;모든 자원들에 대해 process가 요청하는 전체 자원의 합과 새로운 process가 요청하는 자원을 더한 값이 실제 자원의 양보다 작을 때에만 process를 시작하는 것이다. 이는 최악의 경우에도 실행됨을 보장하기 위함이다. 최악의 경우라는 것은 모든 process들이 동시에 자신이 요청할 수 있는 최대 자원량을 한꺼번에 요청하는 상황을 뜻한다. 이러한 보수적인 조건을 만족했을 때에만 process가 실행되는 것이기에 현실에서 사용할 수 없는 방식이다.&lt;/p&gt;

&lt;h3 id=&quot;자원-할당-거부-은행원-algorithm&quot;&gt;자원 할당 거부 (은행원 algorithm)&lt;/h3&gt;

&lt;p&gt;자원 할당 거부를 통한 deadlock prevention는 은행원 algorithm을 사용한다. 은행원 algorithm이란 system의 상태를 safe state와 unsafe state로 구분한다. safe state란 deadlock이 발생하지 않도록 process에게 자원을 할당할 수 있는 경로가 존재하는 상태를 의미하고, unsafe state란 해당 경로가 존재하지 않는 상태를 말한다. 은행원 algorithm은 safe state를 유지할 수 있는 thread의 요청에 대해서만 수락해 자원을 할당해주고, unsafe state가 되는 thread의 요청에 대해서는 계속 거절한다.&lt;/p&gt;

&lt;p&gt;다음은 safe state가 계속되어 정상적으로 모든 process가 실행되는 경우 대한 예시이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-11-17-Concurrency/07.png&quot; alt=&quot;07.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;\(C-A\)는 추가적으로 할당해야 할 자원들의 matrix이다. \(V\)와 비교해 더 작은 값들을 갖는 \(C-A\)의 row를 찾은 뒤 해당 process를 실행시키게 된다. 이후 \(A\)에서 해당 process의 값들이 \(V\)에 더해지게 된다. 해당 process의 값들은 \(C\)와 \(A\), \(C-A\)에서 모두 0이 된다.&lt;/p&gt;

&lt;p&gt;아래는 unsafe state에 대한 예시이다. 실행할 수 있는 process가 없는 경우이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-11-17-Concurrency/08.png&quot; alt=&quot;08.png&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;deadlock-detection&quot;&gt;Deadlock Detection&lt;/h2&gt;

&lt;p&gt;deadlock detection은 deadlock prevention에 비해 상대적으로 낙관적인 방법이다. process의 시작이나 자원 접근에 대해 제약을 가하지 않고, 요청이 들어오면 항상 할당을 한다. 대신 주기적으로 system에서 deadlock이 발생했는지를 검사하고 발생했을 경우 이를 해결하게 된다.&lt;/p&gt;

&lt;h3 id=&quot;deadlock-detection-1&quot;&gt;Deadlock Detection&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-11-17-Concurrency/09.png&quot; alt=&quot;09.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;deadlock prevention과 비슷하게 동작한다. algorithm은 다음과 같다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;임시 vector \(W\)를 생성해 초기 값으로 \(V\)를 복사한다.&lt;/li&gt;
  &lt;li&gt;\(Q\)의 row를 탐색하며 모든 자원이 \(W\)보다 작은 process가 있을 경우 해당 process를 mark한다. 그러한 process가 없을 경우 deadlock이 발생한 것이므로 algorithm을 종료한다.&lt;/li&gt;
  &lt;li&gt;process를 찾았을 경우 \(W\)에 \(A\)에서의 process의 값을 더한다. 2단계로 돌아가 다시 수행한다.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;deadlock-solution&quot;&gt;Deadlock Solution&lt;/h3&gt;

&lt;p&gt;여러 deadlock solution이 있지만, 그 중에서 대표적인 solution들을 살펴본다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;deadlock에 연관된 모든 process 중지&lt;/p&gt;

    &lt;p&gt;실제 많은 OS에서 채택하고 있는 방식이다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;deadlock에 연관된 모든 process roll-back 후 재수행&lt;/p&gt;

    &lt;p&gt;특정 checkpoint까지 roll-back 후 재수행하는 방식이나, 어떤 process가 먼저 수행될 지는 nondeterministic하기 때문에 deadlock이 재발생할 수도 있다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;deadlock이 해소될 때까지 연관된 process를 하나씩 종료&lt;/p&gt;

    &lt;p&gt;비용이 가장 적은 것, 지금까지 사용한 dispatcher 시간이 적은 것, 지금까지 생산한 출력량이 적은 것, 이후 남은 수행 시간이 가장 긴 것, 할당받은 자원이 가장 적은 것, 우선 순위가 낮은 것부터 종료시킨다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;deadlock이 해소될 때까지 연관된 자원들을 하나씩 preemption&lt;/p&gt;

    &lt;p&gt;가장 비용이 적은 자원부터 하나씩 preemption한 후 deadlock detection algorithm을 수행해 deadlock 존재 여부를 파악한다. 자원을 preemption당한 process는 해당 자원을 할당 받기 전으로 roll-back된다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;dinning-philosopher-problem&quot;&gt;Dinning Philosopher Problem&lt;/h2&gt;

&lt;p&gt;식사하는 철학자 problem은 deadlock의 대표적인 예시이다. 여러 철학자가 원탁 테이블에 앉아 식사를 하는데, 철학자가 왼쪽의 포크를 먼저 집은 뒤, 오른쪽에 있는 포크를 집어 식사를 한다. 식사를 마치면 두 포크를 테이블에 내려놓는다. 철학자들은 포크 2개를 모두 가진 상태에서만 식사를 할 수 있다. 이 때 포크가 철학자의 인원수와 동일하게 배치가 되어있다고 하면 deadlock이 발생할 것이다. pseudo code는 다음과 같다.&lt;/p&gt;

&lt;div class=&quot;language-c highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;cp&quot;&gt;#define N 5       //number of philosopher
&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;semaphore&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fork&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;

&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;philosopher&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;think&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;semWait&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fork&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]);&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;semWait&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fork&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mod&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;eat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;semSignal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fork&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mod&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]);&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;semSignal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fork&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]);&lt;/span&gt;
	&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;//semaphore 모두 로 초기화&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;semInit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fork&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

	&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;philosopher&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

	&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;solution은 동시에 테이블에 앉을 수 있는 최대 인원수를 N-1로 제한하는 것이다. 이 때 semaphore를 사용한다.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://cpm0722.github.io/operating%20system/Semaphore/&quot;&gt;Semaphore&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;pseudo code는 다음과 같다.&lt;/p&gt;

&lt;div class=&quot;language-c highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;cp&quot;&gt;#define N 5       //number of philosopher
&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;semaphore&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fork&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;semaphore&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;table&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;philosopher&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;think&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
		&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;semWait&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;table&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;semWait&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fork&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]);&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;semWait&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fork&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mod&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;eat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;semSignal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fork&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mod&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]);&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;semSignal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fork&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]);&lt;/span&gt;
		&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;semSignal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;table&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;
	&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;//semaphore 모두 로 초기화&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;semInit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fork&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

	&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;semInit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;table&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;

	&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;philosopher&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

	&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
</description>
        <pubDate>Mon, 16 Nov 2020 18:00:00 -0600</pubDate>
        <link>http://0.0.0.0:4000/operating%20system/Concurrency/</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/operating%20system/Concurrency/</guid>
        
        <category>Operating System</category>
        
        
        <category>Operating System</category>
        
      </item>
    
      <item>
        <title>[NLP 논문 리뷰] KR-BERT: A Small Scale Korean Specific Language Model</title>
        <description>&lt;h2 id=&quot;paper-info&quot;&gt;Paper Info&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2008.03979&quot;&gt;Archive Link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2008.03979.pdf&quot;&gt;Paper Link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Submit Date: Aug 10, 2020&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;기존의 BERT model은 104개의 language의 Wikipedia dataset으로 학습된 model이다. 범용적으로 사용될 수 있다는 장점에도 불구하고, model의 크기가 과도하게 크다는 단점이 존재한다. 또한 non-English downstream task에서 좋은 성능을 보여주지 못하는 경우가 많다는 한계도 명확하다. 특히나 Korean과 같은 언어에서는 한계가 두드러진다.&lt;/p&gt;

&lt;p&gt;Korean NLP task를 해결하기 위한 BERT model은 다음과 같은 이유들로 인해 많은 어려움이 있다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Korean이 교착어라는 특성으로 인해 과도하게 많은 형태소&lt;/li&gt;
  &lt;li&gt;Hangul의 과도하게 많은 character (10,000개 이상)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;본 논문에서는 위와 같은 Korean의 한계점에도 불구하고 Korean-specific한 BERT model을 고안해냈다. 우선 Multilingual BERT model에 비해 model의 size를 과감히 줄이고, sub-characters BPE를 사용했다. 또한 Bidirectional WordPiece Tokenizer를 사용해 Korean의 linguistic한 특성을 반영하고자 했다. KR-BERT model은 다른 Multilingual BERT Model의 성능을 모든 task에서 능가했고, 이에 더해 KorBERT나 KoBERT와 같은 기존의 Korean-specific model과도 동등하거나 더 좋은 성능을 보였다. 이는 KR-BERT의 작은 model 크기를 고려하면 매우 유의미한 결과이다.&lt;/p&gt;

&lt;h1 id=&quot;related-work&quot;&gt;Related Work&lt;/h1&gt;

&lt;h2 id=&quot;models-after-bert&quot;&gt;Models after BERT&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-11-13-KR-BERT-A-Small-Scale-Korean-Specific-Language-Model/01.jpg&quot; alt=&quot;01.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;BERT 이후로 XLNet과 RoBERTa와 같은 대규모 dataset을 사용한 model들이 많이 등장했다. 그에 비해 DistilBERT나 ALBERT와 같이 #parameters를 줄이고, dataset도 늘리지 않은 small model들도 등장했다.&lt;/p&gt;

&lt;h2 id=&quot;recent-korean-bert-models&quot;&gt;Recent Korean BERT models&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-11-13-KR-BERT-A-Small-Scale-Korean-Specific-Language-Model/02.jpg&quot; alt=&quot;02.jpg&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;the-need-for-a-small-scale-language-specific-model&quot;&gt;The Need for a Small-scale Language-specific Model&lt;/h1&gt;

&lt;p&gt;Korean NLP task에서 multilingual BERT model은 아래와 같은 한계를 지닌다.&lt;/p&gt;

&lt;h2 id=&quot;limit-of-corpus-domain&quot;&gt;Limit of Corpus Domain&lt;/h2&gt;

&lt;p&gt;multilingual BERT는 104개의 language로 구성된 Wikipedia data로 pretrain된 model이다. German이나 French와 같은 data가 풍부한 language에 대해서는 Wikipedia에 더해 추가적인 dataset을 활용해 pretrain을 수행했다. 그러나 그 외 대부분의 language는 Wikipedia dataset만으로 pretrain되었다. Wikipedia dataset은 다양한 corpus를 포함하고 있지 않기에 제대로 된 학습을 기대하기 어렵다.&lt;/p&gt;

&lt;h2 id=&quot;considering-language-specific-properties&quot;&gt;Considering Language-specific Properties&lt;/h2&gt;

&lt;h3 id=&quot;rare-character-problem&quot;&gt;Rare “Character” Problem&lt;/h3&gt;

&lt;p&gt;English와 같은 Alphabet을 사용하는 language는 OOV가 적을 수 밖에 없다. 전체 character가 26개에 불과하기 때문이다. 반면 Korean은 syllable 기반이기 때문에 무려 11,172개의 character가 존재한다. 그러나 multilingual BERT에서는 이 중 오직 1,187개의 character만이 포함되었다. 나머지 character에 대해서는 제대로 학습이 되었다고 볼 수 없는 것이다.&lt;/p&gt;

&lt;h3 id=&quot;inadequacy-for-morphologically-rich-languages&quot;&gt;Inadequacy for Morphologically Rich Languages&lt;/h3&gt;

&lt;p&gt;Korean은 교착어이다. 때문에 English와 같은 language보다 훨씬 많은 형태소를 가짐은 물론 French나 German과 같은 굴절어 보다도 더 많은 형태소를 갖는다. 대표적인 교착어인 Japanese나 Korean은 동사의 활용형만 하더라도 수많은 다른 형태를 갖는다.&lt;/p&gt;

&lt;h3 id=&quot;lack-of-meaningful-tokens&quot;&gt;Lack of Meaningful Tokens&lt;/h3&gt;

&lt;p&gt;character-level의 Korean은 음절 단위인데, 각 음절의 구분은 발음에서의 가치만 있을 뿐 의미론적으로 큰 가치가 없는 구분이다. 오히려 자음/모음 (문자소) 단위가 의미를 갖는 경우가 더 많다. multilingual BERT는 모든 language에 universal하게 적용되는 model을 위해 character-level로 설계가 되었기 때문에 Korean NLP task에 적합하지 않다.&lt;/p&gt;

&lt;h2 id=&quot;large-scale-of-the-model&quot;&gt;Large Scale of the Model&lt;/h2&gt;

&lt;p&gt;XLNet이나 RoBERTa와 같은 대규모 model은 매우 많은 parameters와 큰 dataset, 큰 vocabulary를 사용했다. 그러나 이러한 대규모 model은 자원의 제약이 너무 많이 가해지기 때문에 작은 vocabulary, 적은 parameters, 적은 training dataset으로도 좋은 성능을 보이는 것을 목표로 했다.&lt;/p&gt;

&lt;h1 id=&quot;models&quot;&gt;Models&lt;/h1&gt;

&lt;p&gt;총 4가지 version의 KR-BERT에 대해 제시하고 비교한다. 우선 가장 작은 의미의 단위를 character-level(음절 단위)과 sub-character-level(자음/모음 단위)로 구분한다. 각각의 경우에 대해 BERT의  Original Tokenizer(WordPiece)를 사용한 것과 Bidirectional WordPiece Tokenizer를 사용한 것을 비교한다.&lt;/p&gt;

&lt;h2 id=&quot;subcharacter-text-representation&quot;&gt;Subcharacter Text Representation&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-11-13-KR-BERT-A-Small-Scale-Korean-Specific-Language-Model/03.jpg&quot; alt=&quot;03.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;자음/모음 단위 구분을 통해 얻을 수 있는 이점은 동사나 형용사에 붙는 활용형을 정확하게 잡아낼 수 있다는 것이다. Table 3의 “갔”, “감”, “간”, “갈”은 모두 “가다”의 “가”에 여러 활용형이 붙은 경우이다. 하지만 이를 character-level로 분석하게 되면 모두 별개의 token이 된다. sub-character level로 분석을 함으로써 실제 “가다”의 의미를 파악해 낼 수 있는 것이다.&lt;/p&gt;

&lt;h2 id=&quot;subword-vocabulary&quot;&gt;Subword Vocabulary&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-11-13-KR-BERT-A-Small-Scale-Korean-Specific-Language-Model/04.jpg&quot; alt=&quot;04.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;BPE의 성능은 vocabulary size에 따라 결정된다. 이는 heuristic하게 결정해야 하는데, 8000~20000 사이의 vocabulary size에 대해 test를 진행한 뒤 100,000 step에서의 Masked LM Accuracy를 비교한 결과 vocabulary size가 10,000일 때에 가장 성능이 좋다는 결론을 도출해냈다.&lt;/p&gt;

&lt;p&gt;이후 Korean text에서 빈번하게 사용되는 외국어(Alphabet, 한자, 일본어 등)에 대해 heuristic하게 token을 추가했다.&lt;/p&gt;

&lt;p&gt;[Table 4]에서 볼 수 있듯이 KR-BERT는 character-level과 sub-character-level 모두에 있어서 Multilingual BERT나 KorBERT보다 훨씬 작은 크기의 vocabulary를 사용했다.&lt;/p&gt;

&lt;h3 id=&quot;subword-tokenization&quot;&gt;Subword Tokenization&lt;/h3&gt;

&lt;p&gt;기존의 WordPiece Tokenization과 본 논문에서 새로 제안한 Bidirectional WordPiece Tokenization을 모두 사용해 둘을 비교한다.&lt;/p&gt;

&lt;h3 id=&quot;baselines&quot;&gt;Baselines&lt;/h3&gt;

&lt;p&gt;Multilingual BERT나 KorBERT는 BPE를 사용한 WordPiece Tokenization를 채택했다. 반면 KoBERT는 Unigram LM을 사용한 SentencePiece Tokenization을 채택했다.&lt;/p&gt;

&lt;h3 id=&quot;bidirectional-wordpiece-tokenizer&quot;&gt;Bidirectional WordPiece Tokenizer&lt;/h3&gt;

&lt;p&gt;BPE를 forward로만 진행하지 않고, backward로도 동시에 진행하는 것이다. forward와 backward 각각의 pair를 생성한 뒤, 두 후보 중 더 등장 빈도가 높은 쪽을 선택하게 된다. 이는 한국어의 문법적 특성에 따라 고안된 방식이다. 한국어의 명사는 상대적으로 긴 어근을 갖고 주로 짧은 접두사들이 앞에 붙게 된다. 반면 동사의 경우에는 짧은 어근을 갖고 주로 짧은 접미사들이 뒤에 붙게 된다. Bidirectional BPE는 이러한 경우들에 대해 적절한 tokenizing을 수행할 수 있도록 돕는다.&lt;/p&gt;

&lt;h2 id=&quot;comparison-with-other-korean-models&quot;&gt;Comparison with Other Korean Models&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-11-13-KR-BERT-A-Small-Scale-Korean-Specific-Language-Model/05.jpg&quot; alt=&quot;05.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-11-13-KR-BERT-A-Small-Scale-Korean-Specific-Language-Model/06.jpg&quot; alt=&quot;06.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;[Table 4]를 보면 KR-BERT는 Multilingual BERT, KorBERT에 비해 더 적은 vocabulary, 더 적은 parameter, 더 적은 data size를 갖는다는 것을 확인할 수 있다. 반면 KoBERT에 비해서는 더 많은 vocabulary, 더 많은 parameter를 갖지만 dataset은 더 적다.&lt;/p&gt;

&lt;p&gt;[Table 5]는 각 model들의 vocabulary이 어떤 비율로 구성되어 있는지를 보여준다. Korean Specific한 model들이 Multilingual BERT보다 Korean words와 Korean subwords의 비율이 압도적으로 높다는 것을 확인할 수 있다.&lt;/p&gt;

&lt;p&gt;[Table 6]은 실제로 Tokenization이 어떻게 이루어지는지 구체적인 단어 예시를 통해 보여준다. “냉장고”는 Multilingual BERT와 KorBERT, KoBERT에서 모두  “냉”, “장”, “고”로 tokenizing된다. 반면 KR-BERT에서는 token level과 tokenizer에 관계없이 모든 model에 있어서 “냉장고”라는 하나의 token으로 분류한다. “냉장고”를 각 character 별로 단순하게 tokenizing한 것에 비해 의미론적으로 더 알맞게 tokenization이 된  것이다.&lt;/p&gt;

&lt;p&gt;“춥다”는 Multilingual BERT에서는 아예 OOV로 판별이 된다. KorBERT와 KoBERT에서는 모두 “춥”, “다”로 tokenizing하게 된다. 그러나 KR-BERT에서는 character level은 “춥”, “다”로 다른 Korean Specific Model과 동일하게 tokenizing을 하지만, sub-character level에서는 “추”, “ㅂ다”로 tokenizing을 한다. sub-character level의 tokenizing이 더 적절한 결과를 도출해낸다는 것을 확인할 수 있다.&lt;/p&gt;

&lt;p&gt;“뱃사람”은 Multilingual BERT에서는 OOV이고, KorBERT와 KoBERT에서는 “뱃”, “사람”으로 tokenizing된다. character level의 KR-BERT에서도 마찬가지의 결과를 보여준다. 반면 sub-character level KR-BERT는 “배”, “ㅅ”, “사람”으로 tokenizing을 한다. Korean의 문법적 특성인 ‘사이시옷’까지 잡아낸 것이다.&lt;/p&gt;

&lt;p&gt;“마이크”는 Multilingual BERT와 KoBERT에서는 “마”, “이”, “크”로, KorBERT에서는 “마이”, “크”로 tokenizing된다. 반면 KR-BERT에서는 모든 model에서 동일하게 “마이크”로 tokenizing한다. 외래어 표기에 있어서 기존 model에 비해 더 강력한 성능을 보여주는 것이다.&lt;/p&gt;

&lt;h1 id=&quot;experiments-and-results&quot;&gt;Experiments and Results&lt;/h1&gt;

&lt;p&gt;여러 Korean NLP downstream task에 대해서 Multilingual BERT와 기존의 Korean Specific Model, KR-BERT를 비교한다. sentiment classification, question answering, named entity recognition, paraphrase detection에 대해서 실험을 진행했다.&lt;/p&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;h3 id=&quot;masked-lm-accuracy&quot;&gt;Masked LM Accuracy&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-11-13-KR-BERT-A-Small-Scale-Korean-Specific-Language-Model/07.jpg&quot; alt=&quot;07.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;KR-BERT의 모든 model이 KoBERT보다 더 좋은 MLM Accuracy를 보여준다. 또한 KR-BERT 내에서 Bidirectional WordPiece를 사용한 model이 조금 더 나은 결과를 보여준다.&lt;/p&gt;

&lt;h3 id=&quot;downstream-tasks&quot;&gt;Downstream tasks&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-11-13-KR-BERT-A-Small-Scale-Korean-Specific-Language-Model/08.jpg&quot; alt=&quot;08.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;sentiment classification은 Naver Sentiment Movie Corpus Dataset을, question answering은 KorQuAd Dataset을, named entity recognition는 KorNER Dataset을, paraphrase detection은 Korean Paired Question Dataset을 사용했다.&lt;/p&gt;

&lt;p&gt;모든 경우에 있어서 Multilingual BERT는 Korean Specific Model의 최고 성능을 능가하지 못했다. KR-BERT는 KorQuAD와 KorNER에서 가장 좋은 성능을 보여준다. 반면 NSMC와 Paraphrase Detection에 있어서는 KorBERT가 근소하게 더 높은 수치를 보여준다. 하지만 그럼에도 불구하고 KorQuAD와 KorNER에서의 KorBERT와 KR-BERT의 차이는 7%로 매우 높다는 점, KorBERT의 model size와 풍부한 dataset을 고려한다면 매우 유의미한 결과이다.&lt;/p&gt;

&lt;h2 id=&quot;analysis-of-downstream-tasks&quot;&gt;Analysis of Downstream Tasks&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-11-13-KR-BERT-A-Small-Scale-Korean-Specific-Language-Model/09.jpg&quot; alt=&quot;09.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-11-13-KR-BERT-A-Small-Scale-Korean-Specific-Language-Model/10.jpg&quot; alt=&quot;10.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;사실 KR-BERT model 중에서 sub-character Bidirectional WordPiece model이 일관되게 최고의 성능을 보여주지는 못한다. 하지만 그럼에도 다른 model들에 비해 일관되게 좋은 성능을 유지한다는 점에서 긍정적이다.&lt;/p&gt;

&lt;p&gt;NSMC의 경우에는 웹사이트 사용자들의 data라는 점에서 noise나 문법적 오류가 상대적으로 많고, unformal한 data이다. NER은 task의 특성 상 당연하게도 고유 명사가 많으므로 OOV의 비율이 높을 것이다. 또한 KorQuAD와 Paraphrase Detection은 상대적으로 formal한 data일 것이다.&lt;/p&gt;

&lt;p&gt;[Table 9]를 보면 bidirectional 방식과 sub-character level이 문법적 오류를 더 정확하게 잡아낸다는 점을 확인할 수 있다. “이영화”는 사실 “이”, “영화”의 두 단어로 구분되어야 하지만 중간의 공백이 삽입되지 않은 경우이다. 이에 대해 Bidirectional WordPiece KR-BERT만이 “이”, “영화”로 정확하게 tokenizing을 수행한다. Bidirectional이 아닌 KR-BERT는 “이영”, “화”로 잘못된 tokenizing을 수행했다.&lt;/p&gt;

&lt;p&gt;“재밌는뎅”의 경우에는 “재밌는데”에 “ㅇ”라는 nosie가 추가된 경우이다. 이는 sub-character level KR-BERT가 정확하게 잡아내는데, “재미”, “ㅆ”, “는데”, “ㅇ”로 tokenizing을 수행한다. 반면 character-level KR-BERT는 “재”, “밌”, “는”, “뎅”으로 잘못된 tokenizing을 수행한다.&lt;/p&gt;

&lt;p&gt;NER과 같은 OOV 비율이 높은 task에 대해서는 sub-character level이 더 좋은 성능을 보여준다. 이는 [Table 10]에서 OOV rate를 확인했을 때 sub-character level이 character level 대비 OOV가 훨씬 낮다는 점을 보면 당연한 결과이다.&lt;/p&gt;

&lt;p&gt;KorQuAD나 Paraphrase Detection과 같은 formal data의 경우에는 WordPiece가 Bidirectional WordPiece보다 더 좋은 성능을 보여준다.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;Korean-specific BERT model인 KR-BERT model을 제안했다. 기존의 Korean-specific model에 비해 더 작은 규모에서 더 적은 dataset으로 동등하거나 더 좋은 성능을 보여줬다. 이 과정에서 sub-character level tokenizing, Bidirectional BPE를 사용해 Korean의 문법적 특성을 잡아냈다.&lt;/p&gt;
</description>
        <pubDate>Thu, 12 Nov 2020 18:00:00 -0600</pubDate>
        <link>http://0.0.0.0:4000/machine%20learning/paper%20review/KR-BERT-A-Small-Scale-Korean-Specific-Language-Model/</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/machine%20learning/paper%20review/KR-BERT-A-Small-Scale-Korean-Specific-Language-Model/</guid>
        
        <category>NLP</category>
        
        <category>Korean</category>
        
        
        <category>Machine Learning</category>
        
        <category>Paper Review</category>
        
      </item>
    
      <item>
        <title>[운영체제] Paging Mechanism</title>
        <description>&lt;p&gt;숭실대학교 컴퓨터학부 홍지만 교수님의 2020-2학기 운영체제 강의를 정리 및 재구성했다.&lt;/p&gt;

&lt;h1 id=&quot;paging-mechanism&quot;&gt;Paging Mechanism&lt;/h1&gt;

&lt;p&gt;paging 기법에 대해 자세히 알아보자. 위에서 살펴본 고정 분할 및 가변 분할 기법은 각각 내부 단편화, 외부 단편화의 문제점이 존재했다. paging은 이러한 단점들을 해결하기 위해 고안된 방식이다. paging을 사용하면 결론적으로 외부 단편화는 발생하지 않으며, 내부 단편화는 아주 적은 횟수 (대개 process 당 1회) 발생하게 된다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-10-20-Paging-Mechanism/01.png&quot; alt=&quot;01.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;paging이란 memory 가상화에서 가상 주소와 물리 주소를 mapping시킬 때에 page frame을 단위로 하는 방식이다. page table에는 가상 주소, 물리 주소 뿐만 아니라 P, M, U bit 등의 control bit도 담겨져 있다. P(present) bit는 해당 page가 memory에 위치하는가에 대한 bit로, 1일 경우 가리키는 물리 주소가 물리 memory 영역이라는 의미이고 0일 경우에는 가리키는 물리 주소가 memory가 아닌 disk의 swap 영역이라는 뜻이다. 즉, P bit가 0일 경우에는 swap 영역에 있는 page를 memory로 불러와야 한다. 이러한 과정을 &lt;strong&gt;page fault&lt;/strong&gt;라고 한다. page fault는 결국 disk I/O를 호출하는 것이기에 schedule() 함수를 호출한다. 한편 M(modify) bit는 해당 page가 수정된 적이 있는지에 대한 bit이고, W(write) bit, D(dirty) bit라고도 불린다. U(used) bit는 해당 page를 read한 적이 있는지에 대한 bit로, R(read) bit라고도 불린다. page table은 OS가 각각의 process에게 개별적으로 부여하게 되며,  task_struct와 같은 PCB들은 멤버 변수로 page table을 가리키는 포인터 값을 저장한다. 한편, 대부분의 가상 memory 기법은 page table을 실제 memory가 아닌 가상 memory에 저장하게 된다. process가 running 상태라면, 최소한 해당 process의 page table 중 일부분은 memory에 존재해야 하고, 전체 page table이 memory에 존재하는 것이 가장 바람직할 것이다.&lt;/p&gt;

&lt;h1 id=&quot;virtual-address&quot;&gt;Virtual Address&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-10-20-Paging-Mechanism/02.png&quot; alt=&quot;02.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;page table을 통해 사용되는 가상 주소와 물리 주소는 모두 number + offset의 구조를 갖는다. page number를 통해 page table의 몇 번째 row에 접근할 지를 파악하고, register에 저장된 page table의 포인터 값과 page number를 더해 해당 page table의 row에 접근한다. 이후 얻은 frame number를 통해 실제 물리 memory에 접근하게 된다. 하지만 frame number는 결국 물리 memory에서의 시작 주소를 뜻하는 값이기 때문에 얼마나 data를 읽어들일지에 대한 정보는 알지 못한다. 이 때 사용하는 것이 offset이다. 가상 주소에서의 offset을 그대로 물리 주소에서 사용하게 된다. 이러한 모든 작업은 대개 HW(CPU의 Memory Management Unit)가 수행하게 된다. 과거에는 OS에서 SW를 통해 구현해 사용하기도 했으나 속도가 HW를 이용하는 것에 비해 많이 느리다.&lt;/p&gt;

&lt;p&gt;가상 주소의 bit 사용량을 통해 역으로 OS의 각종 변수 값을 유추할 수도 있다. 가상 주소에서 offset이 차지하는 bit수가 $o$라면, 해당 OS의 page frame size는 $2^o$가 된다. 한편, 가상 주소에서 page number가 사용하는 bit 수가 $p$라면, 해당 OS의 page table의 최대 크기(가질 수 있는 최대 항목 수)는 $2^p$가 된다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-10-20-Paging-Mechanism/03.png&quot; alt=&quot;03.png&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;과도한-크기의-page-table-문제-해결&quot;&gt;과도한 크기의 Page Table 문제 해결&lt;/h1&gt;

&lt;h2 id=&quot;계층-구조-page-table-사용&quot;&gt;계층 구조 Page Table 사용&lt;/h2&gt;

&lt;p&gt;page table의 크기는 page table entry의 size * page table가 가질 수 있는 최대 항목 수로 계산할 수 있다. 즉, page table이 가질 수 있는 최대 항목 수가 클 수록 page table의 크기도 커진다는 것이다. 너무 큰 page table을 운용하게 되면 memory 낭비가 심해진다. 각 process마다 page table 운용을 위해 여러 page frame을 사용하지만 그 중 실제로 page table의 극히 일부만 사용하는 상황이 대표적인 예시이다. 이를 해결하기 위한 대표적인 방법이 계층 구조 page table이다. 주로 2단계 계층 구조, 3단계 계층 구조 등이 있다. 우선 page directory가 있어 각각의 항목이 page table을 가리키도록 한다. page directory가 가리키는 page table이 꽉 찼을 경우에만 page directory의 다음 항목에서 새로운 page table을 가리키도록 동적으로 운용하는 방식이다. 아래는 2단계 계층 구조 page table의 예시이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-10-20-Paging-Mechanism/04.png&quot; alt=&quot;04.png&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;inverted-page-table-사용&quot;&gt;Inverted Page Table 사용&lt;/h2&gt;

&lt;p&gt;Page Number를 그대로 사용하지 않고 hash function을 이용해 얻은 hash value로 사용하게 된다. hash value는 hash table에서의 인덱스이다. hash table의 항목 수는 물리 memory의 page frame의 개수와 동일하다. 즉, hash table은 모든 process가 공용으로 사용하는 것이다. 따라서 hash table entry에는 page number뿐만 아니라 pid까지 함께 담겨져 있다. hash table에서의 collision을 해결하기 위해 linked list로 다음 entry를 연결하게 된다. 이렇게 찾은 hash table entry의 hash table에서의 인덱스를 이용해 page frame을 찾아가게 된다. hash table에서의 인덱스가 $i$라면, mapping된 page frame도 실제 물리 memory에서 $i$번째 page frame이 된다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-10-20-Paging-Mechanism/05.png&quot; alt=&quot;05.png&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;tlb-translation-look-aside-buffer-사용&quot;&gt;TLB (Translation Look-aside Buffer) 사용&lt;/h2&gt;

&lt;p&gt;translation look-aside buffer란 page table 항목들을 저장하기 위한 특수한 고속 cache 장치이다. 가장 최근에 참조된 n개의 page table entry항목들을 저장하게 된다. page table은 기존과 동일하게 control bits와 frame number를 저장하고 있으며, page number를 page table에서의 인덱스로 사용한다. 하지만 TLB에서는 page number를 인덱스로 사용해 각 항목에 접근할 수 없기 때문에 page table의 항목들이 가진 정보에 더해 page number를 추가적으로 저장해야 한다. 이를 &lt;strong&gt;연관 사상(Assosiative Mapping)&lt;/strong&gt;이라고 한다.&lt;/p&gt;

&lt;p&gt;실제 가상 주소를 물리 주소로 변환하는 과정을 따라가보자. 가상 주소가 주어지면 우선 TLB에서 해당 page number가 있는지 확인한다. page number가 TLB에 있을 경우 TLB Hit으로, 바로 frame number를 얻어 물리 주소를 구해낸다. 만약 TLB에 page number가 없을 경우 TLB Miss로, 기존과 동일하게 page table에서 page number를 통해 frame number를 구해낸다. 이후 해당 page number에 관련된 정보들을 TLB에 추가한다.  만약 TLB에 여유 공간이 없을 경우 가장 오래된 항목을 제거해 공간을 확보한다. 한편, 만약 page table에서 P bit가 0이라면 Page Fault로, secondary memory(swap)에 접근해 해당 page frame을 memory로 load한다. 이후 다시 page table에서 물리 주소를 찾아나선다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-10-20-Paging-Mechanism/06.png&quot; alt=&quot;06.png&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;적절한-page-frame-size&quot;&gt;적절한 Page (Frame) Size&lt;/h1&gt;

&lt;p&gt;page의 크기는 HW 설계에 있어서 매우 중요한 issue 중 하나이다. 여러 관점에서 장단점을 고려해서 신중하게 결정해야만 한다. 만약 page의 크기가 작다면 내부 단편화가 적게 발생할 것이다. 대신 한 process 당 필요한 page의 수가 많아지고, 이는 결국 page table의 size를 늘리는 결과를 낳는다. page table의 size가 커지면 Multi-Programming 환경에서는 여러 활성 process 중 일부의 page table이 물리 memory가 아닌 swap 영역에 있어야 함을 의미한다. 최악의 경우에는 한 번의 memory 참조로 Page Fault가 2번(page table, page frame) 발생할 수 있는 것이다.&lt;/p&gt;

&lt;h1 id=&quot;paging-replacement-policy&quot;&gt;Paging Replacement Policy&lt;/h1&gt;

&lt;p&gt;Memory의 모든 page frame이 사용 중인 상황에서 swap에 위치한 page frame을 참조하는 상황이 발생할 수 있다. 이 때에는 memory의 page frame 중 하나를 swap의 page frame과 교체해야 한다. 이러한 현상을 Page Fault라고 부른다. page fault가 다수 발생하는 현상을 thrashing이라고 한다. thrashing을 방지하기 위해 합리적인 page replacement policy를 채택해야 한다. 만약 자주 호출되는 page frame을 memory에서 빼내어 swap으로 이동시키게 되면, page fault 발생 횟수가 증가해 성능에 악영향을 미칠 것이다. page replacement 정책의 핵심은 page frame이 미래에 얼마나 참조될 지를 예측하는 것이다. 미래의 일을 완전히 예견하는 것은 불가능하나 과거의 경향을 근거로 미흡하게나마 예측할 수는 있다. 따라서 page replacement 정책은 대개 과거의 page frame 이동의 경향을 파악해 미래를 예측하고자 한다. 하지만 너무 정교한 page replacement policy를 적용하게 된다면 오히려 HW와 SW 상의 부담이 더 커지기 때문에 적절한 trade-off가 이루어져야 한다. 아래에서는 6가지 Paging Replacement 정책에 대해 살펴본다.&lt;/p&gt;

&lt;h2 id=&quot;optimal&quot;&gt;Optimal&lt;/h2&gt;

&lt;p&gt;미래에 참조될 page의 순서를 모두 아는 상태에서 앞으로 참조될 때까지의 시간이 가장 긴 page를 교체한다. 당연하게도 현실에서는 구현할 수 없다. optimal 정책은 어디까지나 다른 paging replacement 정책을 평가하는 기준으로써의 가치만 있을 뿐, 구현 대상이 아니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-10-20-Paging-Mechanism/07.png&quot; alt=&quot;07.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;5 time에서 page 5가 삽입되는데, 이전 time 기준으로 main memory에 존재하는 page 2, 3, 1는 각각 1 time, 4 time, $\infin$ time 후에 다시 참조된다. 따라서 이후 참조되기까지의 시간이 가장 많이 남은 page 1이 교체되게 된다.&lt;/p&gt;

&lt;h2 id=&quot;fifo-first-input-first-out&quot;&gt;FIFO (First Input First Out)&lt;/h2&gt;

&lt;p&gt;먼저 들어온 page가 먼저 나가는 단순 Queue 방식이다. scheduling 중 RR과 비슷하다고 볼 수 있다. 가장 오래 전에 반입된 page는 memory에 가장 오래 존재했기 때문에 더이상 사용되지 않을 것이라는 논리 하에서 구현된 정책이다. 구현이 매우 간단하지만 좋은 성능을 보이지 못한다. FIFO 정책 하에서 main memory의 page frame 수를 늘릴 경우에는 page fault가 덜 발생할 것 같지만, 의외로 page fault가 더 자주 발생하기도 한다. 이를 FIFO abnormally(이상 현상)이라고 한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-10-20-Paging-Mechanism/08.png&quot; alt=&quot;08.png&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;lifo-last-input-last-out&quot;&gt;LIFO (Last Input Last Out)&lt;/h2&gt;

&lt;p&gt;가장 최근에 들어온 page가 빠져나가는 Stack 방식이다. 하나의 page frame만이 지속적으로 교체되기 때문에 page fault가 매우 자주 발생할 것이다. 하지만 의외로 평균적인 성능은 FIFO와 비슷하다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-10-20-Paging-Mechanism/09.png&quot; alt=&quot;09.png&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;lru-least-recently-used&quot;&gt;LRU (Least Recently Used)&lt;/h2&gt;

&lt;p&gt;가장 오랫동안 참조되지 않은 page를 교체하는 것이다. LRU는 Optimal과 가장 비슷한 성능을 보이지만 실제로는 구현이 매우 곤란하다는 단점이 있다. 각 page frame마다 가장 최근에 참조된 시각을 기록해야 하는데, 결국 물리 memory 내의 모든 page frame에 대해 int 변수를 추가하고 매 참조마다 갱신하는 형태가 될 수 밖에 없다. 이는 시스템에 큰 부하를 줘 좋은 성능을 내지 못한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-10-20-Paging-Mechanism/10.png&quot; alt=&quot;10.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;5 time에서 page 5가 삽입되는데, 이전 time 기준으로 memory에 존재하는 page 2, 3, 1은 각각 참조 시점이 2 time, 3 time, 1 time 전이다. 따라서 가장 오래 전에 참조된 page 3이 교체된다.&lt;/p&gt;

&lt;h2 id=&quot;lfu-least-frequently-used&quot;&gt;LFU (Least Frequently Used)&lt;/h2&gt;

&lt;p&gt;참조된 빈도가 가장 낮은 page를 교체하는 것이다. LRU와 동일하게 좋은 성능을 보이지만 구현하기 곤란하다. LRU와 마찬가지로 각 page frame마다 새로운 변수를 추가해야 하는데, 이 경우에는 참조 횟수일 것이다. 만약 동일한 참조 횟수를 가진다면 FIFO 정책을 채택해 먼저 들어온 page를 교체한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-10-20-Paging-Mechanism/11.png&quot; alt=&quot;11.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;5 time에서 page 5가 삽입되는데, 이전 time 기준으로 memory에 존재하는 page 2, 3, 1은 각각 참조 횟수 2, 1, 1을 갖는다. 따라서 page 3과 1 중 선택을 해야 하는데, FIFO 정책을 채택해 더 먼저 들어온 page 3을 교체한다.&lt;/p&gt;

&lt;h2 id=&quot;clock--nur-not-used-recently&quot;&gt;Clock = NUR (Not Used Recently)&lt;/h2&gt;

&lt;p&gt;Clock 정책은 현대 OS에서 채택하고 있는 page replacement 정책이다. LRU나 LFU와 같이 추가적인 변수를 생성하지 않고, 기존에 page table에 이미 존재하던 R, W bit를 활용하게 된다. 사용하는 bit 수가 더 많아질수록 더 좋은 성능을 보인다. 아래에서는 2 bit를 사용하는 two handed clock이 아닌 one handed clock의 예시이다. 교체하는 우선 순위는 다음과 같다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;참조되지 않았으며, 수정되지 않음 (R = 0, W = 0)&lt;/li&gt;
  &lt;li&gt;참조되었으며, 수정되지 않음 (R = 1, W = 0)&lt;/li&gt;
  &lt;li&gt;참조되었으며, 수정됨 (R = 1, W = 1)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;우선 순위 1을 먼저 찾고, 우선 순위 1이 없을 경우 우선 순위 2를 찾아나가되 그 과정에서 지나치는 모든 page frame의 R bit를 0으로 설정한다. 만약 우선 순위 2도 없을 경우 모든 page frame을 탐색하면서 R bit를 0으로 만들었을 것이다. 그 상태에서 다시 우선 순위 1을 찾는 반복을 수행한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-10-20-Paging-Mechanism/12.png&quot; alt=&quot;12.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;새로운 symbol이 추가되는데, $\rightarrow$는 memory를 가리키는 pointer이다. clock 정책에서 다음에 삽입할 page가 어디인지를 가리킨다. * symbol은 참조 여부이다. *가 있을 경우 참조된 frame (R bit = 1), *가 없을 경우 참조되지 않은 frame (R bit = 0)이다.&lt;/p&gt;

&lt;p&gt;5 time에서 page 5가 삽입되는데, 현재 pointer가 가리키는 page frame은 2이다. page 2는 이미 참조가 된 상태이므로 우선 순위 1에 해당되지 않는다. 따라서 우선 순위 2를 찾아 나선다. 그 과정에서 page 2, 3, 1의 R bit를 0으로 수정한다. 우선 순위 2를 찾기 실패했기 때문에 처음으로 되돌아온다. page 2가 R bit=0이 되었기 때문에 우선 순위 1에 해당한다. 따라서 page 2를 교체한다.&lt;/p&gt;

&lt;p&gt;6 time에서 page 2가 삽입되는데, 현재 pointer가 가리키는 page frame은 3이다. page 3은 R bit=0이므로 우선 순위 1에 해당되기 때문에 3 page를 교체한다.&lt;/p&gt;

&lt;p&gt;7 time에서 page 4가 삽입되는데, 현재 pointer가 가리키는 page frame은 1이다. page 1은 R bit=0이므로 우선 순위 1에 해당되기 때문에 1 page를 교체한다.&lt;/p&gt;

&lt;p&gt;8 time에서 page 5가 삽입되는데, 이미 main memory에 존재하므로 fault가 발생하지 않는다. 따라서 pointer도 이동하지 않는다.&lt;/p&gt;

&lt;p&gt;9 time에서 page 3이 삽입되는데, 현재 pointer가 가리키는 page frame은 5이다. page 5는 이미 참조가 된 상태이므로 우선 순위 1에 해당되지 않는다. 따라서 우선 순위 2를 찾아 나선다. 그 과정에서 page 5, page 2, page 4의 R bit를 0으로 수정한다. 우선 순위 2를 찾기 실패했기 때문에 처음으로 되돌아온다. page 5가 R bit=0이 되었기 때문에 우선 순위 1에 해당한다. 따라서 page 5를 교체한다.&lt;/p&gt;

&lt;p&gt;10 time에서 page 2가 삽입되는데, 이미 main memory에 존재하므로  fault가 발생하지 않는다. 그런데 R bit=0이므로 R bit=1로 변경한다.&lt;/p&gt;

&lt;p&gt;11 time에서 page 5가 삽입되는데, 현재 pointer가 가리키는 page frame은 2이다. page 2는 이미 참조가 된 상태이므로 우선 순위 1에 해당되지 않는다. 따라서 우선 순위 2를 찾아 나선다. 우선 순위 1에 해당하는 page 4를 찾았고, 그 과정에서 page 2의 R bit를 0으로 변경했다. page 4를 교체한다.&lt;/p&gt;

&lt;p&gt;12 time에서 page 2가 삽입되는데, 이미 main memory에 존재하므로 fault가 발생하지 않는다. 그런데 page 2의 R bit=0이므로 R bit를 1로 변경한다.&lt;/p&gt;
</description>
        <pubDate>Mon, 19 Oct 2020 19:00:00 -0500</pubDate>
        <link>http://0.0.0.0:4000/operating%20system/Paging-Mechanism/</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/operating%20system/Paging-Mechanism/</guid>
        
        <category>Operating System</category>
        
        
        <category>Operating System</category>
        
      </item>
    
      <item>
        <title>[NLP 논문 리뷰] An Empirical Study of Tokenization Strategies for Various Korean NLP Tasks</title>
        <description>&lt;h2 id=&quot;paper-info&quot;&gt;Paper Info&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2010.02534&quot;&gt;Archive Link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2010.02534.pdf&quot;&gt;Paper Link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Submit Date: Oct 6, 2020&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;NLP에서 Tokenization은 전처리 과정에서 가장 중요한 issue 중 하나이다. 가장 적절한 Tokenization 전략을 찾기 위한 연구는 수도 없이 이루어져 왔다. 그 중 가장 대표적인 방식이 BPE이다. BPE는 많은 연구를 통해 보편적으로 가장 효율적인 Tokenization 기법으로 알려졌지만, 아직 language나 task에 구애받지 않고 가장 효율적인가에 대해서는 명확하지 않다. 본 논문에서는 English에 비해 언어 형태론적으로 더 난해한 언어인 Korean에 적합한 tokenization 기법을 찾아내고자 한다. BPE는 가장 보편적인 language인 English를 기준으로 연구된 방식이기에 Korean에 적합하지 않을 수 있다는 생각에서 시작된 연구이다. 본 논문에서는 Korean-English translation, natural language understanding, machine reading comprehension, natural language inference, semantic textual similarity, sentiment analysis, paraphrase identification 등 많은 task에서 실험을 진행했다.&lt;/p&gt;

&lt;h1 id=&quot;background&quot;&gt;Background&lt;/h1&gt;

&lt;h2 id=&quot;mecab-ko-a-korean-morphological-analyzer&quot;&gt;MeCab-ko: A Korean Morphological Analyzer&lt;/h2&gt;

&lt;p&gt;MeCab은 Conditional Random Fields(CRFs)를 기반으로 하는 Japanese 형태소 번역기이다. Japanese와 Korean의 형태론, 문법 상의 유사성에서 착안해 Korean에 적용시킨 것이 MeCab-ko이다. MeCab-ko는 Sejong Corpus를 통해 학습되었으며, 많은 Korean NLP task에서 사용되어 왔고 매우 좋은 성능을 보였다.&lt;/p&gt;

&lt;h2 id=&quot;byte-pair-encoding&quot;&gt;Byte Pair Encoding&lt;/h2&gt;

&lt;p&gt;BPE는 data에서의 등장 빈도를 기반으로 묶는 data-driven statistical alogirhtm이다.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://cpm0722.github.io/paper%20review/Neural-Machine-Translation-of-Rare-Words-with-Subword-Units/&quot;&gt;Neural Machine Translation of Rare Words with Subword Units&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;related-work&quot;&gt;Related Work&lt;/h1&gt;

&lt;p&gt;몇몇 연구에서는 단순 BPE보다 해당 language의 구문에 대한 정보를 기반으로 한 segmentation 기법과 BPE를 혼합해 적용하는 것이 더 좋은 성능을 보인다고 주장해왔다. 특히 non-English language, 그 중 형태론적으로 unique한 특성을 갖는 language에 대해서 더욱 두드러진다. Hindi/Bengali, Arabic, Latvian 등에 대해서 BPE와 함께 unique한 segmentation 기법을 혼용한 연구가 진행되었으며, Korean에 있어서도 동일한 연구가 진행되었다. 하지만 Tokenization이 아닌 NMT task에 있어서 사용되는 parallel corpus filtering 전처리에 관한 연구였다는 점에서 본 논문과는 목적이 다르다.&lt;/p&gt;

&lt;h1 id=&quot;tokenization-strategies&quot;&gt;Tokenization Strategies&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-10-10-An-Empirical-Study-of-Tokenization-Strategies-for-Various-Korean-NLP-Tasks/01.jpg&quot; alt=&quot;01.jpg&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;consonant-and-vowel-cv&quot;&gt;Consonant and Vowel (CV)&lt;/h2&gt;

&lt;p&gt;자모 단위로 tokenizing을 하는 기법이다. 공백에 대해서 special token \(\star\)를 추가했다.&lt;/p&gt;

&lt;h2 id=&quot;syllable&quot;&gt;Syllable&lt;/h2&gt;

&lt;p&gt;음절 단위로 tokenizing을 하는 기법이다. 역시나 공백에 대한 special token \(\star\)를 사용한다.&lt;/p&gt;

&lt;h2 id=&quot;morpheme&quot;&gt;Morpheme&lt;/h2&gt;

&lt;p&gt;MeCab-ko의 형태소 단위 tokenizer를 사용한다. 하지만 이를 사용하면 original input에서의 공백이 제거가 되고, 따라서 original sentence로의 복원이 불가능해진다. 이를 해결하기 위해 공백 special token \(\star\)를 추가했다.&lt;/p&gt;

&lt;h2 id=&quot;subword&quot;&gt;Subword&lt;/h2&gt;

&lt;p&gt;SentencePiece를 사용한 BPE를 적용했다. original sentence의 단어 단위를 구분하기 위해서 original sentence의 공백에 대응하는 token \(\_\)를 매 단어의 시작에 삽입했다.&lt;/p&gt;

&lt;h2 id=&quot;morpheme-aware-subword&quot;&gt;Morpheme-aware Subword&lt;/h2&gt;

&lt;p&gt;위의 Subword 방식에서 한 발 더 나아가 언어론적 특징을 기반으로 한 segmentation 전략을 BPE와 결합한 방식이다. Morpheme 방식을 먼저 적용한 뒤, 형태소의 list에 대해서 BPE를 적용하게 된다. Morpheme를 적용한 후에 BPE를 사용하기 때문에 형태소 경계를 뛰어넘는 BPE는 발생하지 않는다. (“나랑 쇼핑하자.”에서 ‘쇼핑’, ‘하’는 각각이 별개의 형태소이기 때문에 (‘핑’,’하’)가 Pair로 묶일 수는 없다.)&lt;/p&gt;

&lt;h2 id=&quot;word&quot;&gt;Word&lt;/h2&gt;

&lt;p&gt;original input에서 공백을 기준으로 단어 단위로 tokenizing을 수행하는 가장 단순한 방식이다.&lt;/p&gt;

&lt;h1 id=&quot;experiments&quot;&gt;Experiments&lt;/h1&gt;

&lt;h2 id=&quot;korean-tofrom-english-machine-translation&quot;&gt;Korean to/from English Machine Translation&lt;/h2&gt;

&lt;h3 id=&quot;dataset&quot;&gt;Dataset&lt;/h3&gt;

&lt;p&gt;AI Hub에서 제공하는 Korean-English parallel corpus를 사용했다. 800K sentences pairs의 news data를 포함하고, 784K의 train data, 8K의 dev data, 8K의 test data로 구분했다.&lt;/p&gt;

&lt;h3 id=&quot;bpe-modeling&quot;&gt;BPE Modeling&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-10-10-An-Empirical-Study-of-Tokenization-Strategies-for-Various-Korean-NLP-Tasks/02.jpg&quot; alt=&quot;02.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;BPE training에서 AI Hub의 data를 사용할지, Wiki의 data를 사용할지 결정하기 위해 실험을 진행한다. AI Hub의 data는 실제 task에서의 dataset과 동일하기 때문에 corpus set이 동일하다는 장점이 있는 반면, dataset의 크기가 작다. Wiki는 dataset의 크기가 크지만, news에서 사용되는 corpus set과는 차이가 있다는 단점이 있다. Korean-English Translation, English-Korean Translation으로 성능을 비교해보는데, English BPE는 동일하게 Wiki의 English data를 사용한 32K BPE model을 사용했다. 그 결과, AI Hub의 data보다 Wiki의 data가 더 좋은 성능을 보였다. 따라서 본 논문의 이후에서는 Korean BPE training을 위해 Wiki dataset을 사용한다.&lt;/p&gt;

&lt;h3 id=&quot;training&quot;&gt;Training&lt;/h3&gt;

&lt;p&gt;다양한 vocabulary size의 BPE model로 AI Hub news dataset에 대해서 tokenization 기법을 테스트한다. 우선 NMT task에서 SOTA를 달성한 Transformer model을 사용한다. 가장 보편적으로 사용되는 hyperparameter 값을 채택했다. FAIRSEQ를 사용해 실험을 진행했다. 50 epochs마다 checkpoint를 저장했다.&lt;/p&gt;

&lt;h3 id=&quot;results&quot;&gt;Results&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-10-10-An-Empirical-Study-of-Tokenization-Strategies-for-Various-Korean-NLP-Tasks/03.jpg&quot; alt=&quot;03.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Ko-En, En-Ko task에서 모두 Subword와 Syllable가 Morpheme이나 Word보다 더 좋은 성능을 보였다. 이는 OOV Rate와 큰 관련이 있다. 한국어의 형태론은 너무 복잡한 규칙을 가져 수많은 형태소가 있기 때문에 64K 이하의 vocabulary size로는 OOV가 많이 발생할 수 밖에 없다. 하지만 Subword나 Syllable은 모두 음절 단위의 model이기 때문에 OOV가 훨씬 더 적게 발생하게 된다.&lt;/p&gt;

&lt;p&gt;한편 CV의 OOV Rate는 당연히 가장 적은 수치를 보여주는데, Syllable나 Subword에 비해서는 더 낮은 성능을 보여준다. 이를 통해 자모 단위는 문맥 정보를 담기에는 너무 작은 단위라는 것을 알 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-10-10-An-Empirical-Study-of-Tokenization-Strategies-for-Various-Korean-NLP-Tasks/04.jpg&quot; alt=&quot;04.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Morpheme-aware Subword가 가장 높은 BLEU Scores를 보여준다. Subword와 Morpheme-aware Subword의 차이점은 BPE 이전에 Morpheme의 수행 여부인데, 이는 결국 형태소 경계를 넘어서는 BPE가 발생하는가(Token Spanning Morpheme Boundaries)에서 차이를 보인다. 위의 Table은 Subword에서 각 vocabulary size마다 발생하는 Tokens Spanning Morpheme Boundaries의 횟수를 보여준다. 6~37%의 수치를 보여준다. 이를 통해 형태소 단위의 구분은 tokenization에서 성능에 큰 영향을 미치며, 따라서 형태소 구분을 무시한 단순 BPE는 Korean Tokenizing에 적합하지 않다는 것을 알 수 있다.&lt;/p&gt;

&lt;h2 id=&quot;korean-natural-language-understanding-tasks&quot;&gt;Korean Natural Language Understanding Tasks&lt;/h2&gt;

&lt;p&gt;BERT model을 사용했다. KorQuAD, KorNLI, KorSTS, NSMC, PAWS의 5개 NLU downstream task에 대해서 테스트를 진행했다.&lt;/p&gt;

&lt;h3 id=&quot;downstream-tasks&quot;&gt;Downstream Tasks&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Machine Reading Comprehension: KorQuAD 1.0 Dataset&lt;/p&gt;

    &lt;p&gt;SQuAD를 Korean에 맞게 적용한 dataset이다. 10,645개의 지문과 66,181개의 질문이 포함되고, 각 지문에 대해 주어진 여러 질문 중 가장 적합한 질문을 선택하는 task이다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Natural Language Inference: KorNLI Dataset&lt;/p&gt;

    &lt;p&gt;950,354개의 sentence pair(전제, 추론)이 있고 각 pair에 대해 두 sentence 사이의 관계가 entailment, contradiction, neutral인지 classification을 수행하는 task이다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Semantic Textual Similarity: KorSTS Dataset&lt;/p&gt;

    &lt;p&gt;8628개의 sentence pair가 있고, 각 pair에 대해 0~5 사이의 semantic similarity를 도출해내는 task이다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Sentiment Analysis: NSMC Dataset&lt;/p&gt;

    &lt;p&gt;네이버 영화 review에서 추출한 400K의 sentence에 대해 0(negative)~1(positive) 사이의 sentiment Analysis를 도출해내는 task이다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Paraphrase Identification: PAWS-X Dataset&lt;/p&gt;

    &lt;p&gt;paraphrase identification dataset인 PAWS-X에서 Korean dataset만 추출해낸 53,338 sentence pairs에 대해 0(negative)~1(positive)의 paraphrase identification을 도출해내는 task이다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;training-1&quot;&gt;Training&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-10-10-An-Empirical-Study-of-Tokenization-Strategies-for-Various-Korean-NLP-Tasks/05.jpg&quot; alt=&quot;05.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;large corpus로 pre-train된 BERT-Base model을 각 5개의 NLU task에 대해 별개로 fine-tuning시켜 실험을 진행했다. Korean Wiki Corpus(640MB)는 pre-train을 진행할 만큼 충분한 크기가 되지 못해 Namu-wiki에서 5.5GB의 corpus를 추출해내 Wiki Corpus와 함께 사용했다. hyperparameter는 batch size=1024, max sequence length=128, optimizer=AdamW, lr=5e-5, warm up steps=10K를 사용했다. pre-trained된 BERT Model을 Tensorflow에서 Pytorch로 convert한 뒤, HuggingFace Transformers를 사용해 fine-tuning을 진행했다. fine-tuning에서의 hyperparameter는 위 Table의 값을 사용했다.&lt;/p&gt;

&lt;h3 id=&quot;results-1&quot;&gt;Results&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-10-10-An-Empirical-Study-of-Tokenization-Strategies-for-Various-Korean-NLP-Tasks/06.jpg&quot; alt=&quot;06.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 5개의 NLU task에 대해 6개의 tokenizing 기법을 사용해 각각 dev set, test set에서의 성능을 측정했다. 예외적으로 KorQuAD의 경우에는 test set이 부족해 dev set만 사용했다.&lt;/p&gt;

&lt;p&gt;KorQuAD task에서는 Subword 64K model이 가장 좋은 성능을 보였다. Morpheme와 Subword에서는 vocabulary size와 Score가 비례 관계이다. 하지만 Morpheme-aware Subword에서는 32K model이 제일 높은 수치를 달성했다. 결론적으로, Morpheme-aware Subword model에서는 성능과 vocabulary size 사이의 유의미한 상관관계를 찾을 수 없었다.&lt;/p&gt;

&lt;p&gt;나머지 다른 4개의 task에 대해서는 모두 Morpheme-aware Subword의 64K model이 가장 좋은 성능을 달성했다. tokenization 방식에 관계 없이 모두 다 vocabulary size와 score가 대체로 비례 관계를 보였다. 그러나 위에서 진행했던 NMT task에 있어서는 Morpheme-aware Subword에서의 높은 vocabulary size가 좋은 성능을 보장하지는 않았는데, 다소 배치되는 결과이다.&lt;/p&gt;

&lt;h1 id=&quot;discussion&quot;&gt;Discussion&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-10-10-An-Empirical-Study-of-Tokenization-Strategies-for-Various-Korean-NLP-Tasks/07.jpg&quot; alt=&quot;07.jpg&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;token-length&quot;&gt;Token Length&lt;/h2&gt;

&lt;p&gt;token의 길이가 성능에 얼마나 영향을 미치는지 알아본다. token length는 한 token에 포함된 음절 개수의 평균으로 정의한다. CV의 경우에는 자모 단위이기 때문에 평균 token length는 0.33~0.5 사이의 값이다. 한 음절은 2~3개의 자모로 구성되어 있기 때문이다. Syllable은 음절 단위로 tokenizing을 한 것이기 때문에 평균 token length는 1의 고정 값을 갖는다. Morpheme는 형태소 단위로 tokenizing을 수행한 것이기 때문에 평균 token length가 vocabulary size에 따라 변하지 않고 일정하다. Subword나 Morpheme-aware-Subword는 모두 BPE를 사용하는 방식이기 때문에 vocabulary size가 증가할수록 token length도 증가하게 된다. 통계적인 빈도를 기반으로 vocabulary size에 따라 상위 N개를 pair로 묶기 때문이다. 위의 figure에는 word model이 누락됐는데, word model은 Ko-En과 En-Ko에서 각각 7.07, 18.42로 매우 낮은 Score를 보여줘 공간상의 제약으로 figure에서 제외했다.&lt;/p&gt;

&lt;p&gt;Figure 1을 분석해보자. 자모 단위로 tokenizing을 수행한 CV의 성능이 기준점이다. 대부분의 model은 평균 token length가 1.0~1.5인 구간에서 가장 좋은 성능을 보여준다. 평균 token length가 1.5를 넘어가기 시작하면서 점차 감소하는 경향을 보인다. 특히 평균 token length가 2.5에 달하는 word model의 경우에는 최악의 성능을 보여줬다.&lt;/p&gt;

&lt;h2 id=&quot;linguistic-awareness&quot;&gt;Linguistic Awareness&lt;/h2&gt;

&lt;p&gt;Figure 1에서 8K Subword model과 16K Morpheme-aware Subword model을 비교해보자. figure에서 파란 색 배경으로 강조 표시가 된 부분이다. 두 model은 평균 token length가 동일한 값이다. 두 model의 차이는 언어론적 지식을 사용했는가(형태소 경계를 넘어서는 pair를 생성했는가)에 있다. Ko-En과 En-Ko 두 task에서 모두 Morpheme-aware Subword model이 더 좋은 성능을 보여줬다는 것은 token length뿐만 아니라 linguistic awareness도 tokenization 전략 수립에 매우 중요한 factor라는 것을 보여준다.&lt;/p&gt;

&lt;h2 id=&quot;under-trained-tokens&quot;&gt;Under-trained Tokens&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-10-10-An-Empirical-Study-of-Tokenization-Strategies-for-Various-Korean-NLP-Tasks/08.jpg&quot; alt=&quot;08.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 1에서 Morpheme model의 경우에만 예외적으로 CV보다 훨씬 못한 성능을 보여준다. 이러한 결과는 Morpheme model의 높은 OOV rate에서 비롯된다. 위의 Experiments에서 살펴본 NMT task에서의 result table을 확인해보면 Morpheme model의 OOV rate가 압도적으로 높다는 것을 확인할 수 있다(본 논의에서는 모든 task에서 최악의 성능을 보여줬던  Word model은 배제한다). OOV는 정의하자면 test set에서만 등장하고, train set에서는 등장하지 않았던 token을 의미한다. 즉, OOV rate가 높다는 것은 model 입장에서는 처음 보는 token이 test set에서 등장하는 비율을 의미한다. 완전히 처음 마주하는 token이 아닌 적게 마주한 token들의 비율에 대해서도 확인을 해보자. OOV가 아니라 하더라도 등장 빈도가 확연히 적은 token들에 대해서는 model이 under-train했을 가능성이 농후하기 때문이다. Figure 2에서는 실제로 등장 빈도가 낮은 token의 비중이 얼마나 되는지를 시각화 한 graph이다.  예상했던 바와 같이 OOV rate가 높은 Morpheme model이 훨씬 더 높은 수치를 보여준다는 것을 확인할 수 있다. 이는 결국 Morpheme model이 under-trained된 token의 비중이 높다는 것을 의미한다. 이러한 이유로 Morpheme model이 타 model 대비 확연히 낮은 성능을 보이는 것이다.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;여러 Korean NLP task에 대해서 다양한 tokenization 전략을 사용한 model들의 성능을 비교했다. Korean-English NMT task에서는 BPE에 언어론적 특성(형태소)를 더한 Morpheme-aware Subword Model이 가장 높은 성능을 보여줬다. NLU task의 KorQuAD를 제외한 모든 task에서 역시 Morpheme-aware Subword Model이 가장 좋은 수치를 달성했다. 이를 통해 각 language의 unique한 linguistic awareness가 model 성능 향상에 매우 큰 영향을 미친다는 사실을 도출해냈다.&lt;/p&gt;
</description>
        <pubDate>Fri, 09 Oct 2020 19:00:00 -0500</pubDate>
        <link>http://0.0.0.0:4000/machine%20learning/paper%20review/An-Empirical-Study-of-Tokenization-Strategies-for-Various-Korean-NLP-Tasks/</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/machine%20learning/paper%20review/An-Empirical-Study-of-Tokenization-Strategies-for-Various-Korean-NLP-Tasks/</guid>
        
        <category>NLP</category>
        
        <category>Korean</category>
        
        
        <category>Machine Learning</category>
        
        <category>Paper Review</category>
        
      </item>
    
      <item>
        <title>[NLP 논문 리뷰] RoBERTa: A Robustly Optimized BERT Pretraining Approach</title>
        <description>&lt;h2 id=&quot;paper-info&quot;&gt;Paper Info&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1907.11692&quot;&gt;Archive Link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1907.11692.pdf&quot;&gt;Paper Link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Submit Date: Jul 26, 2019&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;BERT 계열의 model들은 지금까지 매우 뛰어난 성능을 보여왔다. 본 논문에서는 BERT에 대한 추가적인 연구를 통해 기존의 BERT model들이 undertrained되었음을 보여주고, 다음의 개선 방안들을 제시한다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;더 긴 시간, 더 큰 batch size의 training&lt;/li&gt;
  &lt;li&gt;NSP 제거&lt;/li&gt;
  &lt;li&gt;long sequence에 대한 학습&lt;/li&gt;
  &lt;li&gt;MLM에서의 동적인 masking 정책&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;본 논문의 Contribution은 다음과 같다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;downstream task에서의 성능 향상을 위한 BERT model의 design choice, training 전략을 제시&lt;/li&gt;
  &lt;li&gt;새로운 CC-NEWS dataset을 도입&lt;/li&gt;
  &lt;li&gt;적절한 design choice에 기반한 MLM이 다른 여러 method 대비 좋은 성능을 보임을 입증&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;background&quot;&gt;Background&lt;/h1&gt;

&lt;p&gt;BERT에 대한 overview&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://cpm0722.github.io/paper%20review/BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding/&quot;&gt;BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding&lt;/a&gt; 참고&lt;/p&gt;

&lt;h1 id=&quot;experimental-setup&quot;&gt;Experimental Setup&lt;/h1&gt;

&lt;h2 id=&quot;implementation&quot;&gt;Implementation&lt;/h2&gt;

&lt;p&gt;대부분의 hyperparameters의 값은 BERT와 동일한 값을 채택했다. 하지만 peak learning rate와 #warmup steps, Adam epsilon에 대해서는 tuning을 진행했다. 전체 sequence의 최대 길이 T는 512로 했다.&lt;/p&gt;

&lt;h2 id=&quot;data&quot;&gt;Data&lt;/h2&gt;

&lt;p&gt;BERT-style의 pretraining 방식은 data의 양에 따라 성능이 결정된다. BERT 이후 더 큰 dataset을 사용한 여러 연구가 진행되었으나, 그 dataset이 공개되지는 않았다. 본 논문에서는 총 160GB의 4개의 English dataset을 사용했다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Book Corpus + Wikipedia&lt;/p&gt;

    &lt;p&gt;Original BERT에서 사용했던 dataset으로, 총 16GB이다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;CC-News&lt;/p&gt;

    &lt;p&gt;총 63million개의 2016/09~2019/02 사이의 뉴스 기사를 crawling한 dataset으로, 총 76GB이다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;OpenWebText&lt;/p&gt;

    &lt;p&gt;Reddit과 같은 web site에서 URL 기반으로 crawling을 한 dataset으로, 총 38GB이다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Stories&lt;/p&gt;

    &lt;p&gt;story-like dataset으로, 총 31GB이다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;evaluation&quot;&gt;Evaluation&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;GLUE&lt;/p&gt;

    &lt;p&gt;&lt;a href=&quot;https://cpm0722.github.io/paper%20review/BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding/&quot;&gt;BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding&lt;/a&gt; 참고&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;SQuAD&lt;/p&gt;

    &lt;p&gt;&lt;a href=&quot;https://cpm0722.github.io/paper%20review/BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding/&quot;&gt;BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding&lt;/a&gt; 참고&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;RACE&lt;/p&gt;

    &lt;p&gt;ReAding Comprehension from Examinations이다. 중국에서의 Enghlish Examination에서 추출한 28,000개의 단락과 100,000개의 질문이 존재한다. 각 단락은 여러 질문과 함께 등장하는데, 그 중 하나의 알맞은 질문을 고르는 task이다. 기존의 다른 comprehension dataset에 비해 passage의 길이가 길고, 추론 질문의 비율이 높다는 점에서 차이가 있다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;training-procedure-analysis&quot;&gt;Training Procedure Analysis&lt;/h2&gt;

&lt;p&gt;BERT model L=12, H=768, A=12, #params: 110M으로 은 고정해둔 상태로 실험을 진행한다.&lt;/p&gt;

&lt;h2 id=&quot;static-vs-dynamic-masking&quot;&gt;Static vs Dynamic Masking&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-10-05-RoBERTa-A-Robustly-Optimized-BERT-Pretraining-Approach/01.jpg&quot; alt=&quot;01.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;본 논문에서는 Dynamic Masking 기법을 도입했다. 기존의 BERT에서의 Masking Rule은 전처리 과정에서 한 번 수행한 masking이 계속 유지되는 Static Masking이다. 이는 매 epoch마다 동일한 masking으로 학습을 하게 됨을 의미한다. Dynamic Masking은 training data를 10배로 복제해 각각의 training data마다 다른 masking을 수행했다. 같은 비율의 masking 정책 하에서 (80%/10%/10% 등) 다른 word가 masking되는 것이다. 이를 40 epochs동안 수행하는데, 결국 같은 masking으로 총 4epochs의 학습이 이루어지게 되는 것이다. 이러한 Dynamic Masking 기법은 dataset이 클 수록 Static Masking 대비 더 큰 성능 향상을 보였다. 위 Table에서 볼 수 있듯이 static 대비 dynamic masking이 조금이나마 더 좋은 성능을 보였다.&lt;/p&gt;

&lt;h2 id=&quot;model-input-format-and-next-sentence-prediction&quot;&gt;Model Input Format and Next Sentence Prediction&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-10-05-RoBERTa-A-Robustly-Optimized-BERT-Pretraining-Approach/02.jpg&quot; alt=&quot;02.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;NSP가 성능 향상에 얼마나 기여하는지에 대해서는 많은 연구가 있어왔고, 때로는 각 논문마다 다른 결과를 도출해내기도 했다. 이를 검증하기 위해 위와 같은 실험을 진행했다. 각 항목에 대해서 자세히 살펴보겠다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;SEGMENT-PAIR&lt;/p&gt;

    &lt;p&gt;여러 문장으로 이루어질 수 있는 segment의 pair이다. 한 pair는 최대 512 tokens까지 가질 수 있다. NSP를 포함한다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;SENTENCE-PAIR&lt;/p&gt;

    &lt;p&gt;문장의 pair이다. 한 pair는 최대 512 tokens까지 가질 수 있다. 당연하게도 평균적인 #tokens가 SEGMENT-PAIR보다 작기 때문에, batch size를 늘려 SEGMENT-PAIR의 한 batch와 total #tokens가 비슷해지도록 했다. NSP를 포함한다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;FULL-SENTENCES&lt;/p&gt;

    &lt;p&gt;여러 문장들의 sequences이다. 최대 512 tokens까지 가질 수 있다. 문장이 중간에 끊기는 일은 없도록 한다. 특수한 경우로, 한 document가 끝났음에도 새로운 문장이 삽입될 수 있을 경우, 다음 document의 첫 문장부터 이어서 삽입되게 된다. 이 경우에 있어서는 서로 다른 document에서 온 문장 사이에 특수한 seperator token이 삽입된다. NSP를 포함하지 않는다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;DOC-SENTENCES&lt;/p&gt;

    &lt;p&gt;FULL-SENTENCES와 유사하지만, 서로 다른 document에서 온 문장이 연속되는 일이 없도록 한다. 당연하게도 FULL-SENTENCES에 비해 평균 #tokens가 낮을 수 밖에 없기 때문에, 이 역시 batch size를 늘려 FULL-SENTENCES의 한 batch와 total #tokens가 비슷해지도록 했다. NSP를 포함하지 않는다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;SEGMENT-PAIR와 SENTENCE-PAIR를 비교해보자. SENTENCE-PAIR가 더 낮은 성능을 보였다. SENTENCE-PAIR는 model이 long-range dependencies를 학습할 능력이 없다고 가정한 것인데, 실제로는 BERT model이 long sequences에 대해서도 dependency를 학습할 수 있음을 알 수 있다.&lt;/p&gt;

&lt;p&gt;한편, DOC-SENTENCES와 BERT_BASE를 비교해보면 DOC-SENTENCES가 original BERT보다 더 좋은 성능을 보인다는 것을 확인할 수 있다. 이는 NSP가 오히려 downstream task의 성능에 악영향을 미친다는 것을 보여준다.&lt;/p&gt;

&lt;p&gt;마지막으로, FULL-SENTENCES와 DOC-SENTENCES를 비교하면 한 document 안의 문장만 묶는 DOC-SENTENCES가 미약하게나마 더 좋은 성능을 보인다는 것을 확인할 수 있다. 하지만 DOC-SENTENCES는 batch size가 각 batch마다 다르기 때문에, 본 논문의 이후 실험에서는 편리성을 위해 FULL-SENTENCES를 사용하기로 한다.&lt;/p&gt;

&lt;h2 id=&quot;training-with-large-batches&quot;&gt;Training with large batches&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-10-05-RoBERTa-A-Robustly-Optimized-BERT-Pretraining-Approach/03.jpg&quot; alt=&quot;03.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;batch-size를 증가시킬수록 optimization speed와 성능 향상에 기여한다는 것은 알려져 있다. BERT 역시 larget batch size가 효과가 있는지 확인을 해보기로 한다. batch size=256, #steps=1M은 original BERT_BASE의 값이다. 이와 동일한 computational cost를 갖는 다른 batch size &amp;amp; #steps를 실험해본다. 실제로 같은 computational cost를 갖는 경우에도 batch size가 커질수록 perplexity가 감소함을 확인할 수 있었다. end-task에서의 정확도도 상승했다.&lt;/p&gt;

&lt;h2 id=&quot;text-encoding&quot;&gt;Text Encoding&lt;/h2&gt;

&lt;p&gt;BPE는 등장 빈도를 기반으로 subword를 생성해내는 기법으로, OOV가 없다는 장점이 있다. 기존의 BPE는 모두 character 단위로 이루어졌는데, original BERT도 이를 채택했다. 본 논문에서는 unicode character 단위가 아닌 byte 단위로 하는 BPE를 도입하기로 한다. original BERT의 BPE는 vocabulary size가 30K였다면, 새로운 방식은 50K 정도의 큰 vocabulary size가 필요하다. 하지만 기존에는 필수적이던 전처리 과정이 필요없다는 장점을 갖는다. 사실 새로운 BPE는 약간의 성능 하락을 보여주지만, universal한 encoding 방식을 도입했다는 점에서 미미한 정도의 성능 하락을 감안하고서라도 채택해볼 만 하다.&lt;/p&gt;

&lt;h1 id=&quot;roberta&quot;&gt;RoBERTa&lt;/h1&gt;

&lt;p&gt;정리하자면, &lt;strong&gt;RoBERTa&lt;/strong&gt;는 &lt;strong&gt;R&lt;/strong&gt;obustly &lt;strong&gt;o&lt;/strong&gt;ptimized &lt;strong&gt;BERT&lt;/strong&gt; &lt;strong&gt;a&lt;/strong&gt;pproach의 약자로, 다음의 4가지 특징을 갖는다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;dynamic masking 기법&lt;/li&gt;
  &lt;li&gt;FULL-SENTENCES&lt;/li&gt;
  &lt;li&gt;large mini batches&lt;/li&gt;
  &lt;li&gt;byte-level BPE&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-10-05-RoBERTa-A-Robustly-Optimized-BERT-Pretraining-Approach/04.jpg&quot; alt=&quot;04.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;original BERT_LARGE model과 성능을 비교하기 위해서 RoBERTa의 model 크기를 BERT_LARGE와 동일하게 했다. 또한 original BERT에서 사용했던 dataset으로만 pretraining한 경우, 추가적인 dataset으로 pretraining한 경우, pretraining 횟수를 100K에서 300K, 500K로 증가시킨 경우를 비교했다. RoBERTa는 BERT_LARGE나 XLNet_LARGE와 동일한 조건에서도 더 높은 성능을 보였으며, 당연하게도 가장 많은 pretraining을 시킨 경우가 가장 좋은 성능을 보였다.&lt;/p&gt;

&lt;h2 id=&quot;glue-results&quot;&gt;GLUE Results&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-10-05-RoBERTa-A-Robustly-Optimized-BERT-Pretraining-Approach/05.jpg&quot; alt=&quot;05.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Single-task single models on dev는 GLUE의 각 subtask에 대해 별개로 fine-tuning을 진행한 model들이다.&lt;/p&gt;

&lt;p&gt;Ensembles on test는 GLUE leaderboard에 있는 다른 score와 비교한 것이다. 특이하게 RoBERTa는 RTE, SST, MRPC subtask에 대해서 pretrained RoBERTa에서 fine-tuning을 시작하지 않고, MNLI single-task model에서 fine-tuning을 시작했다. 이 경우가 더 좋은 성능을 보였다고 한다.&lt;/p&gt;

&lt;p&gt;Single-task, single models에서는 RoBERTa가 9개의 모든 GLUE subtask에서 SOTA를 달성했다. 주목할만한 점은, 여기서의 RoBERTa는 original BERT_LARGE와 동일한 model architecture, 동일한 masking rule(static masking)을 적용했다는 점이다. 이는 굳이 dataset size나 training time을 배제하더라도, training objective(NSP 제거)가 얼마나 큰 영향을 미치는지를 보여준다.&lt;/p&gt;

&lt;p&gt;Ensembles on test에서 RoBERTa는 전체 9개 중 4개의 subtask에서 SOTA를 달성했다. 비교 대상인 다른 model들과 달리 RoBERTa는 multi-task fine-tuning을 수행하지 않았다는 점에서 큰 의미가 있다.&lt;/p&gt;

&lt;h2 id=&quot;squad-results&quot;&gt;SQuAD Results&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-10-05-RoBERTa-A-Robustly-Optimized-BERT-Pretraining-Approach/06.jpg&quot; alt=&quot;06.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;SQuAD 2.0에 대해서는 추가적으로 answerable에 대한 binary classification을 수행하고, 기존의 loss와 더했다. 한편, RoBERTa는 original BERT나 XLNet과 달리 pretraining에서 추가적인 QA dataset을 사용하지 않고, 바로 SQuAD에 대해 fine-tuning을 진행했다. 그럼에도 불구하고 BERT나 XLNet에 비해 더 좋은 성능을 보였다.&lt;/p&gt;

&lt;h2 id=&quot;race-results&quot;&gt;RACE Results&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-10-05-RoBERTa-A-Robustly-Optimized-BERT-Pretraining-Approach/07.jpg&quot; alt=&quot;07.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;RoBERTa 는 RACE task에서도 Middle school data와 High school data 모두에서 BERT_LARGE나 XLNET_LARGE 대비 더 좋은 성능을 보였다.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;BERT model의 성능을 향상시키는 여러 방법을 제시했다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;더 많은 횟수, 더 큰 batch size, 더 많은 data로 pretraining&lt;/li&gt;
  &lt;li&gt;NSP 제거&lt;/li&gt;
  &lt;li&gt;longer sequences로 pretraining&lt;/li&gt;
  &lt;li&gt;dynamic masking&lt;/li&gt;
  &lt;li&gt;byte 기반 BPE&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;이를 종합한 RoBERTa는 GLUE, SQuAD, RACE에서 SOTA를 달성했다.&lt;/p&gt;
</description>
        <pubDate>Sun, 04 Oct 2020 19:00:00 -0500</pubDate>
        <link>http://0.0.0.0:4000/machine%20learning/paper%20review/RoBERTa-A-Robustly-Optimized-BERT-Pretraining-Approach/</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/machine%20learning/paper%20review/RoBERTa-A-Robustly-Optimized-BERT-Pretraining-Approach/</guid>
        
        <category>NLP</category>
        
        
        <category>Machine Learning</category>
        
        <category>Paper Review</category>
        
      </item>
    
      <item>
        <title>[운영체제] Address &amp; Memory</title>
        <description>&lt;p&gt;숭실대학교 컴퓨터학부 홍지만 교수님의 2020-2학기 운영체제 강의를 정리 및 재구성했다.&lt;/p&gt;

&lt;h1 id=&quot;virtual-memory의-등장-과정&quot;&gt;Virtual Memory의 등장 과정&lt;/h1&gt;

&lt;h2 id=&quot;single-programming&quot;&gt;Single Programming&lt;/h2&gt;

&lt;p&gt;초기 (1950~1970)의 운영체제는 물리 memory에 하나의 program만을 올리는 Single Programming 형태였다. 즉, memory는 OS 영역, 실행 중인 1개의 program이 올라가는 영역으로 구분됐다. memory 가상화에 대한 개념도 존재하지 않았다. 동시에 memory를 점유할 수 있는 program의 개수가 최대 1개였기 때문에 multi-tasking도 불가능했다. Single Programming을 채택한 대표적인 OS로는 MS-DOS가 있다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-10-05-Address-Memory/01.png&quot; alt=&quot;01.png&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;multi-programming&quot;&gt;Multi Programming&lt;/h2&gt;

&lt;p&gt;이후 Multi Programming이 등장했다. 동시에 여러 process를 실행(되는 것 처럼 보이게)할 수 있다. OS는 하나의 process를 매우 짧은 시간동안 실행하고, 이후 다른 process로 전환(context switch)한다. 이러한 과정을 계속 반복하게 된다. 이를 Time Sharing(시분할)이라고 한다. 이 때 기존에 실행되던 process의 상태를 disk에 저장하게 되는데, disk에 저장한다는 것은 결국 I/O가 발생한다는 것이므로, 실행 속도가 느릴 수 밖에 없었다. 또한 다른 process의 memory 공간에 침범을 할 위험성도 존재했다. 즉, process 간 isolation이 제대로 보장되기 힘들었다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-10-05-Address-Memory/02.png&quot; alt=&quot;02.png&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;virtual-memory&quot;&gt;Virtual Memory&lt;/h2&gt;

&lt;p&gt;이러한 문제를 해결하기 위해 memory 가상화 개념이 도입되었다. 각 process마다 가상의 memory 주소 체계를 도입해 실제 물리 memory의 공간과 mapping을 시키는 것이다. 이 과정에서 page table, MMU가 등장하게 된다. page table은 가상 memory의 공간을 page 단위로 나눠 물리 memory의 주소와 mapping시키는 table이다. 이 과정을 Memory Management Unit (MMU)가 수행하게 되는데, MMU는 과거에는 OS에 구현이 되어있었으나, 현대의 대부분의 CPU는 HW로 MMU를 탑재하고 있다. 가상 memory 공간은 각 process마다 별개로 보유하고 있다. page table 역시 마찬가지이다. 따라서 서로 다른 process들의 가상 memory 상에서의 주소가 같다고 하더라도 실제 물리 memory에서의 주소는 다른 page를 가리키게 된다. 이러한 방식으로 process간 isolation을 보장하게 된다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-10-05-Address-Memory/03.png&quot; alt=&quot;03.png&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;memory-management&quot;&gt;Memory Management&lt;/h1&gt;

&lt;h2 id=&quot;memory-management의-목적&quot;&gt;Memory Management의 목적&lt;/h2&gt;

&lt;p&gt;Memory Management는 아래의 3가지 목적을 갖는다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;투명성 Transparency&lt;/p&gt;

    &lt;p&gt;가상화와 같은 구체적인 방법을 모르는 상태에서도 원활하게 사용할 수 있도록 하는 것이다. 실행되는 process 입장에서는 자신의 memory 체계가 가상 memory인지조차도 깨닫지 못하게 한다. 이는 가상화의 정의(속임)에도 부합한다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;효율성 Efficiency&lt;/p&gt;

    &lt;p&gt;시간 복잡도와 공간 복잡도를 낮추는 것이다. 특히 공간 복잡도를 중요시 여기는데, 만약 page table의 크기가 너무 클 경우 memory를 많이 사용하게 되므로 공간 복잡도에서 효율적이지 못하다고 할 수 있을 것이다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;보호 Protection&lt;/p&gt;

    &lt;p&gt;process 간 isolation을 보장하는 것과 OS kernel의 isolation을 보장하는 것이다. 가상 memory를 통해 달성할 수 있다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;memory-management의-issue&quot;&gt;Memory Management의 Issue&lt;/h2&gt;

&lt;p&gt;Memory Management에는 아래와 같은 주요 Issue가 있다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;배치 Placement&lt;/p&gt;

    &lt;p&gt;memory의 여러 빈 공간 중 어느 곳에 새로운 process를 삽입할 것인가에 대한 issue이다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;재배치 Replacement&lt;/p&gt;

    &lt;p&gt;memory에 올라가 있던 process가 종료되어 빈 공간이 생길 때, 이러한 빈 공간을 어떻게 최소화할 지에 대한 issue이다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;보호 Protection&lt;/p&gt;

    &lt;p&gt;memory 상의 여러 process 및 OS가 어떻게 서로의 공간을 침범하지 않을지에 대한 issue이다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;반입 loading&lt;/p&gt;

    &lt;p&gt;요청을 하는 시점에 반입을 할 지(on-demand loading), 요청이 올 시점을 예측해 반입을 할 지에 대한 issue이다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;memory-management의-종류&quot;&gt;Memory Management의 종류&lt;/h2&gt;

&lt;h3 id=&quot;단일-사용자-전용-시스템-single-user-system&quot;&gt;단일 사용자 전용 시스템 Single User System&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-10-05-Address-Memory/04.png&quot; alt=&quot;04.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;memory를 동시에 점유할 수 있는 process의 수가 최대 1개이기 때문에 배치, 재배치를 고려할 필요가 없다. 만약 주어진 memory보다 더 큰 process가 들어오게 될 경우, overlay 기법을 사용하게 된다.&lt;/p&gt;

&lt;h3 id=&quot;고정-분할-다중-프로그래밍-시스템-fixed-partition-multi-programming-system&quot;&gt;고정 분할 다중 프로그래밍 시스템 Fixed Partition Multi Programming System&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;절대 번역 어셈블러 (absolute aseembler)&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/assets/images/2020-10-05-Address-Memory/05.png&quot; alt=&quot;05.png&quot; /&gt;&lt;/p&gt;

    &lt;p&gt;memory를 초기에 고정된 크기로 분할시킨 뒤에, 각각의 segment마다 독립적인 queue를 운용하는 것이다. 각 process는 compile time에 결정된 segment에 할당이 되기 때문에, 자신이 배정된 segment는 다른 process가 점유하고 있고, 다른 segment가 비어있는 상황에서도 자신의 segment가 비워질 때 까지 대기해야만 한다. 구현이 간단하다는 장점이 있지만 memory의 낭비가 심하다는 단점이 있다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;재배치 가능 어셈블러 (relocating assembler)&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/assets/images/2020-10-05-Address-Memory/06.png&quot; alt=&quot;06.png&quot; /&gt;&lt;/p&gt;

    &lt;p&gt;memory를 초기에 고정된 크기로 분할시키는 것은 동일하지만, 하나의 공통된 queue를 운용해 각각의 segment에 분배하는 것이다. 이를 통해 재배치가 가능해진다. 즉, compile time의 정적인 결정에 의존하지 않고 memory 내 segment들의 상태에 따라 동적으로 배치되는 것이다. 절대 번역 어셈블러에 비해 효율적이나 구현이 어렵다는 단점이 있다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;위의 2가지 배치 방법은 모두 다 고정적으로 memory를 분할하기 때문에 내부 단편화(internal fragmentation)현상이 발생할 수 밖에 없다 &lt;strong&gt;내부 단편화(internal fragmentation)&lt;/strong&gt;란, segment의 크기보다 더 작은 process가 들어오게 될 경우 해당 segment 내부에 빈 공간이 생기게 되는 현상이다. 각 segment마다 빈 공간이 발생하게 되고, 해당 공간들은 사용되지 못한다. 이는 결국 memory의 낭비가 심해짐을 뜻한다. 또한 동시에 memory에 올릴 수 있는 process(활성 process)의 개수가 정적으로 제한된다는 큰 단점 역시 존재한다.&lt;/p&gt;

&lt;h3 id=&quot;가변-분할-다중-프로그래밍-시스템-variable-partition-multi-programming-system&quot;&gt;가변 분할 다중 프로그래밍 시스템 Variable Partition Multi Programming System&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-10-05-Address-Memory/07.png&quot; alt=&quot;07.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;내부 단편화 문제를 해결하기 위해 가변 분할 다중 프로그래밍 시스템이 등장했다. 공통된 하나의 queue를 운용하는 방식은 동일하지만, memory를 초기에 segment로 분할시키지 않고 통합해 관리하는 것이다. 이는 내부 단편화 현상은 해결하지만, 외부 단편화(external fragmentation)현상이 발생하게 된다. &lt;strong&gt;외부 단편화(external fragmentation)&lt;/strong&gt;란, 하나의 process가 종료됐을 경우 해당 process가 차지하고 있던 공간에 새로운 process가 들어오지 못하고 계속 빈 상태로 유지되는 현상을 뜻한다. 만약 종료된 process가 10KB인데 queue에서 대기하고 있는 process들이 모두 다 10KB 이상일 경우에는 해당 공간은 계속 낭비되게 될 것이다. 이러한 현상을 해결하기 위해서는 compaction 또는 coalescing을 수행해야 한다. &lt;strong&gt;compaction&lt;/strong&gt;은 memory 전체에서 외부 단편화 현상을 제거하는 것이고, &lt;strong&gt;coalescing&lt;/strong&gt;은 특정 두 process 사이의 외부 단편화 현상을 제거하는 것이다.&lt;/p&gt;

&lt;h3 id=&quot;재배치-기법&quot;&gt;재배치 기법&lt;/h3&gt;

&lt;p&gt;위와 같이 여러 fragmentation이 발생했을 경우 여러 fragmentation 중 하나를 선택해 새로운 process를 삽입해야 한다. 물론 해당 framentation이 process의 크기보다 작을 경우 삽입이 불가능하겠지만, 가능한 후보 fragmentation이 여러 개라면 그 중 하나를 선택해야만 할 것이다. 이렇듯 재배치 상황에서 fragmentation을 선택하는 전략에는 아래의 4가지가 있다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;First Fit&lt;/p&gt;

    &lt;p&gt;memory의 처음부터 탐색해 알맞는 첫번째 후보를 선택하는 전략이다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Best Fit&lt;/p&gt;

    &lt;p&gt;memory의 처음부터 탐색해 가장 크기가 작은 후보(크기의 차이가 적은 후보)를 선택하는 전략이다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Worst Fit&lt;/p&gt;

    &lt;p&gt;memory의 처음부터 탐색해 가장 크기가 큰 후보를 선택하는 전략이다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Next Fit&lt;/p&gt;

    &lt;p&gt;memory의 처음부터가 아닌 직후부터 탐색해 알맞는 첫번째 후보를 선택하는 전략이다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;buddy-algorithm&quot;&gt;Buddy Algorithm&lt;/h2&gt;

&lt;p&gt;가변 분할 다중 프로그래밍 시스템은 외부 단편화 현상을 해결하기 위해 compaction, coalescing과 같은 작업을 지속적으로 수행해야 하는 단점이 존재한다. 외부 단편화 현상을 최소화하기 위한 Buddy Algorithm이 있다. 가변 분할 다중 프로그래밍 시스템의 일종인데, 기존의 시스템과의 차이점은 Release가 발생해 free block이 생성될 때 마다 근처의 여유 block과 merge를 수행한다는 것이다. 이를 통해 큰 크기의 block을 운용하고, 새로운 process가 들어올 때 해당 process의 크기에 가장 적합한 크기가 될 때까지 이분할을 수행해 내부 파편화 크기를 최소화한다. 동시에 free block을 큰 size로 묶어 운용하기 때문에 외부 파편화 발생 빈도도 현저히 줄어들게 된다. Buddy Algorithm은 주로 Red-Black Tree를 이용해 구현한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-10-05-Address-Memory/08.png&quot; alt=&quot;08.png&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;process-실행-과정&quot;&gt;Process 실행 과정&lt;/h1&gt;

&lt;p&gt;disk에는 실행 파일이 저장되어 있다. disk에 있는 파일이 메모리에 올라가 process가 된다. disk 상에서 실행 파일(.out, .exe 등)이 저장되는 format은 ELF (windows의 경우 PG) format이다. process가 memory에 올라갈 때와는 구조가 다른데, 아래와 같은 차이를 지닌다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-10-05-Address-Memory/09.png&quot; alt=&quot;09.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Process 구조&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-10-05-Address-Memory/10.png&quot; alt=&quot;10.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ELF format 구조&lt;/p&gt;

&lt;p&gt;disk에 저장되는 파일은 stack/heap을 가질 필요가 없다. stack/heap은 process가 실행될 때 활용하는 공간이기 때문이다. disk에 저장되는 파일은 disk block 단위로 나눠 저장이 되는데, 대개 그 크기는 4KB이다. 한편, memory에서도 page frame 단위로 나누어 보관을 하는데, 그 크기는 disk block의 것을 따른다. 따라서 disk와 memory에서의 process 실행 시의 구조는 아래와 같은 흐름을 따르게 된다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-10-05-Address-Memory/11.png&quot; alt=&quot;11.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;단순 프로그램 실행이 아닌 좀 더 복잡한 경우를 살펴보자. fork()를 호출한 경우이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-10-05-Address-Memory/12.png&quot; alt=&quot;12.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-10-05-Address-Memory/13.png&quot; alt=&quot;13.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;좌측은 fork() 호출 전, 우측은 fork() 호출 후이다. fork()는 부모 process와 자식 process가 동일한 code를 사용하기 때문에 text 영역은 서로 공유하게 된다(같은 물리 memory를 가리킨다). 반면 data와 stack 영역에 대해서는 물리 memory에 새로운 page frame을 생성해 자식 process에게 할당하게 된다. 이러한 방식은 여러 자식 process가 생성될 경우 memory 낭비가 심하다는 단점이 존재한다. 자식 process라고 하더라도 내부에서 지역 변수 및 전역 변수 등을 수정하지 않는 경우라면 굳이 새로운 page frame을 생성하지 않아도 되기 때문이다. 이러한 단점을 해결하기 위해 현대 OS는 대부분 &lt;strong&gt;COW (Copy On Write)&lt;/strong&gt; 방식을 채택하고 있다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-10-05-Address-Memory/14.png&quot; alt=&quot;14.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-10-05-Address-Memory/15.png&quot; alt=&quot;15.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;좌측은 COW 방식에서의 fork() 호출 직후이고, 우측은 fork() 호출 후 전역 변수에 대한 변경이 일어난 뒤이다. COW 방식에서는 fork()를 한다고 해서 바로 data 및 stack 영역에 대한 새로운 page frame을 생성하지 않고 우선 부모 process의 것을 공유한다. 이후 자식 process에서 값 변경이 발생한 시점에 새로운 page frame을 생성해 자식 process에게 할당하게 된다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-10-05-Address-Memory/16.png&quot; alt=&quot;16.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;exec()의 경우에는 fork()와는 다르게 동작하는데, exec()는 실행중이던 process의 모든 memory 영역을 교체하게 된다. 기존에 사용하던 page frame과의 연결을 끊고, exec() 호출 시 넘겨준 새로운 실행 파일을 disk에서 읽어들여 새로운 text page frame, data page frame, stack page frame을 생성해 연결한다.&lt;/p&gt;
</description>
        <pubDate>Sun, 04 Oct 2020 19:00:00 -0500</pubDate>
        <link>http://0.0.0.0:4000/operating%20system/Address-Memory/</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/operating%20system/Address-Memory/</guid>
        
        <category>Operating System</category>
        
        
        <category>Operating System</category>
        
      </item>
    
      <item>
        <title>[운영체제] Scheduling: MLFQ(Multi Level Feedback Queue)</title>
        <description>&lt;p&gt;숭실대학교 컴퓨터학부 홍지만 교수님의 2020-2학기 운영체제 강의를 정리 및 재구성했다.&lt;/p&gt;

&lt;h2 id=&quot;rr과-mlfq&quot;&gt;RR과 MLFQ&lt;/h2&gt;

&lt;p&gt;Round Robin 기법은 평균 응답 시간은 최소화시켰지만, 평균 반환 시간은 최악이라는 점에서 한계가 있었다. 물론 평균 응답 시간이 짧기 때문에 사용자가 속도가 빠른 시스템으로 인지하도록 착각을 유도할 수 있었다. 하지만 짧은 task에 대해서도 slicing을 수행해 여러 번 나눠 작업을 수행하며 오랜 시간이 지난 후에 최종적으로 작업이 종료되기에 짧은 task에 있어서 너무나 불리한 정책이었다. MLFQ는 이러한 RR의 한계점을 극복하기 위해 고안해낸 방법이다. RR의 장점인 짧은 평균 응답 시간은 유지하면서, RR의 단점인 짧은 task의 불리함을 해결하자는 것이다. RR에서는 Queue를 1개만 운용했다면, MLFQ는 다양한 Time Quantum을 가지는 여러 Queue를 동시에 운용한다. 각각의 Queue들은 RR의 Queue와 동일한 방식으로 작동한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-09-28-Scheduling-MLFQ/01.png&quot; alt=&quot;01.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;여러 개의 Queue는 상단에서 하단으로 내려갈수록 Time Quantum은 길어지고, 이는 당연히 process가 CPU를 점유했을 시 한 번에 실행되는 시간이 길어짐을 뜻한다. 반면 상단에서 하단 Queue로 내려갈수록 우선 순위는 떨어져 점유 가능성은 낮아진다. 더 정확히는, 하단에 있는 Queue가 Pop되려면 해당 Queue의 상단에 있는 모든 Queue가 empty 상태여야만 한다.&lt;/p&gt;

&lt;p&gt;SJF, STCF와 같은 정책을 다시 생각해보자. 짧은 시간을 갖는 task에 대해서 우선순위를 부여하려고 하는 방식이었다. 하지만 CPU의 입장에서 처음 마주하는 process가 들어올 때 해당 process의 총 작업 시간이 얼마인지는 알 수 있는 방법이 없다. 따라서 SJF와 STCF와 같은 정책은 현실에서는 적용하기 불가능하다. MLFQ는 짧은 task들이 불리한 RR의 단점을 해결하기 위한 것이 목적이었다. MLFQ는 process의 실행 시간을 알 수 있는 방법이 없기 때문에 어디까지나 process의 실행 시간을 유추하게 된다. 우선은 새로 들어오는 모든 process가 짧은 process라고 가정하고 가장 TQ가 짧은 최상단 Queue에 push한다. 만약 첫번째 Queue에서 process가 끝나지 않았다면 아래에 있는 Queue에 push하게 된다. 우선 순위를 낮추되, 한 번 점유하면 오래 실행되도록 하는 것이다. 이는 해당 process가 실행 시간이 좀 더 길 것이라고 유추한다는 의미이다. 만약 2번째 Queue에서도 process가 종료되지 않았다면 3번째 Queue에 push하게 된다. 3번째 Queue는 1,2번째 Queue가 empty일 때에만 pop이 되기 때문에 우선 순위는 많이 떨어지지만, 한 번 점유하게 되면 오랜 시간 실행되게 된다. 이렇듯 각 process에게 고정적인 우선순위를 부여하는 기존의 scheduling 정책과 반대로 MLFQ는 동적으로 우선순위를 결정하게 된다.&lt;/p&gt;

&lt;h2 id=&quot;mlfq-rule&quot;&gt;MLFQ Rule&lt;/h2&gt;

&lt;p&gt;MLFQ가 작동하는 규칙에 대해 자세하게 살펴보자. 다음과 같은 5개의 규칙을 따른다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;우선순위가 더 높은 (더 상단의 Queue에 위치한) task를 먼저 수행한다.&lt;/li&gt;
  &lt;li&gt;우선순위가 같은 (같은 Queue에 위치한) task 중에서는 먼저 들어온 task를 수행한다.&lt;/li&gt;
  &lt;li&gt;처음 실행되는 task에 대해서는 가장 높은 우선순위를 부여한다(최상단 Queue에 push한다).&lt;/li&gt;
  &lt;li&gt;TQ를 모두 소모하면, 우선순위가 낮아진다(아래의 Queue에 push한다).&lt;/li&gt;
  &lt;li&gt;TQ를 모두 소모하기 전에 CPU 점유를 해제하면, 같은 우선순위를 유지한다(같은 Queue에 push한다).&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;mlfq의-한계&quot;&gt;MLFQ의 한계&lt;/h2&gt;

&lt;p&gt;기본 상태의 MLFQ는 크게 3개의 한계점이 존재한다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;기아 Starvation 발생 가능성&lt;/p&gt;

    &lt;p&gt;실행 시간이 길어 우선순위가 낮아진 task는 계속 신규 task가 들어오게 될 경우 기아 상태에 빠지게 된다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;시간에 따라 process의 특성이 변하더라도 한 번 내려간 우선순위 조정 불가&lt;/p&gt;

    &lt;p&gt;I/O Bound(CPU보다 I/O 위주)인 task는 상단 Queue에 위치한다. 반면 CPU Bound인 task는 하단 Queue에 위치한다. 하지만 CPU Bound에서 I/O Bound로 process의 특성이 변경될 경우, 한 번 내려간 우선순위는 다시 올라가지 않는다. 따라서 해당 process는 제대로 실행되지 못한다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;무의미한 I/O를 빈번히 발생시켜 우선순위를 강제로 유지 (gaming)&lt;/p&gt;

    &lt;p&gt;Time Quantum이 끝나기 전에 CPU 점유를 해제하면 우선순위가 유지된다는 점을 악용해 매 Time Quantum이 끝나기 직전에 무의미한 I/O를 지속적으로 발생시키게 되면 해당 task는 계속 높은 우선순위를 유지하게 된다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;mlfq-개선&quot;&gt;MLFQ 개선&lt;/h2&gt;

&lt;p&gt;위의 한계들을 극복하기 위해 새로운 rule을 도입할 수 있다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;일정 시간 $S$가 지나면, 모든 task들의 우선순위를 초기화한다(모든 task들을 최상위 Queue에 push한다).&lt;/p&gt;

    &lt;p&gt;이러한 작업을 Boosting이라고 부른다. MLFQ의 1, 2번 문제를 해결할 수 있다. $S$는 hyperparameter로, heuristic하게 결정해야 하는 값이다. $S$를 너무 큰 값으로 지정하게 되면 Starvation이 여전히 발생하게 되고, 너무 작은 값으로 지정하게 되면 짧은 task(I/O Bound 또는 대화형 task)가 불리해진다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;CPU 사용 시간의 합이 TQ에 다다를 경우 우선순위를 낮춘다(아래의 Queue에 push한다).&lt;/p&gt;

    &lt;p&gt;해당 Queue 내에서 사용한 총 CPU time을 보관해 TQ에 다다르게 될 경우 그동안 CPU를 몇 번 양도했는지와 관계없이 우선 순위를 낮춰 다음 Queue에 push한다. 이를 통해 MLFQ의 3번 문제를 해결할 수 있다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;mlfq-tuning&quot;&gt;MLFQ Tuning&lt;/h2&gt;

&lt;p&gt;MLFQ에는 여러 hyperparameter들이 있다. 가장 대표적으로 Queue의 개수, 각 Queue의 Time Quantum, boosting에서의 $S$등이 있다. 이러한 값들은 모두 heuristic하게 결정해야 한다.&lt;/p&gt;
</description>
        <pubDate>Sun, 27 Sep 2020 19:00:00 -0500</pubDate>
        <link>http://0.0.0.0:4000/operating%20system/Scheduling-MLFQ/</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/operating%20system/Scheduling-MLFQ/</guid>
        
        <category>Operating System</category>
        
        
        <category>Operating System</category>
        
      </item>
    
      <item>
        <title>[운영체제] Scheduling</title>
        <description>&lt;p&gt;숭실대학교 컴퓨터학부 홍지만 교수님의 2020-2학기 운영체제 강의를 정리 및 재구성했다.&lt;/p&gt;

&lt;h2 id=&quot;단기-scheduling&quot;&gt;단기 Scheduling&lt;/h2&gt;

&lt;p&gt;Scheduling에는 여러 종류가 있다. 장기(long-term) scheduling은 process가 CPU에 의해 실행될 자격을 부여할 지를 결정하는 것이다. 중기(medium-term) scheduling은 process(의 일부)가 Memory에 올라갈 자격을 부여할 지를 결정하는 것이다. 단기(short-term) scheduling은 CPU에 실행될 다음 process를 선택하는 것으로, Dispatcher라고 불린다. 아래에서는 단기 scheduling에 대해서 다룬다.&lt;/p&gt;

&lt;h1 id=&quot;scheduling&quot;&gt;Scheduling&lt;/h1&gt;

&lt;p&gt;여러가지 Scheduling 기법에 대해 알아보자. 하지만 여기서 다루는 기법들은 어디까지나 Ideial한 가정 하에 성립하는 것으로 현실에 그대로 적용되기는 불가능에 가깝다. 앞으로 다루는 Scheduling들은 다음의 가정을 따른다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;모든 task는 동일한 시간 동안에 실행된다.&lt;/li&gt;
  &lt;li&gt;모든 task는 동일한 시간에 도착한다.&lt;/li&gt;
  &lt;li&gt;모든 task는 시작되면 완료될 때까지 실행된다.&lt;/li&gt;
  &lt;li&gt;모든 task는 CPU만 사용한다. (I/O 배제)&lt;/li&gt;
  &lt;li&gt;모든 task의 실행 시간은 알려져 있다.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;preemption-vs-non-preemption&quot;&gt;preemption vs non-preemption&lt;/h2&gt;

&lt;p&gt;Scheduling에 대해 알아보기 전에 우선 preemption에 대한 개념을 확실히 정의하고 가는 것이 좋다. preemption이란 이전에도 설명했듯이, 다른 process가 이미 차지하고 있던 CPU를 빼앗는 행위를 뜻한다. non-preemption이라는 것은 preemption의 반댓말(빼앗김)이 아닌, 빼앗기지 않는다는 의미이다. 다시 말해, 자신이 이미 CPU를 차지하고 있고 다른 process에게 넘겨주지 않는다는 것을 non-preemption이라고 한다.&lt;/p&gt;

&lt;h2 id=&quot;평가-기준&quot;&gt;평가 기준&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Turn around time (반환 시간)&lt;/p&gt;

\[T_{turn\_around} = T_{completion} - T_{arrival}\]
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Response time (응답 시간)&lt;/p&gt;

\[T_{response} = T_{start}-T_{arrival}\]
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Fairness&lt;/p&gt;

    &lt;p&gt;얼마나 고르게 여러 process에 cpu를 분배했느냐에 대한 척도이다. 성능과 상충되는 기준이다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;종류&quot;&gt;종류&lt;/h2&gt;

&lt;h3 id=&quot;first-in-first-out-fifo--first-come-first-service-fcfs&quot;&gt;First In First Out (FIFO) / First Come First Service (FCFS)&lt;/h3&gt;

&lt;p&gt;Queue를 이용한 non-preemption 방식이다. 대기 시간을 기준으로 Scheduling을 수행한다. 예시를 통해 이해해보자.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-09-21-Scheduling/01.png&quot; alt=&quot;01.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 5개의 process가 있다고 가정해보자. Arrival Time과 Service Time이 주어졌다. FIFO Scheduling을 수행하면 아래와 같이 실행되게 된다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-09-21-Scheduling/02.png&quot; alt=&quot;02.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;평균 반환 시간을 계산해보자.&lt;/p&gt;

\[A's\ T_{arround\_time} : 3-0 = 3\]

\[B's\ T_{arround\_time} : 9-2 = 7\]

\[C's\ T_{arround\_time} : 13-4 = 9\]

\[D's\ T_{arround\_time} : 18-6 = 12\]

\[E's\ T_{arround\_time} : 20-8 = 12\]

\[Avr\ T_{arround\_time}=\frac{3+7+9+12+12}{5}\\=\frac{43}{5}=8.6\]

&lt;p&gt;평균 응답 시간을 계산해보자.&lt;/p&gt;

\[A's\ T_{response\_time} : 0-0 = 0\]

\[B's\ T_{response\_time} : 3-2=1\]

\[C's\ T_{response\_time} : 9-4=5\]

\[D's\ T_{response\_time} : 13-6=7\]

\[E's\ T_{response\_time} : 18-8=10\]

\[Avr\ T_{arround\_time}=\frac{0+1+5+7+10}{5}\\=\frac{23}{5}=4.6\]

&lt;p&gt;FIFO 방식은 Convoy Effect (홍위병 효과)라는 치명적인 단점이 존재한다. E를 예시로 들어볼 수 있는데, 실행 시간이 2로 매우 짧음에도 불구하고 늦게 들어왔다는 이유만으로 가장 나중에 실행되어 평균 반환, 응답 시간이 길어지게 되었다.&lt;/p&gt;

&lt;p&gt;실생활에서의 예시를 들어보자. 편의점에 손님이 많아 카운터의 줄이 긴 상태이다. 자신은 음료수 하나만을 구매하려고 줄을 섰는데 바로 앞의 사람은 무려 10만원 어치의 상품을 결제 중이어서 한참을 기다리는 상황이다. “나는 10초면 계산이 끝나는데 먼저 양보해줬으면 좋겠다”는 생각이 들 것이다. 이처럼 매우 짧은 시간이 소요되는 task임에도 조금이라도 늦게 시작됐다는 이유만으로 매우 긴 시간 대기하는 상황을 Convoy Effect라고 한다.&lt;/p&gt;

&lt;h3 id=&quot;shortest-job-first-sjf--shortest-process-next-spn&quot;&gt;Shortest Job First (SJF) / Shortest Process Next (SPN)&lt;/h3&gt;

&lt;p&gt;SJF 방식은 FIFO와 동일하게 non-preemption  방식이다. 그런데 먼저 들어온 순서보다는 task의 소요 시간을 기준으로 Scheduling을 수행한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-09-21-Scheduling/03.png&quot; alt=&quot;03.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;평균 반환 시간을 계산해보자.&lt;/p&gt;

\[A's\ T_{arround\_time} : 3-0 = 3\]

\[B's\ T_{arround\_time} : 9-2 = 7\]

\[C's\ T_{arround\_time} : 15-4 = 11\]

\[D's\ T_{arround\_time} : 20-6 = 14\]

\[E's\ T_{arround\_time} : 11-8 = 3\]

\[Avr\ T_{arround\_time}=\frac{3+7+11+14+3}{5}\\=\frac{38}{5}=7.6\]

&lt;p&gt;평균 응답 시간을 계산해보자.&lt;/p&gt;

\[A's\ T_{response\_time} : 0-0 = 0\]

\[B's\ T_{response\_time} : 3-2=1\]

\[C's\ T_{response\_time} : 11-4=7\]

\[D's\ T_{response\_time} : 15-6=9\]

\[E's\ T_{response\_time} : 9-8=1\]

\[Avr\ T_{arround\_time}=\frac{0+1+7+9+1}{5}\\=\frac{18}{5}=3.6\]

&lt;p&gt;평균 응답, 반환 시간이 FIFO에 비해 줄어든 것을 확인할 수 있다. Convey Effect를 해결한 것이다. 구체적인 예시로, B가 끝난 직후의 상황에서 C나 D 대신 가장 실행 시간이 짧은 E가 선택됨으로써 E의 평균 반환, 응답 시간이 FIFO에 비해 획기적으로 줄어들게 되었다.&lt;/p&gt;

&lt;p&gt;하지만 SJF scheduling도 Starvation(기아) Effect가 발생할 수 있는데, 이 역시 실생활의 예시로 이해해보자. 아까의 편의점 상황을 다시 떠올려 보자. 대신 이번에는 내가 10만원 어치의 구매를 한 고객이다. 계산대 앞에 섰으나 뒤에 금방 계산이 끝나는 사람이 있기에 먼저 양보를 했다. 그러나 그 뒤에 사람들도 모두 자신 역시 금방 계산이 끝난다며 먼저 계산을 하게 되었다. 결국 자신은 여러 사람의 계산이 모두 끝나기를 한참이나 기다린 뒤에 본인의 계산을 할 수 있게 되었다. 이처럼 짧은 시간이 소요되는 task들을 모두 기다리다가 무한정 기다리게 되는 상황을 Starvation Effect라고 한다.&lt;/p&gt;

&lt;h3 id=&quot;shortest-time-to-completion-first-stcf--shortest-remaining-time-srt&quot;&gt;Shortest Time-to-Completion First (STCF) / Shortest Remaining Time (SRT)&lt;/h3&gt;

&lt;p&gt;STCF는 위의 scheduling들과는 달리 preemption 방식이다. 즉, 이미 작업을 수행하던 process가 중단될 수 있다는 것이다. 새로운 process가 들어오게 될 경우, 현재 수행 중인 process의 잔여 시간보다 새로 들어온 process의 수행 시간이 짧을 경우에 preemption을 하게 된다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-09-21-Scheduling/04.png&quot; alt=&quot;04.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;평균 반환 시간을 계산해보자.&lt;/p&gt;

\[A's\ T_{arround\_time} : 3-0 = 3\]

\[B's\ T_{arround\_time} : 15-2 = 13\]

\[C's\ T_{arround\_time} : 8-4 = 4\]

\[D's\ T_{arround\_time} : 20-6 = 14\]

\[E's\ T_{arround\_time} : 10-8 = 2\]

\[Avr\ T_{arround\_time}=\frac{3+13+4+14+2}{5}\\=\frac{36}{5}=7.2\]

&lt;p&gt;평균 응답 시간을 계산해보자.&lt;/p&gt;

\[A's\ T_{response\_time} : 0-0 = 0\]

\[B's\ T_{response\_time} : 3-2=1\]

\[C's\ T_{response\_time} : 4-4=0\]

\[D's\ T_{response\_time} : 15-6=9\]

\[E's\ T_{response\_time} : 8-8=0\]

\[Avr\ T_{arround\_time}=\frac{0+1+0+9+0}{5}\\=\frac{10}{5}=2\]

&lt;p&gt;C가 가장 큰 혜택을 보게 되었다. 4time이 지난 시점에 C가 새로 들어오게 되는데, 이미 수행중이던 B의 잔여 시간은 5초인 반면 C의 전체 수행 시간은 4초이기에 수행 중이던 B에게서 preemption을 해 C가 수행되게 된다. 하지만 STCF 역시 Starvation Effect를 해결하지는 못한다. 전체 수행 시간이 긴 process의 경우 계속 새로 들어온 작업들에 밀려 수행될 수 없게 된다.&lt;/p&gt;

&lt;h3 id=&quot;highest-response-ratio-next-hrrn&quot;&gt;Highest Response Ratio Next (HRRN)&lt;/h3&gt;

&lt;p&gt;HRNN은 다시 non-preemption 방식이다. 대신 이전의 일률적인 기준 (FIFO: 대기 시간, SJF: 수행 시간)이 아닌 두 기준을 결합한 수식을 사용하기로 했다.&lt;/p&gt;

\[(T_{wait}+T_{service})/T_{service}\]

&lt;p&gt;대기 시간과 실행 시간을 더한 값을 대기 시간으로 나눈 값이 큰 process를 먼저 선택한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-09-21-Scheduling/05.png&quot; alt=&quot;05.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;평균 반환 시간을 계산해보자.&lt;/p&gt;

\[A's\ T_{arround\_time} : 3-0 = 3\]

\[B's\ T_{arround\_time} : 9-2 = 11\]

\[C's\ T_{arround\_time} : 13-4 = 9\]

\[D's\ T_{arround\_time} : 20-6 = 14\]

\[E's\ T_{arround\_time} : 15-8 = 7\]

\[Avr\ T_{arround\_time}=\frac{3+11+9+14+7}{5}\\=\frac{44}{5}=8.8\]

&lt;p&gt;평균 응답 시간을 계산해보자.&lt;/p&gt;

\[A's\ T_{response\_time} : 0-0 = 0\]

\[B's\ T_{response\_time} : 3-2=1\]

\[C's\ T_{response\_time} : 9-4=0\]

\[D's\ T_{response\_time} : 15-6=9\]

\[E's\ T_{response\_time} : 13-8=5\]

\[Avr\ T_{arround\_time}=\frac{0+1+0+9+5}{5}\\=\frac{15}{5}=3\]

&lt;p&gt;B가 수행이 끝난 9time 직후를 보면 C, D, E 중 선택을 하는 상황이다. 각각의 time을 계산해보자.&lt;/p&gt;

\[C: \frac{(9-4)+4}{4}=\frac{9}{4}=2.25\]

\[D: \frac{(9-6)+5}{5}=\frac{8}{5}=1.6\]

\[E: \frac{(9-8)+2}{2}=\frac{3}{2}=1.5\]

&lt;p&gt;이 중 가장 큰 값인 C를 선택하게 된다. SJF에서는  C가 E에 밀려 더 나중에 수행됐다면, 여기서는 C가 E보다 대기 시간에서의 보정을 받아 더 먼저 수행된 것이다.&lt;/p&gt;

&lt;h3 id=&quot;round-robin-rr--time-slicing&quot;&gt;Round Robin (RR) / Time Slicing&lt;/h3&gt;

&lt;p&gt;preemption 방식이다. 일정 시간마다 서로 돌아가면서 공정하게 scheduling을 하고자 함이다. 이 때 들어온 순서대로 Queue에 넣고, Queue에서 pop을 해 새로운 process를 선택한다. 만약 수행이 끝났음에도 아직 남아있는 작업이 있다면 다시 Queue에 삽입한다. 가장 중요한 변수는 Time Slicing의 길이인데 만약 무한대에 수렴할 경우, non-preemption과 다를 바 없어지고, 0에 수렴할 경우 context switch 수행 횟수가 급격히 늘어나 overhead가 발생할 것이다. 아래의 예시는 Time Slicing의 길이가 1인 경우이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-09-21-Scheduling/06.png&quot; alt=&quot;06.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;평균 반환 시간을 계산해보자.&lt;/p&gt;

\[A's\ T_{arround\_time} : 4-0 = 4\]

\[B's\ T_{arround\_time} : 18-2 = 16\]

\[C's\ T_{arround\_time} : 17-4 = 13\]

\[D's\ T_{arround\_time} : 20-6 = 14\]

\[E's\ T_{arround\_time} : 15-8 = 7\]

\[Avr\ T_{arround\_time}=\frac{4+16+13+14+7}{5}\\=\frac{54}{5}=10.8\]

&lt;p&gt;평균 응답 시간을 계산해보자.&lt;/p&gt;

\[A's\ T_{response\_time} : 0-0 = 0\]

\[B's\ T_{response\_time} : 2-2=0\]

\[C's\ T_{response\_time} : 5-4=1\]

\[D's\ T_{response\_time} : 7-6=1\]

\[E's\ T_{response\_time} : 10-8=2\]

\[Avr\ T_{arround\_time}=\frac{0+0+1+1+2}{5}\\=\frac{4}{5}=0.8\]

&lt;h3 id=&quot;mlfq-multi-level-feedback-queue&quot;&gt;MLFQ (Multi-level Feedback Queue)&lt;/h3&gt;

&lt;p&gt;RR에서 좀 더 발전해 Time Slicing의 길이가 다른 Queue를 여러 개 운용하는 것이다.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://cpm0722.github.io/operating%20system/Scheduling-MLFQ/&quot;&gt;Scheduling: MLFQ(Multi Level Feedback Queue)&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;incorporating-i-o&quot;&gt;Incorporating I /O&lt;/h3&gt;

&lt;p&gt;I/O를 고려하는 scheduling이다. I/O 수행 중에는 CPU를 사용하지 않는다. 따라서 해당 기간 동안에 CPU를 휴식시키지 않고, 다른 process를 수행하도록 하는 것이다.&lt;/p&gt;

&lt;h3 id=&quot;성능-비교&quot;&gt;성능 비교&lt;/h3&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Scheduling&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;선택 함수&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;preemption&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;응답 시간&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;문맥 교환 비용&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;유리한 process&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Starvation&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;FIFO/FCFS&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;max[w]&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;non-preemption&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;길 수 있음&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;최소&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;연산 많은 process&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;X&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;SJF/SPN&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;min[s]&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;non-preemption&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;연산 적을수록 짧음&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;커질 수 있음&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;짧은 process&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;O&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;STCF/SRT&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;min[s-e]&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;preemption&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;짧음&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;커질 수 있음&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;연산 적은 process&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;O&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;HRRN&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;max((w+s)/s)&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;non-preemption&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;짧음&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;커질 수 있음&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;다소 공정&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;X&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;RR&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;상수&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;preemption&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;연산 적을수록 짧음&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;최소&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;모두에게 공정&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;X&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

</description>
        <pubDate>Sun, 20 Sep 2020 19:00:00 -0500</pubDate>
        <link>http://0.0.0.0:4000/operating%20system/Scheduling/</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/operating%20system/Scheduling/</guid>
        
        <category>Operating System</category>
        
        
        <category>Operating System</category>
        
      </item>
    
      <item>
        <title>[NLP 논문 리뷰] Subword-level Word Vector Representation for Korean</title>
        <description>&lt;h2 id=&quot;paper-info&quot;&gt;Paper Info&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://www.aclweb.org/anthology/P18-1226/&quot;&gt;Archive Link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.aclweb.org/anthology/P18-1226.pdf&quot;&gt;Paper Link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Submit Date: Jul 1, 2018&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;

&lt;p&gt;지금까지의 word representation에 관한 연구는 모두 영어에 집중되어 왔다. language-specific knowledge는 언어 학습에 있어서 매우 결정적인 요소인데 영어와 기원이 다른 여러 다양한 언어들에 대해서는 이러한 연구가 부진했던 것이 사실이다. 본 논문에서는 한국어만의 unique한 언어 구조를 분석해 NLP에 적용해보고자 했다. 구체적으로, Korean에만 존재하는 ‘jamo’ 개념을 도입해 character level에서 더 깊이 들어간 ‘jamo’ 단위로 단어를 분해해 사용했다. 동시에 여러 task에 대해 측정  가능한 한국어 test set도 제안했다. 본 논문에서 제안하는 방식은 word2vec이나 character-level Skip-Grams을 semantic, syntatic 모두에서 능가했다.&lt;/p&gt;

&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;기존의 word representation은 모두 영어 위주였기 때문에 다양한 형태를 가진 한국어에 대해서는 제대로 적용할 수 없었다. 단어를 n-gram set으로 분해하는 방식은 여러 language에서 효과적이긴 했으나 해당 언어만의 unique한 linguistic structure를 무시한다는 단점이 있었다. 이를 해결하기 위해 word vector를 학습하는 과정에서부터 language-specific한 구조를 활용하고자 하는 연구는 활발히 진행되어 왔다. 한국어의 경우 character-level로 단어를 분해하는 연구는 있어왔으나 한국어의 character는 자음과 모음으로 분해가 가능하다는 점에서 다소 부족한 부분이 있었다. 본 논문에서는 ‘jamo’ 단위로까지 분해해 word를 subword로 분해하는 방식을 제안하고, 한국어 원어민들에게서 얻어낸 정확도 높은 한국어 test dataset을 제안하고자 한다.&lt;/p&gt;

&lt;h1 id=&quot;related-work&quot;&gt;Related Work&lt;/h1&gt;

&lt;h2 id=&quot;language-specific-features-for-nlp&quot;&gt;Language-specific features for NLP&lt;/h2&gt;

&lt;p&gt;다양한 언어들의 각각의 고유한 특징은 여러 언어들에 대한 universal model을 개발하는데 큰 장애물이다. universal model은 한국어와 같은 교착어(어근+접사의 결합으로 구성되는 언어)에서는 특히나 좋은 성능을 내지 못했다. 언어 자체의 고유한 구조가 문법과 강력하게 연결되어 있기 때문이다. 한국어에 관련된 기존의 NLP 연구들은 교착어로써의 특성을 반영하고자 노력해왔다. word embedding 이후 ‘Josa’에 대해 특별한 labeling을 부여하거나, ‘jamo’ 단위로 word를 분해해 형태소의 변형을 찾아내려는 시도가 있었다.&lt;/p&gt;

&lt;h2 id=&quot;subword-features-for-nlp&quot;&gt;Subword features for NLP&lt;/h2&gt;

&lt;p&gt;character-level의 subword features 방식은 여러 NLP task에서 성능 향상에 많은 기여를 했다. 특히나 character n-gram model은 sparsity 문제에서 상대적으로 자유롭기 때문에 작은 dataset에서 좋은 성능을 보였다.&lt;/p&gt;

&lt;h1 id=&quot;model&quot;&gt;Model&lt;/h1&gt;

&lt;p&gt;우선 word를 ‘jamo’ 단위로 word를 분해한 뒤, 얻은 ‘jamo’ sequence에서 n-gram을 사용해 word vector를 생성하는 방식을 제안한다.&lt;/p&gt;

&lt;h2 id=&quot;decomposition-of-korean-words&quot;&gt;Decomposition of Korean Words&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-09-20-Subword-level-Word-Vector-Representation-for-Korean/01.jpg&quot; alt=&quot;01.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;우선 한국어 word를 ‘jamo’ 단위로 분해하는 것에 대해서 살펴보자. 영어와는 달리 한국어는 자음과 모음의 규칙이 엄격하다. (영어의 straight을 생각해보자. 모음 a 뒤에 모음 i가 연속해서 등장한다.) 한국어의 character는 영어의 음절과 비슷한 개념이며, 이는 3개의 ‘jamo’ {1. 초성(‘chosung’):자음, 2. 중성(‘joongsung’):모음, 3. 종성( ‘jongsung’):자음}로 구성된다.  ‘joongsung’에 대해서는 예외적으로 없을 수도 있는데, 이 때에는 새로운 symbol \(e\)를 사용했다. 아래는 한국어 ‘해’와 ‘달’에 대한 예시이다.&lt;/p&gt;

\['해'=\{ㅎ,ㅐ,e\},\ '달'=\{ㄷ,ㅏ,ㄹ\}\]

&lt;p&gt;이러한 representation을 이용하면 \(N\)개의 한국어 character는 \(3 * N\)개의 ‘jamo’로 구성된다고 보장 가능하다. word에 대해서도 시작과 끝에 symbol \(\lt\)와 \(\gt\)를 추가했다. 따라서 아래는 한국어 ‘강아지’에 대한 예시이다.&lt;/p&gt;

\['강아지'=\{\lt,ㄱ,ㅏ,ㅇ,ㅇ,ㅏ,e,ㅈ,ㅣ,e,\gt\}\]

&lt;h2 id=&quot;extracting-n-grams-from-jamo-sequence&quot;&gt;Extracting N-grams from jamo Sequence&lt;/h2&gt;

&lt;h3 id=&quot;character-level-n-grams&quot;&gt;Character-level n-grams&lt;/h3&gt;

&lt;p&gt;한국어의 ‘먹었다’ 라는 word를 예시로 n-gram을 추출해보자. 우선 character-level에서는 다음과 같은 3개의 unigram을 얻을 수 있다.&lt;/p&gt;

\[\{ㅁ,ㅓ,ㄱ\},\{ㅇ,ㅓ,ㅆ\},\{ㄷ,ㅏ,e\}\]

&lt;p&gt;2개의 bigram을 얻을 수 있다.&lt;/p&gt;

\[\{ㅁ,ㅓ,ㄱ,ㅇ,ㅓ,ㅆ\},\{ㅇ,ㅓ,ㅆ,ㄷ,ㅏ,e\}\]

&lt;p&gt;1개의 trigram을 얻을 수 있다.&lt;/p&gt;

\[\{ㅁ,ㅓ,ㄱ,ㅇ,ㅓ,ㅆ,ㄷ,ㅏ,e\}\]

&lt;h3 id=&quot;inter-character-jamo-level-n-grams&quot;&gt;Inter-character jamo-level n-grams&lt;/h3&gt;

&lt;p&gt;한국어의 character는 character마다 독립적이지 않고, 이전 character의 영향을 강하게 받는다. 대표적인 예로 조사 ‘이’, ‘가’를 들 수 있다. 두 조사는 semantic에서는 완전히 동일하지만, 직전 character가 종성이 있을 경우 ‘이’를, 종성이 없을 경우 ‘가’를 사용해야만 한다. 이러한 제약을 반영하기 위해 ‘jamo’-level의 n-gram은 인접한 character들까지 통합하도록 했다. 이러한 방식을 통해 다음과 같은 ‘jamo’-level trigram을 얻을 수 있다.&lt;/p&gt;

\[\{\lt,ㅁ,ㅓ\},\{ㅓ,ㄱ,ㅇ\},\{ㄱ,ㅇ,ㅓ\},\{ㅆ,ㄷ,ㅏ\},\{ㅓ,ㅆ,ㄷ\},\{ㅏ,e,\gt\}\]

&lt;h2 id=&quot;subword-information-skip-gram&quot;&gt;Subword Information Skip-Gram&lt;/h2&gt;

&lt;h1 id=&quot;experiments&quot;&gt;Experiments&lt;/h1&gt;

&lt;h2 id=&quot;corpus&quot;&gt;Corpus&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-09-20-Subword-level-Word-Vector-Representation-for-Korean/02.jpg&quot; alt=&quot;02.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;한국어 Wikipedia, 뉴스 기사, Sejong Corpus에서 corpus를 수집했다. 0.12 billion개의 token과 638,708개의 unique words를 얻었는데, 이 중 10번 미만 등장한 corpus는 제외했다.&lt;/p&gt;

&lt;h3 id=&quot;korean-wikipedia&quot;&gt;Korean Wikipedia&lt;/h3&gt;

&lt;p&gt;0.4million개의 기사에서 3.3million개의 sentence와 43.4million개의 word를 얻었다.&lt;/p&gt;

&lt;h3 id=&quot;online-news-articles&quot;&gt;Online News Articles&lt;/h3&gt;

&lt;p&gt;5개의 major 신문사의 사회, 정치, 경제, 국제, 문화, IT의 6가지 분야에서 기사를 수집했다. 2017년 9월~11월의 기사들을 사용했다. 3.2million개의 sentence와 47.1million개의 word를 얻었다.&lt;/p&gt;

&lt;h3 id=&quot;sejong-corpus&quot;&gt;Sejong Corpus&lt;/h3&gt;

&lt;p&gt;Sejong Corpus는 1998년~2007년 사이의 뉴스 기사, 사전, 소설 등의 formal text와 TV, 라디오의 대본 등의 informal text에서 추출한 corpus이다. Wikipedia나 New Article에서 얻을 수 없는 corpus를 얻어낼 수 있었다.&lt;/p&gt;

&lt;h2 id=&quot;evaluation-tasks-and-datasets&quot;&gt;Evaluation Tasks and Datasets&lt;/h2&gt;

&lt;p&gt;similarity task와 analogy task를 통해 word vector의 성능을 측정하고자 했다. 하지만 각각의 task에 대한 한국어 evaluation dataset이 존재하지 않아 evaluation dataset을 개발해 사용했다. 동시에 감정 분석 downstream task를 진행했다.&lt;/p&gt;

&lt;h3 id=&quot;word-similarity-evaluation-dataset&quot;&gt;Word Similarity Evaluation Dataset&lt;/h3&gt;

&lt;p&gt;한국어를 모국어로 사용하는 학생 두 명이 353개의 영어 단어 쌍을 번역한다. 353개의 한국어 단어쌍이 생성된다. 이후 다른 14명의 한국인이 한국어 단어 쌍에 대해 0~10 사이의 유사도 점수를 매긴다. 각 단어 쌍에 매겨진 점수 중에서 최대, 최소 점수를 제외하고 평균을 매긴다. 영어 단어 쌍과 한국어 단어 쌍 사이의 상관계수는 0.82로 매우 유사했다.&lt;/p&gt;

&lt;h3 id=&quot;word-analogy-evaluation-dataset&quot;&gt;Word Analogy Evaluation Dataset&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;Semantic 차원
    &lt;ol&gt;
      &lt;li&gt;
        &lt;p&gt;수도-국가 (Captial-Country)&lt;/p&gt;

        &lt;p&gt;ex) 아테네 : 그리스 = 바그다드 : 이라크&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;남성-여성 (Male-Female)&lt;/p&gt;

        &lt;p&gt;ex) 왕자 : 공주 = 신사 : 숙녀&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;이름-국적 (Name-Nationality)&lt;/p&gt;

        &lt;p&gt;ex) 간디 : 인도 = 링컨 : 미국&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;국가-언어 (Country-Language)&lt;/p&gt;

        &lt;p&gt;ex) 아르헨티나 : 스페인어 = 미국어 : 영어&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;기타 (Miscellaneous)&lt;/p&gt;

        &lt;p&gt;ex) 개구리 : 욜챙이 = 말 : 망아지&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;Syntactic 차원
    &lt;ol&gt;
      &lt;li&gt;
        &lt;p&gt;격 (case)&lt;/p&gt;

        &lt;p&gt;ex) 교수 : 교수가 = 축구 : 축구가&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;시제 (tense)&lt;/p&gt;

        &lt;p&gt;ex) 싸우다 : 싸웠다 = 오다 : 왔다&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;태 (voice)&lt;/p&gt;

        &lt;p&gt;ex) 팔았다 : 팔렸다 = 평가했다 : 평가됐다&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;동사 변형 (verb form)&lt;/p&gt;

        &lt;p&gt;ex) 가다 : 가고 = 쓰다 : 쓰고&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;높임법 (honorific)&lt;/p&gt;

        &lt;p&gt;ex) 도왔다 : 도우셨다 = 됐다 : 되셨다&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;sentiment-analysis&quot;&gt;Sentiment Analysis&lt;/h3&gt;

&lt;p&gt;네이버 영화 감정 분석 dataset을 사용했다. Classifier로 300개의 hidden layer, dropout=0.5의 LSTM을 사용했다.&lt;/p&gt;

&lt;h3 id=&quot;comparison-models&quot;&gt;Comparison Models&lt;/h3&gt;

&lt;p&gt;모든 model의 training epoch=5, #negative samples=5, window size=5, #dimension=300으로 동일하다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Skip-Gram (SG)&lt;/p&gt;

    &lt;p&gt;word-level Skip Gram이다. 모든 unique word에 대해서 unique vector가 부여됐다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Character-level Skip-Gram (SISG(ch))&lt;/p&gt;

    &lt;p&gt;character-level n-gram이다. n=2-4이다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Jamo-level Skip-Gram with Empty Jongsung Symbol (SISG(jm))&lt;/p&gt;

    &lt;p&gt;‘jamo’-level의 n-gram이다. 비어 있는 종성 symbol \(e\)를 추가했다. n=3-6이다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;results&quot;&gt;Results&lt;/h1&gt;

&lt;h3 id=&quot;word-similarity&quot;&gt;Word Similarity&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-09-20-Subword-level-Word-Vector-Representation-for-Korean/03.jpg&quot; alt=&quot;03.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;단어 유사성에 대해 인간의 판단과 model의 cosine 유사도에 대해서 스피어만 상관 계수를 분석한다. word-level skip-gram인 SG보다 character n-gram을 적용한 SISG가 훨씬 더 좋은 성능을 보였다. ‘jamo’-level로 더 깊게 분해한 model이 가장 좋은 성능을 보였다.&lt;/p&gt;

&lt;h3 id=&quot;word-analogy&quot;&gt;Word Analogy&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-09-20-Subword-level-Word-Vector-Representation-for-Korean/04.jpg&quot; alt=&quot;04.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;a:b=c:d의 4개의 단어가 주어진다. 왕:왕비 = 남자:여자 와 같은 형태이다. 여기서 a + b - c와 d 사이의 cosine 유사도를 구한다.&lt;/p&gt;

&lt;h3 id=&quot;sentiment-analysis-1&quot;&gt;Sentiment Analysis&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-09-20-Subword-level-Word-Vector-Representation-for-Korean/05.jpg&quot; alt=&quot;05.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;word-level skip-gram보다 character-level, ‘jamo’-level이 더 좋은 성능을 보였다. 하지만 word-level은 F1 Score에서 본 논문에서 제시한 model보다는 낮지만, character-level, ‘jamo’-level보다 더 좋은 수치를 보였다. 이는 영화 리뷰라는 dataset의 특성 상 고유 명사가 많이 등장하는데, word-level이 고유 명사를 더 잘 잡아내기 때문으로 추측할 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;effect-of-size-n-in-both-n-grams&quot;&gt;Effect of Size n in both n-grams&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-09-20-Subword-level-Word-Vector-Representation-for-Korean/06.jpg&quot; alt=&quot;06.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;‘jamo’-level에서의 n은 증가할수록 대체로 더 좋은 성능을 보여주지만, character-level에서의 n은 그렇지 않다. 이는 한국어의 특성에서 기인하는데, 대부분의 한국어 word는 6자 이하(97.2%)이기 때문에, n=6은 과도하게 큰 값이다. 4자 이하의 word는 전체 한국어 word의 82.6%를 차지하기 때문에 n=4로도 충분하다고 볼 수 있다.&lt;/p&gt;

&lt;h1 id=&quot;conclusion-and-discussions&quot;&gt;Conclusion and Discussions&lt;/h1&gt;

&lt;p&gt;한국어 character를 어떻게 ‘jamo’-level로 분해하는지에 대한 방법론을 제시했다는 점에서 의의가 있다. 특히 비어있는 종성 symbol \(e\)를 추가해 일반화된 표현을 가능하게 했다는 점, inter-character하게 ‘jamo’-level로 분해하는지에 대해서 새로운 방식을 제안했다. 또한 word 단위에서 similarity, analogy 측정을 위한 dataset을 개발했다. sentiment classification task를 통해 word vector 학습이 downstream NLP task에도 큰 영향을 미친다는 점도 알 수 있다.&lt;/p&gt;

&lt;p&gt;한국어를 ‘jamo’-level로 분해하는 방식은 syntatic, semantic의 양 측면에 있어서 모두 긍정적이다. inter-character ‘jamo’-level로 분해해 각종 조사 및 어미에 대해서 syntatic한 feature를 잡아낼 수 있었다. (주어 뒤의 조사 ~은, 동사 뒤의 조사 ~고~, 과거 시제 ~었, 경어체 ~시~ 등) 심지어 더 같은 의미의 더 짧은 character로 축약도 가능했다. (되었다 → 됐다) character level n-gram은 word의 semantic한 feature를 잡아낼 수 있도록 했다. 이러한 방식 덕분에 기존의 word vector보다 더 좋은 성능을 보일 수 있었다.&lt;/p&gt;
</description>
        <pubDate>Sat, 19 Sep 2020 19:00:00 -0500</pubDate>
        <link>http://0.0.0.0:4000/machine%20learning/paper%20review/Subword-level-Word-Vector-Representation-for-Korean/</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/machine%20learning/paper%20review/Subword-level-Word-Vector-Representation-for-Korean/</guid>
        
        <category>NLP</category>
        
        <category>Korean</category>
        
        
        <category>Machine Learning</category>
        
        <category>Paper Review</category>
        
      </item>
    
      <item>
        <title>[운영체제] Process Abstraction</title>
        <description>&lt;p&gt;숭실대학교 컴퓨터학부 홍지만 교수님의 2020-2학기 운영체제 강의를 정리 및 재구성했다.&lt;/p&gt;

&lt;h2 id=&quot;process--program&quot;&gt;Process / Program&lt;/h2&gt;

&lt;p&gt;process와 program를 우선 정의내려 보자. program은 disk에 위치한다. 반면 process는 memory에 위치한다. process는 disk에 위치한 program file을 memory에 올린 것이다. 이 때 program file 전체를 모두 memory에 올릴 수도, 필요한 일부분만 memory에 올릴 수도 있다. 정리하자면, process는 Runnable(Running) program이다.&lt;/p&gt;

&lt;h2 id=&quot;process의-구성&quot;&gt;Process의 구성&lt;/h2&gt;

&lt;p&gt;process는 위에서 언급했듯이 Memory에 위치한다. 구체적으로 Stack, Heap, Data, Text(Code) 등의 영역으로 구분된다. process는 자신만의 물리적 Memory 공간을 할당받아 그 안에서 자신만의 가상 주소 체계를 갖는다. process의 가상 주소 0번지는 Stack의 처음이고, 가상 주소의 가장 끝은 Text의 끝이다. process는 memory로만 구성되지는 않으며 register도 있다. Program Counter (PC), Stack Pointer 등이 있다. PC는 다음에 실행할 명령어를 가리키게 되며, Stack Pointer는 Stack 내의 특정 공간의 위치를 저장해준다.&lt;/p&gt;

&lt;h2 id=&quot;process-생성&quot;&gt;Process 생성&lt;/h2&gt;

&lt;p&gt;fork()와 같은 system call을 사용해 process를 생성할 수 있다. 위에서 설명했듯이 process를 생성한다는 것은 program를 memory로 load하는 작업이다. 더 정확히는 process 자신의 가상 주소 공간 안으로 load한다. disk와의 I/O는 시간이 많이 걸리는 작업이기 때문에, 한꺼번에 전부를 load하지는 않고 가장 먼저 필요한 일부분을 우선 load한 뒤 실행 도중 나머지 code와 data를 page 단위로 나눠 load한다. stack 내에 지역변수, 함수 parameter, return address 등이 저장되며, heap에서 malloc(), free() 등의 동적 할당 작업을 수행하고, 그 사이 OS는 I/O setup (표준 fd 할당) 등의 여타 초기화 작업을 진행한다. process 시작 시에는 진입점 main()을 찾게 되면 os에서 process로 cpu 제어권이 넘어온다.&lt;/p&gt;

&lt;h2 id=&quot;process-status&quot;&gt;Process Status&lt;/h2&gt;

&lt;p&gt;process는 여러 상태를 갖게 된다. 왜냐하면 os는 한 번에 하나의  process만을 수행하는 것이 아니라, dispatcher라는 전환기를 사용해 여러 process를 아주 짧은 간격으로 번갈아 실행하기 때문이다. 하나의 cpu는 한 번에 하나의 process밖에 실행하지 못하므로, 필연적으로 다른 process들은 실행 중인 상태가 아니게 된다. 결국, 여러 가지 상태를 정의해야 한다.&lt;/p&gt;

&lt;h3 id=&quot;2가지-state&quot;&gt;2가지 State&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-09-14-Process-Abstraction/01.png&quot; alt=&quot;01.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;우선 수행(Running), 비수행(not Running)의 두 단계로 process의 상태를 나눌 수 있다. Running이라는 것은 프로그램의 일부 또는 전부가 memory에 올라갔다는 뜻이다. 그 동안 다른 process는 not Running 상태이다. 이후 Time Quntum (TQ)이 지나가거나(Interrupt) disk I/O 요청 등의 cpu가 process를 전환해야 할 시기가 오게 되면 dispatcher가 수행할 다음 process를 선택하게 된다. 그 동안 이전에 수행됐던 해당 process는 not Running 상태가 된다.&lt;/p&gt;

&lt;h3 id=&quot;3가지-state&quot;&gt;3가지 State&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-09-14-Process-Abstraction/02.png&quot; alt=&quot;02.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;not Running 상태를 Ready / Blocked 상태로 나눌 수 있다. Ready 상태는 process가 run할 수 있는 준비가 된상태로, run을 할 조건은 충족됐지만 os가 우선순위 등을 이유로 아직 실행하지 않은 것을 뜻한다. Blocked 상태는 간단하게 말해 자고 있는(Sleep) 상태이다. 대개 disk I/O를 뜻한다. disk I/O 도중에는 SATA와 같은 disk interface가 대부분의 작업을 수행하지, cpu는 별 일을 하지 않는다. 따라서 Blocked 상태의 경우에는 언제든 다른 process에게 cpu 점유를 뺏길(preemption) 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;5가지-state&quot;&gt;5가지 State&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-09-14-Process-Abstraction/03.png&quot; alt=&quot;03.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;생성과 종료 상태가 추가된 것이다.  추가된 State는 특별한 점이 없지만, 여기서는 각 화살표에 대해 주목해봐야 한다. Ready 상태에서 Running 상태로 가는 것을 Dispatch라고 부른다. 또한 Running 상태에서 Blocked 상태로 가는 것(I/O initiate)를 사건 대기라고 명명하고, I/O가 끝난 경우를 사건발생 이라고 부른다. 즉, I/O 가 끝나는 것이 사건이다.&lt;/p&gt;

&lt;h3 id=&quot;9가지-state&quot;&gt;9가지 State&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-09-14-Process-Abstraction/04.png&quot; alt=&quot;04.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;많은 상태가 추가됐는데, 우선 생성 상태에서부터 따라가보자. process 생성 시 memory의 여유가 충분할 경우에는 바로 Ready 상태로 갈 수 있겠지만, 만약 그렇지 않다면 Suspend 상태로 가게 된다. program file이 memory가 아닌 disk 내의 swap 공간으로 이동된 경우를 뜻한다. Suspend 상태에서 할 수 있는 작업은 Swap In(memory로 이동해 Ready 상태가 되는 것)밖에 없다. Ready 상태에서는 Kernel mode로 가거나 User mode로 갈 수 있다. User mode로 가는 방법은 2가지가 있는데, Kernel mode를 거쳐가거나, preemption을 해 거쳐가는 것이다. preemption이란 ‘선점’으로 번역되고는 하는데, 직관적으로 와닿지 않는 번역이다. 본 뜻은 다른 process가 차지하고 있던 CPU를 빼앗는 것이다. User mode에서는 I/O를 시작하거나 System Call 호출, Interrput 등이 발생하게 되면 Kernel mode로 이동하게 된다. Kernel Mode에서 수행이 끝나면 다시 preemption을 통해 User Mode로 돌아가거나 Memory Blocked 상태로 넘어갈 수 있다. Memory Blocked 상태에서 memory에 여유가 사라지면 해당 process는 Suspend상태로 넘어갈 수 있다. 한편, Kernel Mdoe에서는 exit()을 호출해 종료를 위해 Zombie 상태가 될 수도 있다.&lt;/p&gt;

&lt;h2 id=&quot;process-실행&quot;&gt;Process 실행&lt;/h2&gt;

&lt;p&gt;os가 process를 실행하는 과정을 살펴보자. 우선 process list에 새로운 항목을 생성해야 한다. 이후 memory page를 할당하고, disk에 있는 program executable file을 memory page에 load한다. 마지막으로 진입점(main 함수)로 포인터를 이동시킨다.  더 세부적인 과정은 아래와 같다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-09-14-Process-Abstraction/05.png&quot; alt=&quot;05.png&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;mode&quot;&gt;Mode&lt;/h2&gt;

&lt;p&gt;process는 다양한 연산을 수행할 수 있어야 한다. 그런데 그 중에는 I/O나 memory 접근 등의 특수 권한이 필요한 연산도 있다. 이러한 연산들을 process가 수행하게 하기 위해서는 어떻게 할까? 가장 간단한 방법으로는 process에게 별도의 제한 없이 모든 연산을 수행할 수 있게 하는 것이다. 하지만 접근 권한이 없는 곳에 접근하는 연산을 수행하게 되면 당연히 문제가 발생하게 된다. 현대의 OS는 이러한 문제를 해결하기 위해 User mode와 Kernel mode를 구분하는 정책을 채택했다. 특수 권한이 필요한 명령들은 Kernel mode에서 실행하고, 그 외 일상적인 연산들에 대해서는 User mode에서 실행하도록 구분짓는 것이다. User mode에서도 HW resource에 대해서는 모두 접근 가능하며, Kernel mode는 이에 더해 추가적으로 모든 system resource에 대해서도 접근할 수 있다. 기본적으로는 process는 User mode이며, 수행하는 연산에 따라 잠시 Kernel mode로 전환되었다가 다시 User mode로 복구된다. program의 현재 상태 word를 저장하는 PSW bit가 있는데, 이를 통해 현재 process의 mode가 어떤 type인지 파악할 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-09-14-Process-Abstraction/06.png&quot; alt=&quot;06.png&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;interrupt&quot;&gt;Interrupt&lt;/h2&gt;

&lt;p&gt;interrupt란 process가 User mode에서 Kernel mode로 변경되도록 하는 것이다. Interrupt는 크게 HW interrupt와 SW interrupt로 구분할 수 있다. 일반적으로 interrupt라 불리우는 것은 대개 HW interrupt를 의미하며, 현재 수행 중인 process와 관계 없이 외부에서 유발되는 사건에 의해 비동기적으로 발생하는 interrupt를 뜻한다. 대표적으로 I/O가 종료되거나, Time Quntum이 다 되어 running process를 전환해야 할 경우(clock interrupt)가 있다. SW interrupt는 trap과 system call이 있다. trap은 현재 process의 명령어를 수행하는 도중 error나 exception이 발생해 이에 대한 처리를 하는 비동기적 호출이다. system call은 superviser call이라고도 불리우며, 명시적으로 system call (OS function)을 호출(동기적 호출)하는 것이다.&lt;/p&gt;

&lt;h2 id=&quot;hw-interrupt--trap-처리-과정&quot;&gt;HW Interrupt / Trap 처리 과정&lt;/h2&gt;

&lt;p&gt;비동기 interrupt (HW Interrupt, Trap)의 처리 과정에 대해 알아보자. 비동기는 언제든지 발생할 수 있다는 의미이다. Programmable Interrupt Controller가 여러 HW에서 interrupt 정보를 수집하고, interrupt가 발생했을 경우 CPU에 신호를 전달한다. CPU는 Kernel의 Interrupt Descriptor(Vector) Table에서 해당 interrupt의 entry pointer 값을 찾는다. 이 때 mode가 User mode에서 Kernel mode로 전환되게 된다. 찾은 pointer 값은 interrupt handler 내부의 특정 함수를 가리키게 되는데, 이를 이용해 실제 interrupt 처리가 수행되게 된다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-09-14-Process-Abstraction/07.png&quot; alt=&quot;07.png&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;system-call&quot;&gt;System Call&lt;/h2&gt;

&lt;h3 id=&quot;system-call-처리-과정&quot;&gt;System Call 처리 과정&lt;/h3&gt;

&lt;p&gt;fork()를 호출하는 예시를 따라가며 system call 처리 과정을 이해해보자. system call도 결국 interrupt이기 때문에 위에서 살펴본 interrupt 처리 과정과 유사하다. c source code에서의 fork() 명령은 assembly code에서는 movl과 int의 두 명령어로 변환된다. 이 중 int 명령어는 interrupt 명령어이다. 이를 통해 IDT를 찾아가게 된다. system call은 결국 interrupt의 한 종류이기 때문에 IDT에서 한 공간을 system call의 entry가 차지하고 있다. 이 pointer값을 이용해 찾아간 interrupt handler는 system call들에 대한 주소들을 저장하는 table sys_call_table을 갖고 있는데, 해당 table 내부에서 sys_fork()에 대한 주소를 찾은 뒤 system call handler가 sys_fork() 명령을 실행하도록 명령한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-09-14-Process-Abstraction/08.png&quot; alt=&quot;08.png&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;system-call-추가&quot;&gt;System Call 추가&lt;/h3&gt;

&lt;p&gt;sys_call_table은 실제로 어디에 위치하고 있을까? 운영체제마다 세부 위치는 다르지만, /usr/src/[os kernel version]/arch 밑에 위치한다는 사실은 공통적이다. 만약 OS에 새로운 System Call을 추가하고 싶다면 해당 sys_call_table 파일을 열어 새로운 번호를 추가하는 것이 먼저이다. 이후 실제 해당 함수에 대한 원형을 선언해야 하는데, system call의 원형을 저장하는 header file은 /usr/src/[os kerner version]/arch/…/include/syscalls.h에 위치한다. 실제 함수 body를 정의한 뒤 이를 object file로 compile하면, 해당 object file을 /usr/src/[os kernel version]에 위치시키면 작업이 완료된다. 정리하자면 다음의 4단계를 거친다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;syscall table에 새로운 함수 등록&lt;/li&gt;
  &lt;li&gt;함수 header syscalls.h에 추가&lt;/li&gt;
  &lt;li&gt;함수 body .c file 작성&lt;/li&gt;
  &lt;li&gt;함수 body 작성한 .c file 컴파일 해 .o file 생성 후 kernel 경로에 저장&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;context-문맥&quot;&gt;Context (문맥)&lt;/h2&gt;

&lt;p&gt;문맥은 크게 3가지로 구분될 수 있다. HW (register) 문맥, system 문맥, memory 문맥이 그 예이다. 실제 task_struct 구조체에서도 struct mm struct *mm으로 memory 문맥을, struct threadstruct로 HW 문맥을 구현했다.&lt;/p&gt;

&lt;h2 id=&quot;process-전환&quot;&gt;Process 전환&lt;/h2&gt;

&lt;h3 id=&quot;process-전환-1&quot;&gt;Process 전환&lt;/h3&gt;

&lt;p&gt;schedule() 함수는 다음 실행할 process의 PCB를 return하는 함수이다. PCB란 Process Control Block으로 os가 process를 추상화한 것이다. linux에서는 PCB가 task_struct type이다. 이렇게 얻어낸 다음 process의 PCB와 현재 실행 중이던 process의 PCB를 switch()함수에 인자로 넣으면(switch(현재 PCB, 다음 PCB)) 문맥 교환이 수행된다. 자세한 단계는 아래와 같이 진행된다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;현재 실행 중이던 process의 context를 저장&lt;/li&gt;
  &lt;li&gt;현재  실행 중이던 process의 PCB를 갱신 (상태를 Ready, Block, Sleep, Zombie 등으로 변경)&lt;/li&gt;
  &lt;li&gt;PCB를 os의 schedule 정책에 따른 특정 queue로 이동&lt;/li&gt;
  &lt;li&gt;os의 schedule 정책에 따라 다음 process를 선택&lt;/li&gt;
  &lt;li&gt;선택된 process의 PCB를 갱신 (상태를 Running으로 전환)&lt;/li&gt;
  &lt;li&gt;선택된 process가 이전 수행에서 사용했던 context를 복원&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;위에서 정리한 9단계 State를 참고하면, 아래의 경우에 대해 각각의 process 전환 이후의 상태를 특정할 수 있다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;exit() → schedule() → switch() : zombie 상태&lt;/li&gt;
  &lt;li&gt;I /O → schedule() → switch() : block 상태&lt;/li&gt;
  &lt;li&gt;clock interrupt → schedule() → switch() : runnable (또는 memory가 부족할 경우 suspend) 상태&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;mode-switch-vs-process-switch&quot;&gt;Mode Switch vs Process Switch&lt;/h3&gt;

&lt;p&gt;mode의 전환은 Kernel mode / User mode 사이의 전환을 말한다. mode 전환은 현재 실행 중인 process의 상태 (running, runnable, zombie …)를 바꾸지 않고 수행 가능하다. context를 따로 저장할 필요도 없기 때문에 많은 연산을 요구하지 않는다. 반면 process 전환은 실제 context를 따로 저장 및 교환해야 하기 때문에 mode 전환에 비해 훨씬 더 많은 연산을 요구하게 된다.&lt;/p&gt;

&lt;h3 id=&quot;process-전환의-한계&quot;&gt;Process 전환의 한계&lt;/h3&gt;

&lt;p&gt;기본적으로 대부분의 os는 중첩 interrupt를 금지한다. 즉, 이미 interrupt가 발생해서 처리 중인 경우에는 그 사이에 발생된 새로운 interrupt에 대해 처리하지 않는다는 것이다.&lt;/p&gt;
</description>
        <pubDate>Sun, 13 Sep 2020 19:00:00 -0500</pubDate>
        <link>http://0.0.0.0:4000/operating%20system/Process-Abstraction/</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/operating%20system/Process-Abstraction/</guid>
        
        <category>Operating System</category>
        
        
        <category>Operating System</category>
        
      </item>
    
      <item>
        <title>[운영체제] Introduction to Operating Systems</title>
        <description>&lt;p&gt;숭실대학교 컴퓨터학부 홍지만 교수님의 2020-2학기 운영체제 강의를 정리 및 재구성했다.&lt;/p&gt;

&lt;h1 id=&quot;os-operating-system&quot;&gt;OS (Operating System)&lt;/h1&gt;

&lt;h2 id=&quot;os의-정의&quot;&gt;OS의 정의&lt;/h2&gt;

&lt;p&gt;OS는 SW의 일종이다.  ‘자원’(HW)관리’자’(SW)로 정의할 수 있다. HW를 SW로 관리해주는 역할을 수행한다. 구체적으로 CPU, Memory, Disk 등을 struct로 정의해 각각 Process, Virtual Memory, File System을 만들어낸다.  OS는 kernel 함수를 이용해 HW를 관리하며, 사용자는 kernel 함수를 직접 호출하지 않고, system call을 사용하게 된다.&lt;/p&gt;

&lt;h2 id=&quot;os의-역할&quot;&gt;OS의 역할&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;사용자가 프로그램을 쉽게 사용할 수 있도록 돕는다.&lt;/p&gt;

    &lt;p&gt;실제로 HW가 어떻게 작동하는지를 숨기고(가상화), 사용자가 간단하게 사용할 수 있는 관리 도구를 제공(추상화)한다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;시스템이 정확하고 효율적으로 작동하는지 확인한다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;추상화-vs-가상화&quot;&gt;추상화 vs 가상화&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;추상화: HW를 SW로 표현하는 것
    &lt;ul&gt;
      &lt;li&gt;CPU → Process&lt;/li&gt;
      &lt;li&gt;Memory → Virtual Memory&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;가상화: 실제 생김새나 기능을 사용자에게 숨겨 구조 등을 착각하게 만듦
    &lt;ul&gt;
      &lt;li&gt;CPU → 프로그램마다 별도의 CPU를 모두 보유한 것처럼 보이게 함 (Context Switch)&lt;/li&gt;
      &lt;li&gt;Memory → 프로그램마다 별도의 Memory를 모두 보유한 것처럼 보이게 함 (Share Memory)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;cpu-가상화&quot;&gt;CPU 가상화&lt;/h1&gt;

&lt;p&gt;Context Switch(문맥 교환)이란 아주 빠른 속도(대개 10ms)로 하나의 CPU가 process들을 interleave하게 실행하는 것이다. 사용자는 각 process들이 동시에 실행되는 것으로 착각하게 된다. 이를 통해 Multi-tasking이 구현되는 것이다.&lt;/p&gt;

&lt;p&gt;문맥 교환이 발생하는 시점은 3가지로 구분된다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;TQ (문맥 교환 발생하는 시간이 충족됐을 때)&lt;/li&gt;
  &lt;li&gt;exit()&lt;/li&gt;
  &lt;li&gt;DISK I/O&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;위의 경우가 오게 되면 switchto() 함수를 호출해 문맥 교환을 수행하게 된다. 인자로는 두 task_struct를 받는다.&lt;/p&gt;

&lt;div class=&quot;language-c highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;switchto&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;현재&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;수행중인&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;process&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;의&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;task_struct&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;다음&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;수행할&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;process&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;의&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;task_struct&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;다음 수행할 process의 task_struct는 schedule() 함수의 return으로 얻을 수 있다.&lt;/p&gt;

&lt;p&gt;문맥 교환은 여러 process들을 번갈아가며 수행하는데, 이 때 전환되는 process의 순서는 일정하지 않을 수 있다. 예를 들어 process가 A → B → C → D 순으로 문맥 교환이 발생했다고 하더라도, 다음에도 같은 순서로 process가 실행된다고 보장할 수 없다.&lt;/p&gt;

&lt;h1 id=&quot;memory-가상화&quot;&gt;Memory 가상화&lt;/h1&gt;

&lt;p&gt;같은 프로그램 파일(disk에 위치)로 두 process(memory에 위치)를 실행하게 되면 두 process가 동일한 memory 주소를 참조하는 것처럼 보이는 상황이 있을 수 있다. 이는 실제 물리적으로 같은 memory를 참조하는 것이 아니라 각각의 process가 갖고 있는 가상 memory 주소 상에서 같은 주소를 참조하는 것이다. 각 process의 가상 memory 주소는 시작 주소가 다를 것이므로, 가상 memory 주소가 같다고 하더라도 실제 물리 memory 주소는 같지 않다. OS는 이렇게 가상의 memory 주소와 물리적 memory 주소를 mapping하는 역할을 수행한다. (최근의 CPU에는 이 역할을 OS 대신 수행해주는 MMU 기능이 들어가 있다. SW인 OS보다 HW인 CPU가 처리하는 것이 더 빠르기 때문이다.) 가상 Memory 주소 공간은 register 크기에 따라 달라진다(32bit, 64bit). 한편, 가상 memory는 가상 Memory 주소 공간을 의미하는 것이 아닌, 실제 Memory + SWAP(Disk에 위치하지만 Memory로 임시로 사용되는 영역, Linked LIst로 구현됨)을 뜻한다.&lt;/p&gt;

&lt;h1 id=&quot;concurrency-동시-실행&quot;&gt;Concurrency (동시 실행)&lt;/h1&gt;

&lt;p&gt;process 간 전환이 일어날 때, 항상 원자적(atomic)으로 실행되지는 않는다. 따라서 같은 조건에서 같은 횟수로 process를 실행했다고 하더라도 따라서 카운터의 값이 항상 같지는 않다. memory → register load와 ++, register→memory save의 과정이 원자적으로 일어나지 않기 때문이다.&lt;/p&gt;
</description>
        <pubDate>Sun, 06 Sep 2020 19:00:00 -0500</pubDate>
        <link>http://0.0.0.0:4000/operating%20system/Introduction-to-Operating-Systems/</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/operating%20system/Introduction-to-Operating-Systems/</guid>
        
        <category>Operating System</category>
        
        
        <category>Operating System</category>
        
      </item>
    
      <item>
        <title>[NLP 논문 리뷰] MASS: Masked Sequence To Sequence Pre Training For Language Generation</title>
        <description>&lt;h2 id=&quot;paper-info&quot;&gt;Paper Info&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1905.02450&quot;&gt;Archive Link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1905.02450.pdf&quot;&gt;Paper Link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Submit Date: Jun 21, 2019&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;

&lt;p&gt;BERT에서 영감을 받아 Pre-training / fine-tuning, encoder/decoder를 채택한 MAsked Sequence to Sequence (MASS) model을 만들어냈다. random하게 input sentence에 연속적인 mask를 부여한 뒤 decoder가 predict하는 방식으로 encoder와 decoder를 Pre-training시켜 Language Generation Task에 적합한 Model을 개발했다. 특히 dataset이 적은 Language Generation task에서 비약적인 성능 향상이 있었다.&lt;/p&gt;

&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;Pre-training은 target task에 대한 labeled data(pair data)가 적으면서 해당 language에 대한 data(unpaired data)는 많을 때에 가장 적합하다고 할 수 있다. BERT는 language understanding을 목표로 하는 방식이기에 language generation task에는 적합하지 않다. MASS의 BERT와 Masking rule에서 다음과 같은 차이점을 둔다. 첫번째로, MASK token이 연속적으로 배치되고, 해당 MASK Token을 예측하면서 encoder는 unmasked token들의 context를 학습할 수 있도록 한다. 두번째로 decoder의 target token에도 MASK를 부여함으로써 predict 시에 encoder를 더 많이 활용할 수 있도록 한다.&lt;/p&gt;

&lt;h1 id=&quot;mass&quot;&gt;MASS&lt;/h1&gt;

&lt;h2 id=&quot;sequence-to-sequence-learning&quot;&gt;Sequence to Sequence Learning&lt;/h2&gt;

&lt;p&gt;source sentence를 \(x\), target sentence를 \(y\)라고 한다. 각각 domain \(X\)와 \(Y\)에 속한다. sentence pair를 다음과 같이 정의할 수 있다.&lt;/p&gt;

\[\left(x,y\right) \in \left(X,Y\right), \\x= \left(x_1,x_2, ..., x_m\right)\\y = \left(y_1,y_2, ..., y_n\right)\]

&lt;p&gt;Objective Function은 다음과 같다. domain \(X\)와 \(Y\)에 대한 모든 sentence pair들에 대해 \(x\)가 주어졌을 때 \(y\)를 구하는 조건부 확률의 log liklihood를 더한 것이다.&lt;/p&gt;

\[L(\theta;(X,Y)) = \sum_{\left(x,y\right)\in\left(X,Y\right)} {log P\left(y|x;\theta\right)}\]

&lt;p&gt;조건부 확률을 구하는 수식은 다음과 같다. source sentence 전체와 target sentence에서 현재 token 이전의 모든 token들이 주어졌을 때 현재 token에 대한 조건부 확률이다.&lt;/p&gt;

\[P\left(y|x;\theta\right)=\prod_{t=1}^n{P\left(y_t|y_{&amp;lt;t},x;\theta\right)}\]

&lt;p&gt;\(y_{&amp;lt;t}\)는  \(y_1\sim y_{t-1}\)의 token들이다.&lt;/p&gt;

&lt;h2 id=&quot;masked-sequence-to-sequence-pre-training&quot;&gt;Masked Sequence to Sequence Pre-training&lt;/h2&gt;

&lt;p&gt;MASS는 BERT와 달리 MASK token이 discrete하게 분포되어 있지 않고 연속적으로 뭉쳐져 있다. 이에 따라 새로운 parameter \(k\)가 등장한다. \(k\)는 MASK token의 개수인데, \(k\)개의 MASK token은 연속되어 있다. MASK token이 \(u\)부터 \(v\)까지 분포되어 있다면 \(0&amp;lt;u&amp;lt;v&amp;lt;m\) (\(m\)은 전체 sentence 길이)이고, \(k = v - u + 1\)이다. Pre-training에서 사용하는 Objective Function은 다음과 같다. 조건부 확률의 조건으로 다음의 2가지 값이 주어지게 된다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;MASK가 씌워진 input sentence 전체&lt;/li&gt;
  &lt;li&gt;input sentence에서 MASK가 씌워진 token들 중 현재 token \(x_t\) 이전의 token들의 MASK 씌워지기 이전 원본 token&lt;/li&gt;
&lt;/ol&gt;

\[L(\theta;X)=\frac{1}{\vert X\vert}\sum_{x\in X}log\ P\left(x^{u:v}|x^{\backslash u:v};\theta\right)\\=\frac{1}{\vert X\vert}\sum_{x\in X}log\prod_{t=u}^vP\left(x_t^{u:v}|x_{&amp;lt;t}^{u:v},x^{\backslash u:v};\theta\right)\]

&lt;p&gt;\(x^{u:v}\)는 sentence \(x\)에서 \(u\)부터 \(v\)까지의 tokens를 뜻하고, \(x^{\backslash u:v}\)는 \(u\)부터 \(v\)까지 MASK된 sentence \(x\) 전체를 뜻한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-09-06-MASS-Masked-Sequence-to-Sequence-Pre-training-for-Language-Generation/01.jpg&quot; alt=&quot;01.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;구체적인 예시를 살펴보자. 위의 figure는 \(x_3, x_4, x_5, x_6\)이 masking된 상황이다. \(k=4\)이고, \(u=3, v=6\)이다. Encoder의 input 으로는 masking된 input sentence \(x^{\backslash u:v}\)가 들어오게 되는데, 이 경우에는 \(x^{\backslash 3:6}\)이다. Attention 기법을 적용해 Decoder로 값이 넘어오고, Decoder에서는 새로운 input으로 \(x^{u:v}\), 이 경우에는 \(x^{3:6}\)을 입력으로 받는다. 이 때 input sentence에서 masking이 되지 않은 token들 (\(x_1, x_2, x_7,x_8)\)의 경우에는 Decoder에 input으로 들어오지 않는다. Decoder의 input으로 들어온 token들 \(x_3, x_4, x_5, x_6\) 중 실제로는 \(x^{u:v}_{&amp;lt;t}\)로 사용되기 때문에 마지막 token \(x_6\)은 사용되지 않는다.&lt;/p&gt;

&lt;h2 id=&quot;discussions&quot;&gt;Discussions&lt;/h2&gt;

&lt;h3 id=&quot;special-case--k1-km&quot;&gt;Special Case ( k=1, k=m)&lt;/h3&gt;

&lt;p&gt;MASS에서 hyperparameter \(k\)는 매우 중요한 parameter이다. \(k\)가 특수한 값일 때에 대해서 살펴보자.&lt;/p&gt;

&lt;p&gt;\(k=1\)인 경우는 사실 BERT에서의 MLM(Masked Langage Model)이다. BERT의 MLM에 대한 자세한 설명은 아래를 참조하자.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://cpm0722.github.io/paper%20review/BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding/&quot;&gt;BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-09-06-MASS-Masked-Sequence-to-Sequence-Pre-training-for-Language-Generation/02.jpg&quot; alt=&quot;02.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;BERT의 MLM은 MASK token에 대해서 predict만 하는 방식으로 Pre-training을 수행했다. 즉 Decoder에 어떠한 input도 추가적으로 주어지지 않고, Encoder에서 넘어온 Context Vector만을 사용해 MASK token을 predict하는 training이다. 이는 MASS에서 \(k=1\)일 때의 경우이다.&lt;/p&gt;

&lt;p&gt;한편, \(k=m\) (\(m\)은 sentence의 token 개수)인 경우는 일반적인 Language Generation Model이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-09-06-MASS-Masked-Sequence-to-Sequence-Pre-training-for-Language-Generation/03.jpg&quot; alt=&quot;03.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;\(k=m\)인 경우는 사실 일반적인 Language Model의 경우이다. \(k=m\)라는 것은 다시 말해 input sentence의 모든 token이 masking되었다는 의미이고, 이는 Encoder의 input으로 아무 값도 들어오지 않는 경우와 같다. 한편 Decoder의 입장에서는 input으로 original sentence의 masked token들이 들어오게 되는데, original sentence는 모두 masking되었으므로 모든 token이 Decoder로 들어오는 경우이다. 이는 결국 Encoder가 없이 Decoder만 작동하는 상황이라고 볼 수 있다. 일반적인 GPT model와 같다.&lt;/p&gt;

&lt;p&gt;위의 두 가지 special case와 일반적인 case를 Table로 정리하면 아래와 같다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-09-06-MASS-Masked-Sequence-to-Sequence-Pre-training-for-Language-Generation/04.jpg&quot; alt=&quot;04.jpg&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;comparison-with-existing-model&quot;&gt;Comparison with existing model&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;MASK token만 predict하게 함으로써 Encoder는 unmasked token들에 대한 context를 학습하게 되고, decoder가 encoder로부터 더 좋은 정보를 가져갈 수 있도록 한다. (encoder가 context vector를 제대로 생성해내도록 한다.)&lt;/li&gt;
  &lt;li&gt;MASK token을 연속적으로 배치함으로써 Decoder가 단순 word들이 아닌 subsentence를 만들어낼 수 있도록 한다. (better language modeling capability)&lt;/li&gt;
  &lt;li&gt;Decoder의 input으로 source sentence의 unmasked token들이 들어오지 못하게 함으로써 Decoder의 input token들에서 정보를 활용하기보다 Encoder에서 넘어온 Context Vector의 정보를 활용할 수 있도록 했다.&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;experiments-and-results&quot;&gt;Experiments and Results&lt;/h1&gt;

&lt;h2 id=&quot;mass-pre-training&quot;&gt;MASS Pre-training&lt;/h2&gt;

&lt;h3 id=&quot;model-configuration&quot;&gt;Model Configuration&lt;/h3&gt;

&lt;p&gt;6개의 encoder, decoder layer를 가진 Transformer를 Base Model로 선택했다. NMT를 위해 source language와 target language에 대해 각각 monolingual data로 pre-train을 진행했다. English-French, English-German, English-Romanian의 3가지 pair를 사용했다. 각 pair에 대해서 별개로 학습을 진행했으며, 이 때 source language와 target language를 구분하기 위해 새로운 language embedding을 encoder input과 decoder input에 추가했다. Text Summarization과 Conversational Response Generation task에 대해서는 모두 English에 대해서만 pre-train을 진행했다.&lt;/p&gt;

&lt;h3 id=&quot;datasets&quot;&gt;Datasets&lt;/h3&gt;

&lt;p&gt;WMT News Crawl Dataset을 사용했다. English, French, German, Romanian에 대해서 Pre-train을 진행했다. 이 중 Romanian의 경우에는 data가 적은 language이다. low-resource language에 대한 MASS의 pre-training 성능을 측정하기 위해 채택했다. 모든 language에 대해 BPE를 적용했다.&lt;/p&gt;

&lt;h3 id=&quot;pre-training-details&quot;&gt;Pre-Training Details&lt;/h3&gt;

&lt;p&gt;BERT와 동일한 masking rule을 채택했다.MASK token으로 변경되는 token 중 실제로 변경되는 token은 80%이고, 다른 random한 token으로 변경되는 것이 10%, 변경되지 않는 것이 10%이다. hyperparameter \(k\)는 전체 sentence 길이 \(m\)의 50%와 비슷한 수치가 되도록 설정했다. decoder의 input으로 들어오는 sentence에 대해서는 기존의 original sentence에서의 positional encoding은 수정되지 않는다. Adam Optimizer를 사용했고, lr은 0.0001이며, batch_size는 3000이다.&lt;/p&gt;

&lt;p&gt;fine-tuning을 수행할 dataset이 적은 경우(paired sentence가 적은 경우)에 대해서도 성능을 측정한다. 특히 아예 fine-tuning data가 없는 상태에서도 NMT를 잘 수행할 수 있는지에 대해서 살펴본다.&lt;/p&gt;

&lt;h2 id=&quot;fine-tuning-on-nmt&quot;&gt;Fine-Tuning on NMT&lt;/h2&gt;

&lt;h3 id=&quot;experimental-setting&quot;&gt;Experimental Setting&lt;/h3&gt;

&lt;p&gt;Unsupervised NMT를 수행하기 위해서 back-translation을 사용한다. bilingual data가 없기 때문에 soruce language data로 가상의 bilingual data를 생성해내는 것이다. Pre-Training과 동일하게 Adam Optimizer, lr=0.0001을 채택했으며, batck_size는 2000이다. evaluation을 위해 BLEU Score를 사용했다.&lt;/p&gt;

&lt;h3 id=&quot;results-on-unsupervised-nmt&quot;&gt;Results on Unsupervised NMT&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-09-06-MASS-Masked-Sequence-to-Sequence-Pre-training-for-Language-Generation/05.jpg&quot; alt=&quot;05.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;RNN 계열의 Model들(1,2행)과 Pre-train 방식이 아닌 Transformer Model(3,4행), Pre-train Transfor Model(5행)들을 모두 능가했다. Unsupervised NMT는 난제이기에 절대적인 Score는 낮지만, 기존의 SOTA Model인 XLM을 능가했다는 점에서 의미가 있다.&lt;/p&gt;

&lt;h3 id=&quot;compared-with-other-pre-training-methods&quot;&gt;Compared with Other Pre-training Methods&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-09-06-MASS-Masked-Sequence-to-Sequence-Pre-training-for-Language-Generation/06.jpg&quot; alt=&quot;06.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;다양한 Pre-train Methods를 적용한 Model들과 Unsupervised NMT에서의 BLEU Score를 비교해본다. BERT와 동일한 방식으로 Pre-train을 진행한 BERT+LM Model, denoising auto-encoder Pre-train 방식을 적용한 DAE를 모두 능가했다.&lt;/p&gt;

&lt;h3 id=&quot;experiments-on-low-resource-nmt&quot;&gt;Experiments on Low-Resource NMT&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-09-06-MASS-Masked-Sequence-to-Sequence-Pre-training-for-Language-Generation/07.jpg&quot; alt=&quot;07.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Pre-train은 20000 step 진행했으며, bilingual dataset의 sample 크기가 10K, 100K, 1M인 경우에 대해서 각각의 언어에 대해 별개로 성능을 측정했다. baseline model은 pre-train 과정이 없는 model이다. 모든 경우에 있어서 MASS가 baseline model을 압도했으며, 특히나 Sample의 크기가 작을 수록(fine-tuning을 적게 수행할수록) 성능의 차이가 컸다.&lt;/p&gt;

&lt;h2 id=&quot;fine-tuning-on-text-summarization&quot;&gt;Fine-Tuning on Text Summarization&lt;/h2&gt;

&lt;h3 id=&quot;experiment-setting&quot;&gt;Experiment Setting&lt;/h3&gt;

&lt;p&gt;Gigaword corpus를 fine-tuning data로 사용했다. sample size가 10K, 100K, 1M, 3.8M인 경우에 대해서 별개로 성능을 측정했으며, encoder의 input은 article로, decoder의 output은 title로 설정했다.성능 측정은 ROUGE-1, ROUGE-2, ROUGE-L에 대한 F1 score로 측정했다. beam size=5인 beam search를 사용했다.&lt;/p&gt;

&lt;h3 id=&quot;results&quot;&gt;Results&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-09-06-MASS-Masked-Sequence-to-Sequence-Pre-training-for-Language-Generation/08.jpg&quot; alt=&quot;08.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;pre-training을 수행하지 않은 basemodel과 비교를 수행했으며, dataset이 적은 경우에 대해서 압도적인 성능 격차를 보였다는 것을 확인할 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;compared-with-other-pre-training-methods-1&quot;&gt;Compared with Other Pre-Training Methods&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-09-06-MASS-Masked-Sequence-to-Sequence-Pre-training-for-Language-Generation/09.jpg&quot; alt=&quot;09.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;다른 Pre-training model에 대해서도 더 좋은 성능을 보였다.&lt;/p&gt;

&lt;h2 id=&quot;fine-tuning-on-conversational-response-generation&quot;&gt;FIne-Tuning on Conversational Response Generation&lt;/h2&gt;

&lt;h3 id=&quot;experimental-setting-1&quot;&gt;Experimental Setting&lt;/h3&gt;

&lt;p&gt;Cornell movie dialog corpus를 Dataset으로 사용했다. 총 140K의 pair 중에서 10K는 validation set, 20K는 test set, 나머지는 모두 training set으로 사용했다. Perplexity(PPL)을 성능 측정 단위로 사용했다.&lt;/p&gt;

&lt;h3 id=&quot;results-1&quot;&gt;Results&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-09-06-MASS-Masked-Sequence-to-Sequence-Pre-training-for-Language-Generation/10.jpg&quot; alt=&quot;10.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Sample 크기가 10K, 110K인 경우에 대해서 성능을 측정했다. MASS는 모든 경우에서 Pre-training을 수행하지 않은 Baseline Model과, Pre-training을 수행한 BERT Model보다 더 좋은 성능을 보였다. PPL은 더 낮은 Score가 더 좋은 성능을 뜻한다.&lt;/p&gt;

&lt;h2 id=&quot;analysis-of-mass&quot;&gt;Analysis of MASS&lt;/h2&gt;

&lt;h3 id=&quot;study-of-different-k&quot;&gt;Study of Different k&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-09-06-MASS-Masked-Sequence-to-Sequence-Pre-training-for-Language-Generation/11.jpg&quot; alt=&quot;11.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;hyperparameter \(k\)에 대해서 자세히 살펴보자. \(k\)의 값 변화에 따른 Score들을 측정해본다.&lt;/p&gt;

&lt;p&gt;(a)와 (b)는 각각 English, French에 대해 Pre-training을 시킨 직후(fine-tuning 없이)의 PPL Score를 나타낸 것이다. \(k\)가 \(m\)의 50%~70% 인 구간에서 가장 좋은 수치를 보였다.&lt;/p&gt;

&lt;p&gt;(c)는 English-French NMT에 대한 BLEU Score이다. (d)는 Text Summarization에 대한 ROUGUE score이다. (e)는 Conversational Response Generation에 대한 PPL Score이다. 모두 공통적으로 \(k\)가 \(m\)의 50%인 구간에서 가장 좋은 수치를 보였다.&lt;/p&gt;

&lt;p&gt;\(k\)가 \(m\)의 50%라는 수치는 직관적으로 이해했을 때에도 가장 적합하다.&lt;/p&gt;

&lt;p&gt;\(k\)의 값이 감소한다면 masking을 덜 수행하게 되므로 Encoder Input에 변형이 덜 발생한다는 의미이며, 동시에 Decoder Input으로 들어오는 값이 감소함을 뜻한다. 따라서 Encoder에 대한 의존도를 높이게 된다.&lt;/p&gt;

&lt;p&gt;반대로 \(k\)의 값이 증가한다면 masking을 더 많이 수행하게 되므로 Encoder Input에 변형이 더 발생한다는 의미이며, 동시에 Decoder Input으로 들어오는 값이 증가함을 뜻한다. 따라서 Decoder에 대한 의존도를 높이게 된다.&lt;/p&gt;

&lt;p&gt;Language Generation task에서는 Encoder(source)와 Decoder(target) 중 어느 쪽으로도 편향되지 않아야 좋은 성능을 나타낼 것이다. 따라서 \(k\)가 \(m\)의 50%라는 수치가 가장 적합함을 직관적으로 이해할 수 있다.&lt;/p&gt;

&lt;p&gt;위의 Figure에서도 볼 수 있듯이 당연하게도 \(k=1\)인 경우(BERT의 MLM), \(k=m\)인 경우(General Language Model) 모두 Language Generation task에서는 좋은 성능을 보이지 못한다.&lt;/p&gt;

&lt;h3 id=&quot;ablation-study-of-mass&quot;&gt;Ablation Study of MASS&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-09-06-MASS-Masked-Sequence-to-Sequence-Pre-training-for-Language-Generation/12.jpg&quot; alt=&quot;12.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;MASS에서 추가된 새로운 Masking Rule 다음의 2가지로 정리할 수 있다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;MASK token을 연속적으로 배치&lt;/li&gt;
  &lt;li&gt;encoder input의 unmasked token을 decoder input에서 masking&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;위의 Table의 Discrete는 1번 rule을 제거한 것이고(비연속적으로 MASK token 배치), Feed는 2번 rule을 제거한 것이다(decoder input이 original sentence). Unsupervised English to French NMT에서 MASS가 가장 좋은 성능을 보였다.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;MASS는 Dataset이 적은 경우(또는 Dataset이 아예 없는 경우)의 Language Generation task에서 기존의 SOTA를 능가하는 성능을 보였다. 특히나 Unsupervised NMT에서 비약적인 성능 향상을 이뤄냈다.&lt;/p&gt;
</description>
        <pubDate>Sat, 05 Sep 2020 19:00:00 -0500</pubDate>
        <link>http://0.0.0.0:4000/machine%20learning/paper%20review/MASS-Masked-Sequence-to-Sequence-Pre-training-for-Language-Generation/</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/machine%20learning/paper%20review/MASS-Masked-Sequence-to-Sequence-Pre-training-for-Language-Generation/</guid>
        
        <category>NLP</category>
        
        
        <category>Machine Learning</category>
        
        <category>Paper Review</category>
        
      </item>
    
      <item>
        <title>[NLP 논문 리뷰] Xlnet: Generalized Autoregressive Pretraining for Language Understanding</title>
        <description>&lt;h2 id=&quot;paper-info&quot;&gt;Paper Info&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1906.08237&quot;&gt;Archive Link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1906.08237.pdf&quot;&gt;Paper Link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Submit Date: Jun 19, 2019&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;Unsupervised Learning을 pretraining에 적용시키는 방식은 NLP domain에서 매우 큰 성과를 이뤄냈다. Unsupervised pretraining하는 방법론은 크게 AutoRegressive(AR)과 AutoEncoding(AE)가 있다. AutoRegressive는 순방향 또는 역방향으로 다음의 corpus를 예측하는 방식으로 학습한다. 이는 단방향 context만 학습할 수 있다는 단점이 있다. 하지만 현실의 대부분의 downstream task는 bidirectional context가 필수적이기에 이는 크나큰 한계가 된다.&lt;/p&gt;

&lt;p&gt;반면 AutoEncoding은 변형된 input을 다시 본래의 input으로 재구성하는 방식이다. BERT가 대표적인 예시인데, input data의 일부분을 [MASK] token 등으로 변화를 준 뒤, 원래의 input을 만들어내도록 학습시킨다. 이러한 방법은 bidirectional context를 학습할 수 있다는 점에서 AutoRegressive에 비해 상대적으로 좋은 성능을 보인다.하지만 인위적인 변형을 가해 만들어낸 [MASK] token 등은 pretraining 과정에서만 존재하는 token이고, 이후 downstream task를 학습시키는 fine-tuning 과정에서는 존재하지 않는 token이 된다. 따라서 pre-training과 fine-tuning 사이의 괴리가 발생하게 된다. 또한 각 [MASK] token을 predict하는 과정은 independent하기 때문에 predict된 token들 사이의 dependency는 학습할 수 없다는 한계도 있다.&lt;/p&gt;

&lt;p&gt;본 논문에서 제시하는 XLNet은 이러한 AR과 AE을 모두 사용해 각각의 장점만을 취하도록 했다.&lt;/p&gt;

&lt;h1 id=&quot;proposed-method&quot;&gt;Proposed Method&lt;/h1&gt;

&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;

&lt;h3 id=&quot;ar-autoregressive&quot;&gt;AR (Autoregressive)&lt;/h3&gt;

&lt;p&gt;일반적인 AR의 objective function은 다음과 같다.&lt;/p&gt;

\[\underset{\theta}{max}\ \ log{\ p_\theta(x)} = \sum_{t=1}^Tlog{\ p_\theta\left(x_t|x_{&amp;lt;t}\right)} = \sum_{t=1}^Tlog{\ \frac{exp\left(h_\theta\left(x_{1:t-1}\right)^Te\left(x_t\right)\right)}{\sum_{x'}{exp\left(h_\theta\left(x_{1:t-1}\right)^Te\left(x'\right)\right)}}}\]

&lt;p&gt;\(h_\theta\left(x_{1:t-1}\right)\)는 model의 context representation이고, \(e\left(x'\right)\)는 x의 embedding이다.&lt;/p&gt;

&lt;h3 id=&quot;ae-autoencoding&quot;&gt;AE (Autoencoding)&lt;/h3&gt;

&lt;p&gt;일반적인 AE의 objective function은 다음과 같다.&lt;/p&gt;

\[\underset\theta{max}\ log{\ p_\theta\left(\bar{x}|\hat{x}\right)} \approx \sum_{t=1}^T{m_tlog{\ p_\theta\left(x_t|\hat{x}\right)}} = \sum_{t=1}^T{m_tlog{\ \frac{exp\left(H_\theta\left(\hat{x}\right)_t^Te\left(x_t\right)\right)}{\sum_{x'}{exp\left(H_\theta\left(\hat{x}\right)_t^Te\left(x'\right)\right)}}}}\]

&lt;p&gt;\(\hat{x}\)는 [MASK] token 등이 추가된 변형된 input이고, \(\bar{x}\)는 masked token이다.&lt;/p&gt;

&lt;p&gt;\(m_t=1\)인 경우 \(x_t\)가 masked된 경우를 뜻하고, \(H_\theta\)는 Transformer의 hidden vector를 뜻한다.&lt;/p&gt;

&lt;h3 id=&quot;xlnet&quot;&gt;XLNet&lt;/h3&gt;

&lt;p&gt;XLNet은 AR와 AE를 아래의 3가지 관점에서 비교하며 각각의 장점만 취한다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Independence Assumption&lt;/p&gt;

    &lt;p&gt;AE의 objective function은 조건부확률을 계산하는 것이다. 이 때 \(\approx\)를 사용한다. 이는 모든 \(\bar{x}\)에 대한 reconstruction이 independent하게 이루어진다는 가정 하에 이루어지기 때문이다. 반면 AR의 objective function은 이러한 가정 없이도 성립하기에 \(=\)를 사용한다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Input Noise&lt;/p&gt;

    &lt;p&gt;AE에서는 [MASK] token과 같이 실제 input에 없던 token들이 추가되게 된다. 이는 pretraining 때에만 존재하는 token으로 fine-tuning 과정에서는 존재하지 않는다. 이러한 pretraining과 fine-tuning 사이의 괴리를 해결하기 위해 BERT에서는 masking에 대해 모두 [MASK] token으로 변경하지 않고 일부분은 original token 그대로 두는 등의 기법을 사용했으나, 이는 전체 token에서 극히 일부분에만 적용되기 때문에 (0.15 * 0.1 == 0.015) 의미있는 결과를 도출해내지 못한다. AR에서는 input에 대한 변경이 없기 때문에 이러한 문제가 발생하지 않는다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Context Dependency&lt;/p&gt;

    &lt;p&gt;AE는 bidirectional context를 모두 학습할 수 있지만, AR은 unidirectional context만 학습한다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;objective-permutation-language-modeling&quot;&gt;Objective: Permutation Language Modeling&lt;/h2&gt;

&lt;p&gt;AR의 장점은 모두 취하면서(no Indepence Assumption, no Input Noise) AR의 단점은 해결하는(Bidirectional Context) Objective function을 정의하기로 한다.&lt;/p&gt;

\[\underset{\theta}{max} = E_{z\thicksim Z_T}\left[\sum_{t=1}^T{log{\ p_\theta\left(x_{z_t}|x_{z_{&amp;lt;t}}\right)}}\right]\]

&lt;p&gt;\(Z_T\)는 길이가 \(T\)인 sequence의 모든 순열 집합 을 뜻하고, \(z_t\)는 \(Z_T\)에서 \(t\)번째 element를 뜻한다. \(z_{&amp;lt;t}\)는 \(Z_T\)에서 \(0\) ~ \(t-1\)번째 원소들을 뜻한다.&lt;/p&gt;

&lt;p&gt;위의 Objective Function은 \(x_i\)에 대해 \(x_i\)를 제외한 모든 \(x_t\)를 전체 집합으로 하는 순열에 대해 likelihood를 구하게 된다. AR의 구조를 채택했으나 순열을 사용해 bidirectional context까지 학습하도록 한 것이다.&lt;/p&gt;

&lt;h2 id=&quot;architecture-two-stream-self-attention-for-target-aware-representations&quot;&gt;Architecture: Two-Stream Self-Attention for Target-Aware Representations&lt;/h2&gt;

&lt;p&gt;일반적인 Transformer의 Self-Attention 구조에서는 Query, Key, Value가 모두 같은 값으로 시작하게 된다. 즉, 하나의 hidden state의 값을 공유한다. 그러나 XLNet에서는 구조상 Query의 값과 Key, Value의 값이 분리되어야 한다. 이를 위해 새로운 representation을 추가하게 된다.&lt;/p&gt;

&lt;p&gt;구체적인 예시를 들어보자. \(T = 4\)일 때, 두가지의 순열이 선택되었다고 하자.&lt;/p&gt;

\[Z_1 = [x_2,x_3,x_1,x_4]\]

\[Z_2 = [x_2,x_3,x_4,x_1]\]

&lt;p&gt;\(Z_1\)에서 \(t=3\)에 대한 조건부 확률을 구하는 식은 다음과 같다.&lt;/p&gt;

\[p\left(x_1|x_{z_{&amp;lt;3}}\right) =p\left(x_1|x_2,x_3\right)=\frac{exp\left(e\left(x_1\right)^Th_\theta\left(x_2,x_3\right)\right)}{\sum_{x'}{exp\left(e\left(x'\right)^Th_\theta\left(x_2,x_3\right)\right)}}\]

&lt;p&gt;\(Z_2\)에서 \(t=3\)에 대한 조건부 확률을 구하는 식은 다음과 같다.&lt;/p&gt;

\[p\left(x_4|x_{z_{&amp;lt;3}}\right) =p\left(x_4|x_2,x_3\right)=\frac{exp\left(e\left(x_4\right)^Th_\theta\left(x_2,x_3\right)\right)}{\sum_{x'}{exp\left(e\left(x'\right)^Th_\theta\left(x_2,x_3\right)\right)}}\]

&lt;p&gt;위의 두 조건부확률 식은 분모는 완전히 같은 값이다. 만약 \(x_1\)과 \(x_4\)가 같은 word였다고 한다면 (a, an, the와 같은 관사 등) 완전히 같은 조건부 확률을 계산하는 상황이 발생하게 된다. 직전 시점 \(t-1\)까지의 정보 embedding 정보만을 저장하는 representation \(h_\theta\left(x_{z_{&amp;lt;t}}\right)\)만으로는 이러한 문제를 해결할 수 없다. 따라서 현재 시점의 위치정보 까지 받는 새로운 representation \(g_\theta\left(x_{z_{&amp;lt;t}},z_t\right)\)을 추가한다.  최종적으로 아래의 수식을 정의하게 된다.&lt;/p&gt;

\[p\left(X_{z_t}=x|x_{z_{&amp;lt;t}}\right) =\frac{exp\left(e\left(x\right)^Tg_\theta\left(x_{z&amp;lt;t},z_t\right)\right)}{\sum_{x'}{exp\left(e\left(x'\right)^Tg_\theta\left(x_{z&amp;lt;t},z_t\right)\right)}}\]

&lt;p&gt;두 representation에 대해 자세히 알아보자.&lt;/p&gt;

&lt;h3 id=&quot;content-representation&quot;&gt;Content Representation&lt;/h3&gt;

&lt;p&gt;\(h_\theta\left(x_{z&amp;lt;t}\right)\)는 기존 Transformer의 hidden state와 동일한 구조이다. 현재 시점(\(t\))의 정보까지 포함해 입력으로 받는다. 이를 Content Representation이라고 하고, Key, Value에 사용하게 된다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-08-16-XLNet-Generalized-Autoregressive-Pretraining-for-Language-Understanding/01.jpg&quot; alt=&quot;01.jpg&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;query-representation&quot;&gt;Query Representation&lt;/h3&gt;

&lt;p&gt;\(g_\theta\left(x_{z_{&amp;lt;t}},z_t\right)\)는 현재 시점(\(t\))의 정보는 제외하고 입력으로 받는다. 대신 현재 시점(\(t\))의 위치 정보(\(z_t\))는 입력으로 받는다. 이를 Query Representation이라고 하고, Query에 사용하게 된다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-08-16-XLNet-Generalized-Autoregressive-Pretraining-for-Language-Understanding/02.jpg&quot; alt=&quot;02.jpg&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;permutation-language-modeling-with-two-stream-attention&quot;&gt;Permutation Language Modeling with Two-Stream Attention&lt;/h3&gt;

&lt;p&gt;전체적인 Two-Stream Attention의 구조는 아래와 같다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-08-16-XLNet-Generalized-Autoregressive-Pretraining-for-Language-Understanding/03.jpg&quot; alt=&quot;03.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Query의 초기값은 weight \(w\), Key와 Value의 초기 값은 embedding된 input 값 \(e\left(x_i\right)\)이다. 이후 아래와 같이 갱신된다.&lt;/p&gt;

&lt;p&gt;Query Stream은 현재 시점 \(t\)의 위치 정보(\(z_t\))는 알 수 있지만, 실제 값(\(x_{z_t}\))는 알지 못하는 상태로 구해진다.&lt;/p&gt;

&lt;p&gt;Content Stream은 현재 시점 \(t\)의 위치 정보(\(z_t\))는 물론, 실제 값(\(x_{z_t}\))도 사용해 구해진다.&lt;/p&gt;

\[g_{z_t}^{\left(m\right)} = Attention\left(Q=g_t^{\left(m-1\right)},KV=h_{z_{&amp;lt;t}}^{\left(m-1\right)};\theta\right)\]

\[h_{z_t}^{\left(m\right)}=Attention\left(Q=h_{z_t}^{\left(m-1\right)},KV=h_{z_{z\leq t}}^{\left(m-1\right)};\theta\right)\]

&lt;p&gt;\(m\)은 Multi-head Atention Layer의 현재 Layer Number이다.&lt;/p&gt;

&lt;h2 id=&quot;incorporating-ideas-from-transformer-xl&quot;&gt;Incorporating Ideas from Transformer-XL&lt;/h2&gt;

&lt;h2 id=&quot;modeling-multiple-segments&quot;&gt;Modeling Multiple Segments&lt;/h2&gt;

&lt;p&gt;BERT의 input과 동일한 구조를 채택했다. [CLS, A, SEP, B, SEP]의 구조이다. [CLS], [SEP] token은 BERT와 동일한 역할이고, [A], [B]는 각각 sentence A, sentence B이다. BERT와의 차이점은 NSP (Next Sentence Predict)를 Pretraining에 적용하지 않은 것인데, 유의미한 성능 향상이 없었기 때문이라고 한다.&lt;/p&gt;

&lt;h3 id=&quot;relative-segment-encodings&quot;&gt;Relative Segment Encodings&lt;/h3&gt;

&lt;p&gt;BERT의 segment embedding은 \(S_A\)와 \(S_B\) 등으로 \(A\)문장인지, \(B\)문장인지를 드러냈다. XLNet에서는 Transformer-XL의 relative positional encoding의 idea를 segment에도 적용해 relative한 값으로 표현했다. XLNet의 Segment Encoding은 두 position \(i, j\)가 같은 segment라면 \(s_+\), 다른 segment라면 \(s_-\)로 정의된다. \(s_+\)와 \(s_-\)는 모두 training 과정에서 학습되는 parameters이다. 이러한 relative segment encoding은 재귀적으로 segment encoding을 찾아내면서 generalization된 표현이 가능하다는 점, 두 개 이상의 segment input에 대한 처리 가능성을 열었다는 점에서 의의가 있다.&lt;/p&gt;

&lt;h2 id=&quot;discussion&quot;&gt;Discussion&lt;/h2&gt;

&lt;p&gt;구체적인 예시를 들어 BERT와 비교해보자. BERT와 XLNet이 “New York is a city.”라는 문장을 pretraining하는 상황이다. [New, York]의 두 token을 predict하는 것이 목표이다. BERT의 objective는 다음의 수식이다.&lt;/p&gt;

\[J_{BERT}=log{\ p\left(New\ |\ is\ a\ city\right)} + log{\ p\left(York\ |\ is\ a\ city\right)}\]

&lt;p&gt;XLNet은 순열을 특정해야 objective를 구체화할 수 있다. [is, a, city, New, York]의 순열이라고 가정하자. 다음의 수식이 XLNet의 objective이다.&lt;/p&gt;

\[J_{XLNet}=log{\ p\left(New\ |\ is\ a\ city\right)} + log{\ p\left(York\ |\ \textbf{New}\ is\ a\ city\right)}\]

&lt;p&gt;XLNet은 AutoRegressive Model이기 때문에 input sentence에 변형을 가하지 않고, 따라서 predict target word 사이의 dependency 역시 학습할 수가 있다. 위의 예시에서는 ‘York’를 predict할 때에 ‘New’ token의 정보를 활용했다.&lt;/p&gt;

&lt;h1 id=&quot;experiments&quot;&gt;Experiments&lt;/h1&gt;

&lt;h2 id=&quot;pretraining-and-implementation&quot;&gt;Pretraining and Implementation&lt;/h2&gt;

&lt;p&gt;Pretraining의 Dataset으로 BooksCorpus, Giga5, CLue Web2012-B, Common Crawl dataset의 Dataset을 사용했다. Google SentencePiece Tokenizer를 사용했다. XLNet-Large는 512 TPU v.3를 사용해 2.5일동안 500K step의 학습을 진행했다. 이 때 Adam Optimizer를 사용했다. Dataset의 크기에 비해 학습량이 적어 Unerfitting된 상태이지만, Pretraining을 더 수행한다고 하더라도 실제 downstream task에서 유의미한 성능 향상은 없었다.&lt;/p&gt;

&lt;h2 id=&quot;fair-comparison-with-bert&quot;&gt;Fair Comparison with BERT&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-08-16-XLNet-Generalized-Autoregressive-Pretraining-for-Language-Understanding/04.jpg&quot; alt=&quot;04.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;XLNet-Large는 모든 task에서 BERT-Large보다 좋은 성능을 보였다.&lt;/p&gt;

&lt;h2 id=&quot;comparison-with-roberta-scailing-up&quot;&gt;Comparison with RoBERTa: Scailing Up&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-08-16-XLNet-Generalized-Autoregressive-Pretraining-for-Language-Understanding/05.jpg&quot; alt=&quot;05.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;XLNet은 RACE task에서도 BERT, GPT, RoBERTa 등의 model들보다 좋은 성능을 보였다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-08-16-XLNet-Generalized-Autoregressive-Pretraining-for-Language-Understanding/06.jpg&quot; alt=&quot;06.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;XLNet은 SQuAD2.0 task에서도 BERT, RoBERTa보다 좋은 성능을 보였다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-08-16-XLNet-Generalized-Autoregressive-Pretraining-for-Language-Understanding/07.jpg&quot; alt=&quot;07.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;XLNet은 GLUE task에서도 BERT, RoBERTa보다 좋은 성능을 보였다.&lt;/p&gt;

&lt;h2 id=&quot;ablation-study&quot;&gt;Ablation Study&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-08-16-XLNet-Generalized-Autoregressive-Pretraining-for-Language-Understanding/08.jpg&quot; alt=&quot;08.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;1~4를 살펴보면 XLNet-Base가 BERT나 Transformer-XL보다 좋은 성능을 보인다. 이를 통해 permutation language modeling objective가 효과적이었다는 것을 알 수 있다.&lt;/p&gt;

&lt;p&gt;1~2를 살펴보면 Transformer-XL이 BERT보다 RACE와 SQuAD2.0 task에서 더 좋은 성능을 보인다. 이를 통해 Transformer-XL 계열의 model이 long sequence modeling에 효과적이라는 것을 알 수 있다.&lt;/p&gt;

&lt;p&gt;4~5행을 살펴보면 memory caching mechanism이 빠진 경우 RACE나 SQuAD2.0과 같은 long sequence task에서 성능 저하가 있었다는 것을 알 수 있다.&lt;/p&gt;

&lt;p&gt;4, 6~7행을 살펴보면 span-based prediction과 bidirectional data가 성능 향상에 기여했다는 것을 알 수 있다.&lt;/p&gt;

&lt;p&gt;마지막으로 4, 8행을 통해 NSP가 RACE task를 제외한 모든 경우에서 오히려 성능을 하락시켰다는 것을 알 수 있다.&lt;/p&gt;

&lt;h1 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h1&gt;

&lt;p&gt;Permutation을 사용한 Autoregressive Pretraining 방식을 개척했다는 점에서 의의가 있다.&lt;/p&gt;
</description>
        <pubDate>Sat, 15 Aug 2020 19:00:00 -0500</pubDate>
        <link>http://0.0.0.0:4000/machine%20learning/paper%20review/XLNet-Generalized-Autoregressive-Pretraining-for-Language-Understanding/</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/machine%20learning/paper%20review/XLNet-Generalized-Autoregressive-Pretraining-for-Language-Understanding/</guid>
        
        <category>NLP</category>
        
        
        <category>Machine Learning</category>
        
        <category>Paper Review</category>
        
      </item>
    
      <item>
        <title>[NLP 논문 리뷰] BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding</title>
        <description>&lt;h2 id=&quot;paper-info&quot;&gt;Paper Info&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1810.04805&quot;&gt;Archive Link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1810.04805.pdf&quot;&gt;Paper Link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Submit Date: Oct 11, 2018&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;NLP에서도 pre-trained된 model을 사용하는 기법은 있었지만, pre-train에서 단방향의 architecture만 사용할 수 있다는 한계점이 있었다. 이는 양방향에서의 문맥 정보가 모두 중요한 token-level task에서 좋은 성능을 보이지 못하는 원인이 되었다. 본 논문에서는 MLM(Masked Language Model)을 사용해 bidirectional한 context도 담을 수 있는 BERT Model을 제시한다. MLM이란 문장에서 random하게 단어를 선택해 masking을 하고, model이 bidirectional context를 통해 해당 단어를 predict하도록 하는 것이다. BERT는 pre-train을 통해 task에 대한 학습이 아닌 language 자체에 대한 학습을 한 뒤, task에 맞게 fine-tuning을 하고, pre-training 과정에서 bidirectional한 context를 학습한다. 그 결과, BERT는 11개의 NLP task에서 SOTA를 달성했다.&lt;/p&gt;

&lt;h1 id=&quot;related-work&quot;&gt;Related Work&lt;/h1&gt;

&lt;p&gt;language의 context를 학습하는 pre-training 방법은 크게 2가지로 구분된다.&lt;/p&gt;

&lt;h2 id=&quot;feature-based-approach&quot;&gt;Feature-based Approach&lt;/h2&gt;

&lt;p&gt;task-specific한 model이 pre-trained된 model을 feature로 사용한다. pre-trained된 feature를 concat해서 model의 input으로 사용하는 방식 등이 있다.&lt;/p&gt;

&lt;h2 id=&quot;fine-tuning-approach&quot;&gt;Fine-tuning Approach&lt;/h2&gt;

&lt;p&gt;pre-train에서 일반적인 task를 위한 model을 학습하는데, 이 때 task-specific한 parameter를 최대한 배제한다. 이후 downstream task에서 이전에 학습된 parameter들을 task specific하게 fine-tuning한다. 즉, language에 대해 학습된 값으로 initialized된 상태에서 task specific한 layer를 추가한 뒤 fine-tuning을 시작한다. 이 때 pre-train된 parameter들과 task specific layer의 parameter들이 모두 학습된다.&lt;/p&gt;

&lt;h1 id=&quot;bert&quot;&gt;BERT&lt;/h1&gt;

&lt;p&gt;BERT는 Fine-tuning Approch를 채택했다. 따라서 Pre-training, Fine-tuning의 2가지 Step으로 구분된다.&lt;/p&gt;

&lt;p&gt;Pre-Training에서는 Unsupervised Learning을 통해 Language 자체의 representation을 학습한다. 특정 task에 부합하는 학습이 아닌 language의 일반적 특성을 학습한다. 이후 Fine-tuning에서는 각각의 task에 맞는 labeled data를 사용해 Supervised Learning을 수행한다. 이 과정에서 Pre-trained 단계에서 학습을 통해 얻어진 parameters의 값들도 변경된다. 즉, Pre-training은 parameters의 초기값이 language의 일반적 특성을 담는 값으로 시작하도록 설정하는 역할이다.&lt;/p&gt;

&lt;h2 id=&quot;architecture&quot;&gt;Architecture&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-07-19-BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding/01.jpg&quot; alt=&quot;01.jpg&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;inputoutput-representation&quot;&gt;Input/Output Representation&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-07-19-BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding/02.jpg&quot; alt=&quot;02.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Input은 Token Embedding + Segment Embedding + Position Embedding이다. Token은 실제 word에 대한 embedding, Segment Embedding은 몇번째 Sentence에 포함되는지에 대한 Embedding, Position Embedding은 Input에서 몇번째 word인지에 대한 Embedding이다. Transformer에서도 언급했듯이 병렬 처리를 위해 RNN을 제거하고, Sequential한 정보를 보존하기 위해 Position Embedding을 추가한 것이다. BERT는 Input으로 최대 2개의 Sentence까지 입력받을 수 있는데, 이는 Q&amp;amp;A task와 같은 2개의 문장에 대한 task도 처리할 수 있게 하기 위함이다. 이를 처리하기 위해 Seperate Token SEP을 추가했다. 이와 별개로 Classification을 위한 CLS Token도 Input Sequence의 제일 앞에 항상 위치하는데, Transformer Encoder의 최종 Output에서 CLS Token과 대응되는 값은 Classification을 처리하기 위해 sequence representation을 종합해서 담게 된다.&lt;/p&gt;

&lt;h2 id=&quot;pre-training&quot;&gt;Pre-training&lt;/h2&gt;

&lt;p&gt;BERT는 MLM과 NSP라는 2가지의 Unsupervised task를 사용해 Pre-training을 수행한다.&lt;/p&gt;

&lt;h3 id=&quot;mlm-masked-language-model&quot;&gt;MLM (Masked Language Model)&lt;/h3&gt;

&lt;p&gt;기존의 전통적인 Pre-train은 left to right model과 right to left model을 단순하게 concat한 뒤 사용했다는 점에서 제대로 된 Bidirectional context를 담지 못했다. BERT는 MLM을 사용해 진정한 의미의 Bidirectional context를 담게 된다. 기존의 Model들이 unidirectional model의 결과들을 concat해서 사용한 이유는, bidirectional model은 word 자기 자신을 masking하더라도 다층 Layer에서는 간접적으로 자기 자신에 대한 정보를 알 수 있기에 제대로 된 학습이 불가능했기 때문이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-07-19-BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding/03.jpg&quot; alt=&quot;03.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 왼쪽 Model은 BERT로, Bidirectional Model이다. 반면 오른쪽 Model은 OpenAI GPT로, left to right Unidirectional Model이다. Bidirectional Model에서 다층 Layer일 경우에는 이전 Layer의 모든 Output에서 모든 Token에 대한 정보를 담게 되기 때문에 특정 Token을 Masking했다고 하더라도 다음 Layer에서는 자기 자신에 대한 정보를 간접적으로 참조할 수 있게 된다. 반면 Unidirectional Model에서는 이런 일이 발생하지 않는다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-07-19-BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding/04.jpg&quot; alt=&quot;04.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이를 해결하기 위해 BERT는 Masking을 하되, 그 중 80%에 대해서만 실제 MASK Token으로 변경하고, 10%에 대해서는 random한 다른 Token으로, 나머지 10%에 대해서는 변경을 하지 않았다. Masking을 하는 비율은 전체 Word 중에서 15%만 수행했으므로, 실제로 MASK Token으로 변경되는 비율은 12%밖에 되지 않는다. 이를 통해 얻을 수 있는 이점은 Model이 모든 Token에 대해서 실제로 맞는 Token인지 의심을 할 수 밖에 없게 되기에 제대로 된 학습을 이뤄낼 수 있다. 이는 Bidirectional Model의 한계인 간접 참조도 해결했다.&lt;/p&gt;

&lt;h3 id=&quot;nsp-next-sentence-prediction&quot;&gt;NSP (Next Sentence Prediction)&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-07-19-BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding/05.jpg&quot; alt=&quot;05.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;QA, NLI와 같은 task는 token단위보다 sentence 단위의 관계가 더 중요하다. 위의 MLM만으로 Pre-training을 하게 될 경우 token level의 정보만을 학습하기 때문에 NSP를 통해 sentence level의 정보도 담기로 한다. 두 문장이 서로 연결되는 문장인지를 isNext, NotNext의 Binary classification으로 해석하게 된다. 50%의 확률로 isNext, NotNext의 data를 생성한 뒤에 학습을 시킨다. 이 때 위에서 언급한 CLS Token이 주요하게 사용된다.&lt;/p&gt;

&lt;h3 id=&quot;pre-training-detail&quot;&gt;Pre-training Detail&lt;/h3&gt;

&lt;p&gt;실제 Pre-training 단계는 다음과 같이 진행된다.&lt;/p&gt;

&lt;p&gt;corpus에서 문장들을 뽑아내 두 문장의 sequence로 만들고, 각각 A, B를 Segment Embedding으로 부여한다. 50%의 확률로 B 문장은 실제로 A문장에 이어지는 문장이고(IsNext), 50%의 확률로 A문장에 이어지지 않는 문장이다(NotNext). 이후 Word Piece Tokenization을 한 뒤, Masking을 수행한다.&lt;/p&gt;

&lt;p&gt;Hyperparameters는 다음과 같다.&lt;/p&gt;

&lt;p&gt;batch size = 256 sequences&lt;/p&gt;

&lt;p&gt;sequence size = 512 tokens&lt;/p&gt;

&lt;p&gt;#epoch = 40&lt;/p&gt;

&lt;p&gt;Optimizer: Adam (learning rate = 1e-4, B_1 = 0.9, B_2 = 0.999, L2 weight decay = 0.01)&lt;/p&gt;

&lt;p&gt;dropout: 0.1 in all layers&lt;/p&gt;

&lt;p&gt;activation function: gelu&lt;/p&gt;

&lt;p&gt;loss function: sum of the mean MLM likelihood + mean NSP likelihood&lt;/p&gt;

&lt;p&gt;Pre-training에 BERT_BASE는 16개의 TPU로 4일, BERT_LARGE는 64개의 TPU로 4일이 소요됐다.&lt;/p&gt;

&lt;h2 id=&quot;fine-tuning&quot;&gt;Fine-tuning&lt;/h2&gt;

&lt;p&gt;Fine-tuning은 Pre-training에 비해 매우 빠른 시간 내에 완료된다. Fine-tuning에서는 각각의 task에 specific하게 input size, output size 등을 조정해야 한다. 또한 token-level task일 경우에는 모든 token들을 사용하고, sentence-level task일 경우에는 CLS token을 사용한다.&lt;/p&gt;

&lt;p&gt;Hyperparameters는 대부분은 pre-training과 동일하게 진행하는 것이 좋지만, batch size, learning rate, #epochs는 task-specific하게 결정해야 한다. 하지만 다음의 값들에 대해서는 대체적으로 좋은 성능을 보였다.&lt;/p&gt;

&lt;p&gt;batch size: 16, 32&lt;/p&gt;

&lt;p&gt;learing rate: 5e-5, 3e-5, 2e-5&lt;/p&gt;

&lt;p&gt;#epochs: 2, 3, 4&lt;/p&gt;

&lt;p&gt;dataset의 크기가 클 수록 Hyperparameter의 영향이 줄어들었으며, 대부분의 경우 Fine-tuning은 매우 빠른 시간 내에 완료되기 때문에 많은 parameters에 대해 테스트를 진행할 수 있었다.&lt;/p&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-07-19-BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding/06.jpg&quot; alt=&quot;06.jpg&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;glue-general-language-understanding-evaluation&quot;&gt;GLUE (General Language Understanding Evaluation)&lt;/h3&gt;

&lt;p&gt;GLUE 에 맞춰 fine-tuning을 진행한다. CLS token에 대응하는 hidden layer의 state 값이 h차원의 vector C라고 한다면, Classification을 위해 k x h (k는 classification할 label의 수) 차원의 weight matrix W를 생성한다.&lt;/p&gt;

\[log(softmax(CW^T))\]

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-07-19-BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding/07.jpg&quot; alt=&quot;07.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;BERT_BASE는 L=12, H=768, A=12, #Parameters=110M이고, BERT_LARGE는 L=24, H=1024, A=16, #Parameters=340M이다. (L: Layer 개수, H: hidden size, A: self-attention head 개수)&lt;/p&gt;

&lt;p&gt;batch size는 32, #epoch=3로 학습을 진행했다. learning rate는 5e-5, 4e-5, 3e-5, 2e-5 중 가장 잘 학습이 진행되는 것으로 선택했다. fine-tuning이 unstable한 경우가 있어서, random하게 restart하는 과정도 추가했다.&lt;/p&gt;

&lt;p&gt;BERT model은 GLUE의 모든 task에서 SOTA를 달성했다. base, large 모두 다른 model들을 4.5~7%정도 능가했다. 또한 BERT_LARGE model이 BERT_BASE를 모든 경우에서 능가했다. Model의 크기가 클 수록 성능이 좋다는 의미이다.&lt;/p&gt;

&lt;p&gt;BERT_BASE와 OpenAI GPT는 attention masking에서의 차이를 제외하고 모두 동일한 조건이었음에도 불구하고 4.5%의 성능 차이를 보였다. 이를 통해 unidirectional attention이 아닌 bidirectional attention을 적용한 것이 성능 향상에 매우 큰 기여를 했다는 것을 알 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;squad-v11&quot;&gt;SQuAD v1.1&lt;/h3&gt;

&lt;p&gt;SQuAD에 맞춰 Fine-tuning을 진행한다. SQuAD는 Q&amp;amp;A dataset이다. fine-tuning 학습 과정에서 Answer 문장의 시작 token S와 끝 token E를 구해내게 된다. transformer layer의 마지막 state들을 이용해 해당 값이 S나 E일 확률을 구하게 되는데, S와 T_i를 dot product한 후 softmax로 확률값으로 변환한다.&lt;/p&gt;

\[P_i = \frac{e^{(S or E) \cdot T_i}}{\sum _j{e^{(S or E) \cdot T_j}}}\]

&lt;p&gt;위의 score를 이용해서 \(S \cdot T_i\)와 \(E \cdot T_j\)를 더한 값이 가장 큰 &amp;lt;i, j&amp;gt;쌍 (단, j ≥ i)을 최종 Answer 영역으로 정한다.&lt;/p&gt;

\[\hat s_{i,j} = max_{j \geq i}\left( S \cdot T_i + E \cdot T_j\right)\]

&lt;h3 id=&quot;squad-v20&quot;&gt;SQuAD v2.0&lt;/h3&gt;

&lt;p&gt;SQuAD v1.1에서 대답이 불가능한 질문을 포함한 dataset이다. fine-tuning 학습 과정에서 CLS token을 이용해 대답 가능 여부를 Binary Classification하면서 token C를 구한다.&lt;/p&gt;

&lt;p&gt;대답이 불가능할 경우의 Score는 다음으로 계산한다.&lt;/p&gt;

\[s_{null} = S \cdot C + E \cdot C\]

&lt;p&gt;대답이 가능한 경우의 Score는 SQuAD v1.1과 동일하다. 대답 가능 여부는 두 Score를 비교해 판단하게 된다. 이 때 threshhold값 r이 사용된다.&lt;/p&gt;

\[\hat s_{i,j} &amp;gt; s_{null} + r\]

&lt;h3 id=&quot;swag-situations-with-adversarial-generations&quot;&gt;SWAG (Situations With Adversarial Generations)&lt;/h3&gt;

&lt;p&gt;일반적인 추론을 하는 dataset이다. sentence 1개에 추가적으로 4개의 sentence가 주어지고, 4개의 sentence 중 가장 적합한 것을 선택하는 task이다. 각각의 보기들에 대해 앞의 sentence와 묶어 segment embedding A, B를 부여한 data쌍을 만들어낸다. Score는 CLS token에 대응하는 token C와 task specific parameter 1개를 dot product한 뒤 softmax를 수행해서 구한다.&lt;/p&gt;

&lt;h2 id=&quot;ablation-studies&quot;&gt;Ablation Studies&lt;/h2&gt;

&lt;p&gt;동일한 환경(pre-training data, fine-tuning scheme, hyperparameters)에서 특정 조건만을 변경해 어느 정도의 영향을 끼치는지 분석했다.&lt;/p&gt;

&lt;h3 id=&quot;effect-of-pre-training-tasks&quot;&gt;Effect of Pre-training Tasks&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-07-19-BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding/08.jpg&quot; alt=&quot;08.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;No NSP는 pre-training 단계에서 MLM만 수행하고, NSP는 수행하지 않은 model이다. LTR &amp;amp; No NSP는 NSP 는 수행하지 않고, MLM 대신 Left to Right의 Unidirectional attention을 적용한 model이다.&lt;/p&gt;

&lt;p&gt;BERT와 No NSP를 비교함으로써 NSP Pre-training이 성능 향상에 영향을 끼친다는 것을 알 수 있다. 한편, No NSP와 LTR &amp;amp; No NSP를 비교함으로써 Bidirectional Attention(MLM)이 성능 향상에 매우 큰 영향을 준다는 것을 알 수 있다. Token Level에서 Right to Left Context 정보를 얻기 위해 MLM이 아닌 BiLSTM을 추가했다. 해당 Model은 LTR &amp;amp; No NSP에 비해 SQuAD와 같은 task에서 매우 큰 성능 향상을 보였지만, 여타 task에서는 오히려 성능 하락을 보였다.&lt;/p&gt;

&lt;p&gt;물론 Bidirectional Context를 담기 위해 LTR과 RTL을 각각 학습시킨 뒤 두 token을 concatenation하는 방법도 있지만, 이는 비용이 2배나 높고, QA와 같은 RTL 학습이 불가능한 task에서는 적용이 불가능하다는 점, 결론적으로 MLM과 같은 Deep Bidirectional Model에 비해 성능이 낮다는 점에서 비효율적이다.&lt;/p&gt;

&lt;h3 id=&quot;effect-of-model-size&quot;&gt;Effect of Model Size&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-07-19-BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding/09.jpg&quot; alt=&quot;09.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;GLUE task 중 3개를 뽑아 Model Size에 따라 성능을 측정했다. Model Size가 증가할수록 성능이 높아지는 경향을 확인할 수 있다. 특히 MRPC task는 pre-training task와 차이가 큰 task이면서 3600개의 적은 labeled training data를 사용했음에도 불구하고 Model Size가 증가함에 따라 성능도 향상됐다. 이를 통해 Model Size의 증가는 번역과 Langauge Modeling과 같은 큰 scale의 task에서도 성능 향상에 기여함은 물론, 충분한 pre-training이 있었다는 전제 하에 작은 scale의 task에서도 성능 향상에 기여함을 알 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;feature-based-approach-with-bert&quot;&gt;Feature-based Approach with BERT&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-07-19-BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding/10.jpg&quot; alt=&quot;10.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;지금까지의 BERT는 모두 fine-tuning model이었다. Feature-based Approach가 갖는 장점은 크게 두 가지로 정리할 수 있다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Transformer Encoder의 Output에 몇몇 Layer를 추가하는 간단한 작업만으로는 해결할 수 없는 task들이 존재한다.&lt;/li&gt;
  &lt;li&gt;매우 큰 pre-training model의 경우 pre-trained된 features를 계속 update하지 않고 고정된 값으로 사용함으로써 연산량을 획기적으로 줄일 수 있다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;NER(Named Entity Recognition) task에 대해 Feature-based Approach를 적용해본다. Fine-tuning Step을 제거하고, Transformer Layer의 output을 그대로 768-dimensional BiLSTM의 input으로 사용한다. 그 뒤 classification layer를 통과시켜 결과를 도출해낸다. Fine-tuning Approach를 적용한 BERT_LARGE Model이 SOTA를 달성했지만, Fine-tunning을 적용한 BERT_BASE model과 Feature-based를 적용한 BERT_BASE Model의 F1 Score는 0.3밖에 차이가 나지 않는다. 연산량을 고려한다면 충분히 가치있는 결과이다. 결론적으로, BERT Model은 Fine-tunning Approach와 Feature-based Approach에서 모두 기존 Model들을 뛰어넘는다.&lt;/p&gt;

&lt;h3 id=&quot;effect-of-number-of-training-steps&quot;&gt;Effect of Number of Training Steps&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-07-19-BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding/11.jpg&quot; alt=&quot;11.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Training을 많이 수행할 수록 성능은 향상되지만, 일정 수준 이상을 지나면 점차 converge하게 된다. MLM과 LTR을 비교했을 때 MLM이 수렴이 더 늦게 일어나기 때문에 Training에 더 많은 시간이 소요된다고 볼 수 있다. 하지만 절대적인 성능 수치는 시작과 거의 동시에 LTR을 뛰어넘는다.&lt;/p&gt;

&lt;h3 id=&quot;different-masking-procedure&quot;&gt;Different Masking Procedure&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-07-19-BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding/12.jpg&quot; alt=&quot;12.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Masking Rate를 다르게 하며 성능을 비교해보자. MASK는 실제로 [MASK] token으로 변경된 비율을, SAME은 동일한 word로 남아있는 비율을, RND는 random한 다른 word로 변경된 비율을 뜻한다. BERT는 각각 80%, 10%, 10%를 채택했다. MASK가 100%나 RND가 100%인 경우에 성능이 최악이라는 것을 알 수 있다.&lt;/p&gt;
</description>
        <pubDate>Sat, 18 Jul 2020 19:00:00 -0500</pubDate>
        <link>http://0.0.0.0:4000/machine%20learning/paper%20review/BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding/</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/machine%20learning/paper%20review/BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding/</guid>
        
        <category>NLP</category>
        
        
        <category>Machine Learning</category>
        
        <category>Paper Review</category>
        
      </item>
    
      <item>
        <title>[NLP 논문 리뷰] Attention Is All You Need (Transformer)</title>
        <description>&lt;h2 id=&quot;paper-info&quot;&gt;Paper Info&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot;&gt;Archive Link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1706.03762.pdf&quot;&gt;Paper Link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Submit Date: Jun 12, 2017&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;RNN과 LSTM을 사용한 Neural Network 접근 방식은 Sequencial Transduction Problem에서 매우 좋은 성능을 달성했다. 그 중 특히 Encoder-Decoder를 사용한 Attention Model이 state-of-art를 달성했다. 하지만 Recurrent Model은 본질적으로 한계가 존재하는데, 바로 Parallelization이 불가능하다는 문제점이다. 이는 Sequence의 길이가 긴 상황에서 매우 큰 단점이 된다. 최근의 연구들이 computation을 최소화하는 방향으로 Model의 성능 향상을 이뤄내기는 했지만, 결국 Recurrent Model의 본질적인 문제는 해결하지 못했다. 본 논문에서 소개하는 Transformer Model은 RNN을 제거해 Recurrent Model의 문제점에서 벗어났고,  Parallelization을 가능하게 했다. 이에 따라 매우 좋은 성능을 달성했다.&lt;/p&gt;
&lt;h1 id=&quot;model-architecture&quot;&gt;Model Architecture&lt;/h1&gt;

&lt;p&gt;Transformer Model은 attention seq2seq model과 비슷한 구조를 지닌다. Encoder-Decoder가 존재하고, fixed length의 하나의 context vector를 사용하는 방식이 아닌, 하나의 output word마다 각기 다른 새로운 attention을 갖는 context vector를 생성해 활용한다. 차이점은 RNN을 제거했다는 점이다. NLP에서 RNN을 사용하는 가장 큰 이유는 sequential 정보를 유지하기 위함(각 단어들의 순서 및 위치 정보를 활용하기 위함)이다. Transformer에서는 RNN 대신 FC Layer를 사용하되, 각 word vector마다 positional Encoding 과정을 추가해 각 word의 position 정보를 word vector 안에 추가했다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-07-05-Attention-is-All-You-Need/01.jpg&quot; alt=&quot;01.jpg&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;encoder&quot;&gt;Encoder&lt;/h3&gt;

&lt;p&gt;Transformer의 Encoder는 6개의 동일한 Encoder Layer를 Stack 구조로 쌓아올린 형태이다. 각각의 Encoder Layer는 2개의 SubLayer로 구분되는데, Self Attention Layer와 Feed Forward Layer이다. Self Attention Layer에서는 word vector에 attention을 담는다. 이 결과를 Feed Forward Layer를 거쳐 다음 Encoder Layer의 input으로 넣는다. 이러한 과정을 6회 반복해 최종 output을 Decoder로 넘겨준다. 각각의 Self Attention Layer는 Attention을 word vector에 담는 기능을 수행하는데, 이를 중첩해 반복하면 단어 단위가 아닌 점차 전체 문맥에서의 Attention을 담게 된다. Self Attention Layer의 동작은 아래에서 자세히 살펴보겠다. 각 SubLayer의 결과에는 항상 Normalization이 수행되는데, 이 때 SubLayer를 통과하기 전 input이 그대로 더해지게 된다. 이를 Residual Connection이라고 한다. Residual Connection은  BackProp시 gradient vanishing을 최소화시켜 Model 성능 향상을 돕는다.&lt;/p&gt;

&lt;h3 id=&quot;self-attention-layer&quot;&gt;Self Attention Layer&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-07-05-Attention-is-All-You-Need/02.jpg&quot; alt=&quot;02.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-07-05-Attention-is-All-You-Need/03.jpg&quot; alt=&quot;03.jpg&quot; /&gt;&lt;/p&gt;

\[Attention\left(Q,K,V\right)=softmax\left(\frac{QK^T}{\sqrt{d_k}}\right)V\]

&lt;p&gt;Self Attention Layer는 word vector에 attention을 추가하는 layer이다. Query, Key, Value가 사용된다. Query, Key, Value는 각각 input word에 대해 특정한 weight matrix를 곱한 결과값이다. 해당 weight matrix들은 학습되는 parameter들이다. 우선 각 단어들을 기준으로 나눠 살펴보자.&lt;/p&gt;

&lt;p&gt;현재 단어의 word vector를 \(x_i\)라고 하면, query는 \(x_i\)에 대한 query \(q_i\)이다. input word \(x_i\)에 대한 연관성을 묻는 Query(질의)이다. Key와 Value는 \(x_i\)와 연관성을 검사하는 대상 \(x_j\)에 대한 값이다. 사실 Key와 Value는 동일한 값인데, 사용되는 위치만이 다르다. \(q_i\)와 \(k_j\)를 곱하면 \(x_i\)와 \(x_j\)가 연관된 수치를 의미하는 score가 생성된다. 이를 Dependency라고 한다. Dependency를 softmax에 넣으면 확률값이 생성된다. 이를 \(v_j\)와 곱하고, 해당 값을 모두 더하면 attention이 담기긴 vector가 완성된다.&lt;/p&gt;

&lt;p&gt;query, key, value는 모두 위의 단어 예시에서 vector 단위였다. 이를 concatenate해 하나의 matrix로 만들 수 있다. 그렇다면 dot product 연산으로 모든 sentence 전체에 대한 attention matrix를 구할 수 있다. 이렇게 병렬 처리를 함으로써 model의 학습 속도를 증가시켰다. 위의 좌측 이미지는 이러한 과정을 표현한 것이다. Scale은 \(d_k\)의 제곱근으로 Q와 K의 dot product 결과값을 나누는 것인데, 절댓값이 너무 커져 softmax에서 gradient가 줄어드는 것을 방지하기 위함이다.&lt;/p&gt;

&lt;p&gt;사실 이런 dot product를 통해 attention을 구하는 하나의 과정 자체도 병렬적으로 동시에 여러 번 수행된다. 위의 우측 이미지를 보면 dot product 연산을 병렬적으로 h번 수행해 모두 concatuation을 해 하나의 matrix를 생성한다는 것을 알 수 있다. 해당 matrix를 기존의 input matrix size로 변경하기 위해 다른 weight matrix \(W\)와 dot product를 수행한 뒤 그 결과를 다음 Feed Forward Layer에 넘기게 된다.&lt;/p&gt;

&lt;h3 id=&quot;decoder&quot;&gt;Decoder&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-07-05-Attention-is-All-You-Need/04.jpg&quot; alt=&quot;04.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;출처: &lt;a href=&quot;https://jalammar.github.io/illustrated-transformer/&quot;&gt;https://jalammar.github.io/illustrated-transformer/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Decoder는 Encoder와 매우 비슷한 구조를 갖는다. 동일한 6개의 Decoder Layer를 Stack 구조로 쌓아올린 형태이고, 각각의 Decoder Layer에는 Encoder Layer와 같은 Self Attention Layer와 Feed Forward Layer가 존재한다. Encoder와의 차이점은 Self Attention Layer 이전에 Masked Layer가 있다는 점이다. 6개의 Decoder Layer를 거친 최종 output 값을 softmax를 사용해 확률값으로 구하는데, 이는 i번째 output word에 대한 조건부 확률값이다. 6개의 Decoder Layer는 모두 마지막 Encoder Layer의 output인 context vector를 input으로 받고, 동시에 이전 단계의 Decoder Layer에서 생성된 attention vector도 input으로 받는다. Masked Attention Layer에서는 현재 word의 기준으로 이후 word에 mask를 씌운 vector값이다. 현재 시점의 attention을 생성할 때 이후 word들의 영향을 없애기 위함이다.&lt;/p&gt;
</description>
        <pubDate>Sat, 04 Jul 2020 19:00:00 -0500</pubDate>
        <link>http://0.0.0.0:4000/machine%20learning/paper%20review/Attention-is-All-You-Need/</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/machine%20learning/paper%20review/Attention-is-All-You-Need/</guid>
        
        <category>NLP</category>
        
        
        <category>Machine Learning</category>
        
        <category>Paper Review</category>
        
      </item>
    
      <item>
        <title>[NLP 논문 리뷰] Neural Machine Translation By Jointly Learning To Align And Translate (Attention Seq2Seq)</title>
        <description>&lt;h2 id=&quot;paper-info&quot;&gt;Paper Info&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1409.0473&quot;&gt;Archive Link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1409.0473.pdf&quot;&gt;Paper Link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Submit Date: Sep 1, 2014&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;

&lt;p&gt;기존의 seq2seq model에서 사용된 LSTM을 사용한 encoder-decoder model은 sequential problem에서 뛰어난 성능을 보였다. 하지만 encoder에서 생성해낸 context vector를 decoder에서 sentence로 만들어내는 위와 같은 방식에서 고정된 vector size는 긴 length의 sentence에서 bottleneck이 된다는 사실을 발견했다. 본 논문에서는 이러한 문제점을 해결하기 위해 source sentence의 정보를 context vector 하나에 담는 것이 아닌, 각 시점마다의 context vector를 따로 생성해 decoder에서 사용했다.&lt;/p&gt;

&lt;h1 id=&quot;decoder&quot;&gt;Decoder&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-05-17-Neural-Machine-Translation-By-Jointly-Learning-To-Align-And-Translate/01.jpg&quot; alt=&quot;01.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-05-17-Neural-Machine-Translation-By-Jointly-Learning-To-Align-And-Translate/02.jpg&quot; alt=&quot;02.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-05-17-Neural-Machine-Translation-By-Jointly-Learning-To-Align-And-Translate/03.jpg&quot; alt=&quot;03.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;\(i\)번째 token으로 \(y_i\)가 등장할 조건부 확률에 대한 수식이다. \(y_1 \sim y_{i-1}\) (\(y_i\) 이전의 output sentence)와 \(x\)(input sentence 전체)에 대해 다음 token으로 \(y_i\)가 생성될 조건부 확률이다. 이는 \(g\) 함수에 \(y_{i-1}\), \(s_i\),  \(c_i\)를 인자로 넣어 생성된 값이다. \(y_{i-1}\)은 직전 시점 \(i-1\)에서 생성한 output token이고, \(s_i\)는 현재 시점 \(i\)에서의 RNN hidden state, \(c_i\)는 현재 시점 i에 생성된 context vector이다. 직관적으로 해석해보면 이전 단어 \(y_{i-1}\) 이후에 나올 단어 \(y_i\)를 예측하는 것인데, 이 때 이전 output sentence의 상태 정보를 모두 포함하고 있는 \(s_i\)를 입력으로 받음으로써 output sentence의 문맥을 반영하고, input sentence에 대한 context vector \(c_i\)를 통해 input sentence의 문맥을 반영한다. 이전 seq2seq model에서는 context vector가 input sentence 전체에 대한 vector였는데, 이번 attention seq2seq model에서는 특정 시점 i에 대한 context vector \(c_i\)가 주어진다. 즉, context vector가 input sentence 전체에 대한 하나의 vector가 아니라 각 시점 i에 대해 \(c_i\)가 각각 정의된다는 것이다. 아래에서는 \(c_i\)에 대해 좀 더 자세하게 살펴본다.&lt;/p&gt;

\[c_i=\sum^{T_x}_{j=1}{\alpha_{ij}h_j}\]

&lt;p&gt;\(c_i\)는 \(a_{ij}\)와 \(h_j\)에 대해 \(j\)부터 \(T_x\)까지 더한 vector이다. \(j\)부터 \(T_x\)까지의 의미는 input sentence의 처음부터 끝까지 각 input token에 대해 \(j\)로 순회한다는 의미이다. 즉, \(j\)는 input sentence에서의 token index이다. 반대로 \(i\)는 output sentence에서 현재 시점 i이다. 정리하자면 \(j\)는 input에 대한 index, \(i\)는 output에 대한 index이다. 우리는 output의 \(i\) 시점에서 생성되는 context vector \(c_i\)에 대한 수식을 살펴보는 것이다. \(h_j\)는 input sentence 전체의 context를 포함하지만 동시에 특히 \(j\)번째 token과 그 주변에 대해 더 attention(집중)을 한 vector이다.\(a_{ij}\)는 \(i\)번째 output token이 \(j\)번째 input token과 align될 확률값을 뜻한다. \(a_{ij}\)에 대해 더 살펴보자.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-05-17-Neural-Machine-Translation-By-Jointly-Learning-To-Align-And-Translate/04.jpg&quot; alt=&quot;04.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-05-17-Neural-Machine-Translation-By-Jointly-Learning-To-Align-And-Translate/05.jpg&quot; alt=&quot;05.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;\(e_{ij}\)는 \(i\)번째에 들어올 output token과 \(j\)번째 input token이 얼마나 서로 잘 match되는지에 대한 값이다. output의 문맥을 반영하기 위해 \(s_{i-1}\)를 input으로 받고, \(j\)번째 input token에 대한 attention을 주기 위해 \(h_j\)를 input으로 받는다. 이렇게 완성된 \(e_{ij}\)를 softmax한 \(a_{ij}\)는 \(i\)번째 들어올 output token과 \(j\)번째 input token이 align될 확률값을 뜻한다.&lt;/p&gt;

&lt;p&gt;다시\(c_i\)의 의미로 되돌아와보면, \(c_i\)는 \(i\) 시점에서 모든 input sentence token \(j\)에 대해 \(a_{ij}\)와 \(h_j\)를 곱한 vector에 대한 합이다. \(a_{ij}\)는 현재 시점 \(a_i\)에서 생성될 output token이 \(j\)번째 input token과 align될 확률값이며, \(h_j\)는 input sentence에 대한 context이되, \(j\)번째 input token에 특히 attention한 vector이다. 결론적으로 \(c_i\)는 현재 시점 \(i\)에서 input sentence의 어느 token에 더 attention을 해야 하는지에 대한 context라고 직관적으로 해석 가능하다.&lt;/p&gt;

&lt;p&gt;위의 model이 기존 seq2seq model에 비해 가지는 이점은 encoder가 고정된 size context vector 1개에 모든 input sentence token에 대한 attention을 담을 필요가 없다는 것이다. 왜냐하면 decoder model에서 단지 context vector 1개만 사용해 output sentence를 생성해내는 것이 아니라, \(h_j\)와 같은 input sentence token에 대한 attention data를 사용하기 때문이다.&lt;/p&gt;

&lt;h1 id=&quot;encoder&quot;&gt;Encoder&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-05-17-Neural-Machine-Translation-By-Jointly-Learning-To-Align-And-Translate/06.jpg&quot; alt=&quot;06.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;기존 seq2seq model에서는 encoder는 1개의 context vector를 생성해내기 위해 존재했다. 하지만 본 model은 위에서 언급했듯이 하나의 context vector가 아닌 각 시점에 대한 context vector를 각각 생성한다. 이 때 사용하는 \(h_j\)를 구하는 것이 encoder의 역할이다. \(h_j\)는 위에서 언급했듯이 \(j\)번째 input token에 대해 attention한 vector이다. 이 때 attention한다는 의미는 \(j\)번째 input token에 대해 당연히 가장 높은 가중치를 주고, \(j\)번째에서 멀어질수록 낮은 가중치를 주는 것이다. 이는 \(j-1\), \(j-2\) …의 역방향, \(j+1\), \(j+2\) …의 순방향, 즉 양방향에 대해 모두 적용되어야 한다. 이를 위해 사용한 것이 Bidirectional RNN이다. 순방향에 대한 \(h_j\)를 생성하고, input sentence의 끝에 도달하면 다시 역방향에 대한 \(h_J\)를 생성한다. 그 후 순방향에 대한 \(\overrightarrow{h_j}\)와 역방향에 대한 \(\overleftarrow{h_j}\)를 함께 반영해 최종 \(h_j\)를 만들어낸다.&lt;/p&gt;

&lt;h1 id=&quot;results&quot;&gt;Results&lt;/h1&gt;

&lt;p&gt;본 논문에서 개발한 attention seq2seq model을 RNNsearch라고 명명한다. 이전 seq2seq model은 RNNencdec라고 명명한다.&lt;/p&gt;

&lt;p&gt;model 명 뒤의 숫자는 train 시 사용했던 dataset의 최대 setnence length이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-05-17-Neural-Machine-Translation-By-Jointly-Learning-To-Align-And-Translate/07.jpg&quot; alt=&quot;07.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;가장 주목할만한 점은 sentence length에 관계없이 robust한 결과를 보여줬다는 것이다. 이를 통해 fixed length context vector에 모든 context를 저장함으로써 발생한 bottleneck을 해결했다는 것을 알 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-05-17-Neural-Machine-Translation-By-Jointly-Learning-To-Align-And-Translate/08.jpg&quot; alt=&quot;08.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;UNK (Out Of Vocabulary)를 포함한 경우와 포함하지 않은 경우 모두 기존 seq2seq model보다 월등한 수치를 보여줬다. RNNsearch-50*는 더이상 성능 향상이 없을 때까지 계속 training을 시킨 model이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-05-17-Neural-Machine-Translation-By-Jointly-Learning-To-Align-And-Translate/09.jpg&quot; alt=&quot;09.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위는 \(a_{ij}\)를 시각화한 그림인데, 대체로 monotonic한 match를 볼 수 있다. English-French translate이기에 그렇다. 하지만 (a)를 보면 조사나 명사에 대해서 monotonic하지 않은 case가 있음에도 불구하고 정확히 align을 했다는 사실을 확인 가능하다. (d)에서는 본 model에서 채택한 soft-align 방식의 이점이 드러난다. soft-align이란 가장 높은 확률값을 가진 token pair 1개만을 채택해 align하는 것이 아니라 여러 token에 대해 각각의 align probability를 적용해 soft하게 align했다는 의미이다. 만약 soft-align이 아닌 hard-align을 했다면 [the man]의 2개의 token을 각각 [l’ homme]의 두 token 중 하나에 align해야 하는데 이러한 작업은 translation에 결코 도움이 되지 않는다. 따라서 soft-align을 사용해 여러 token에 대한 align을 모두 고려하는 방식이 적합하다는 것을 알 수 있다.&lt;/p&gt;
</description>
        <pubDate>Sat, 16 May 2020 19:00:00 -0500</pubDate>
        <link>http://0.0.0.0:4000/machine%20learning/paper%20review/Neural-Machine-Translation-By-Jointly-Learning-To-Align-And-Translate/</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/machine%20learning/paper%20review/Neural-Machine-Translation-By-Jointly-Learning-To-Align-And-Translate/</guid>
        
        <category>NLP</category>
        
        
        <category>Machine Learning</category>
        
        <category>Paper Review</category>
        
      </item>
    
      <item>
        <title>[NLP 논문 리뷰] Sequence To Sequence Learning With Neural Networks (Seq2Seq)</title>
        <description>&lt;h2 id=&quot;paper-info&quot;&gt;Paper Info&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1409.3215&quot;&gt;Archive Link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1409.3215.pdf&quot;&gt;Paper Link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Submit Date: Sep 10, 2014&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;DNN (Deep Neural Network)는 음성 인식, 사물 인식 등에서 꾸준한 성과를 내어왔다. 하지만 input size가 fixed된다는 한계점이 존재하기 때문에 sequencial problem을 제대로 해결할 수 없다는 한계점이 존재했다. 본 논문에서는 2개의 LSTM (Long Short Term Memory)을 각각 Encoder, Decoder로 사용해 sequencial problem을 해결하고자 했다. 이를 통해 많은 성능 향상을 이루어냈으며, 특히나 long sentence에서 더 큰 상승 폭을 보였다. 이에 더해 단어를 역순으로 배치하는 방식으로도 성능을 향상시켰다.&lt;/p&gt;

&lt;h1 id=&quot;the-model&quot;&gt;The model&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-05-10-Sequence-to-Sequence-Learning-with-Neural-Networks/01.jpg&quot; alt=&quot;01.jpg&quot; /&gt;&lt;/p&gt;

\[h_t = sigmoid\left(W^{hx}x_t + W^{hh}h_{t-1}\right)\\y_t = W^{yh}h_t\]

\[p\left(y_1,\cdots,y_{T'}|x_1,\cdots,x_T\right)=\prod_{t=1}^{T'}p\left(y_t|v,y_1,\cdots,y_{t-1}\right)\]

&lt;p&gt;RNN은 기본적으로 sequencial problem에 매우 적절한 model이다. 하지만 input size와 output size가 다른 경우에 대해서는 좋은 성능을 보일 수 없었다. 본 논문에서 제시하는 model은 Encoder LSTM에서 하나의 context vector를 생성한 뒤 Decoder LSTM에서 context vector를 이용해 output sentence를 생성하는 방식으로 RNN의 한계점을 극복하고자 했다. input과 output sentence 간의 mapping을 하는 것이 아닌, input sentence를 통해 encoder에서 context vector를 생성하고, 이를 활용해 decoder에서 output sentence를 만들어내는 것이다. Encoder LSTM의 output인 context vector는 Encoder의 마지막 layer에서 나온 output이다. 이를 Decoder LSTM의 첫번째 layer의 input으로 넣게 된다. 여기서 주목할만한 점은 input sentence에서의 word order를 reverse해 사용했다는 것이다. 또한 &lt;EOS&gt; (End of Sentence) token을 각 sentence의 끝에 추가해 variable length sentence를 다뤘다.&lt;/EOS&gt;&lt;/p&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;p&gt;WMT’14의 English to French dataset으로 실험을 진행했다. source / target language 각각에 fixed size vocabulary를 사용했다 (source: 160,000 / target: 80,000). OOV는 “UNK” token으로 대체된다. long sequence에서는 source sentence를 reverse시킨 경우가 특히나 성능이 더 좋았다. 구체적인 수치로 BLEU score가 25.9에서 30.6으로 증가했다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;원래의 순서대로 나열된 단어의 경우 source와 target에서의 연결되는 단어쌍(pair of word) 사이의 거리가 모두 동일하다. 하지만 reverse시킬 경우에는 sentence에서 앞에 위치한 단어일수록 단어쌍 사이의 거리가 짧아지게 된다. 이는 결국 sentence에서 뒤에 위치한 단어들에 대해서는 오히려 reverse하지 않았을 때보다 단어쌍 사이의 거리가 멀어지는 결과를 낳는다. 생각해보면 결국 reverse한 뒤나, 하지 않았을 때에나 단어쌍 사이의 거리 mean값은 동일하다. 그런데 왜 reverse시 더 좋은 성능이 나오는 것인지 의문일 수 있는데, sequencial problem의 개념으로 다시 돌아와 생각해보면 어느정도 이유를 추론 가능하다. sequencial problem에서는 앞쪽에 위치한 data가 뒤의 모든 data에 영향을 주기 때문에 앞에 위치한 data일 수록 중요도가 더 높다고 할 수 있다. 따라서 reverse를 통해 source sentence에서 앞쪽에 위치한 data(word)들의 target sentence에서의 연관 word와의 거리를 줄이는 것은 더 중요도 높은 data에 대해 더 좋은 성능을 보장하게 되는 효과를 낳는다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;dataset의 대부분은 short length sentence이기에 mini batch 사용 시 각 batch 마다 아주 적은 수의 long length sentence가 포함되는 문제가 존재했다. 따라서 각 batch마다 대략적으로 비슷한 length를 가진 sentence가 포함되도록 normalization을 수행한 뒤 실험을 진행했다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-05-10-Sequence-to-Sequence-Learning-with-Neural-Networks/02.jpg&quot; alt=&quot;02.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-05-10-Sequence-to-Sequence-Learning-with-Neural-Networks/03.jpg&quot; alt=&quot;03.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;SOTA(State of the Art)에 비해 0.5 낮은 BLEU Score를 달성했다. OOV가 여전히 존재함에도 SOTA와 동등한 성능을 달성했다는 것은 충분히 의미가 있다.&lt;/p&gt;

&lt;p&gt;위에서 언급했듯이 long Sentence에서도 매우 좋은 성능을 보였다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-05-10-Sequence-to-Sequence-Learning-with-Neural-Networks/04.jpg&quot; alt=&quot;04.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-05-10-Sequence-to-Sequence-Learning-with-Neural-Networks/05.jpg&quot; alt=&quot;05.jpg&quot; /&gt;&lt;/p&gt;
</description>
        <pubDate>Sat, 09 May 2020 19:00:00 -0500</pubDate>
        <link>http://0.0.0.0:4000/machine%20learning/paper%20review/Sequence-to-Sequence-Learning-with-Neural-Networks/</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/machine%20learning/paper%20review/Sequence-to-Sequence-Learning-with-Neural-Networks/</guid>
        
        <category>NLP</category>
        
        
        <category>Machine Learning</category>
        
        <category>Paper Review</category>
        
      </item>
    
      <item>
        <title>[NLP 논문 리뷰] Neural Machine Translation of Rare Words with Subword Units (BPE)</title>
        <description>&lt;h2 id=&quot;paper-info&quot;&gt;Paper Info&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1508.07909&quot;&gt;Archive Link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1508.07909.pdf&quot;&gt;Paper Link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Submit Date: Aug 15, 2015&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;backgrounds&quot;&gt;Backgrounds&lt;/h1&gt;

&lt;h2 id=&quot;bleu-score-bilingual-evaluation-understudy-score&quot;&gt;BLEU Score (Bilingual Evaluation Understudy) score&lt;/h2&gt;

\[BLEU=min\left(1,\frac{\text{output length}}{\text{reference_length}}\right)\left(\prod_{i=1}^4precision_i\right)^{\frac{1}{4}}\]

&lt;p&gt;reference sentence와 output sentence의 일치율을 나타내는 score이다. 3단계 절차를 거쳐 최종 BLEU Score를 도출해낸다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;n-gram에서 순서쌍의 겹치는 정도 (Precision)
    &lt;ul&gt;
      &lt;li&gt;Example
        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;output sentence&lt;/p&gt;

            &lt;p&gt;&lt;strong&gt;빛이 쐬는&lt;/strong&gt; 노인은 &lt;strong&gt;완벽한&lt;/strong&gt; 어두운 곳에서 &lt;strong&gt;잠든 사람과 비교할 때&lt;/strong&gt; 강박증이 &lt;strong&gt;심해질&lt;/strong&gt; 기회가 &lt;strong&gt;훨씬 높았다&lt;/strong&gt;&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;true sentence&lt;/p&gt;

            &lt;p&gt;&lt;strong&gt;빛이 쐬는&lt;/strong&gt; 사람은 &lt;strong&gt;완벽한&lt;/strong&gt; 어둠에서 &lt;strong&gt;잠든 사람과 비교할 때&lt;/strong&gt; 우울증이 &lt;strong&gt;심해질&lt;/strong&gt; 가능성이 &lt;strong&gt;훨씬 높았다&lt;/strong&gt;&lt;/p&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;1-gram precision&lt;/p&gt;

\[\frac{\text{\# of correct 1-gram in output sentence}}{\text{all 1-gram pair in output sentence}}=\frac{10}{14}\]
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;2-gram precision&lt;/p&gt;

\[\frac{\text{\# of correct 2-gram in output sentence}}{\text{all 2-gram pair in output sentence}}=\frac{5}{13}\]
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;3-gram precision&lt;/p&gt;

\[\frac{\text{\# of correct 3-gram in output sentence}}{\text{all 3-gram pair in output sentence}}=\frac{2}{12}\]
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;4-gram precision&lt;/p&gt;

\[\frac{\text{\# of correct 4-gram in output sentence}}{\text{all 4-gram pair in output sentence}}=\frac{1}{11}\]
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;같은 단어에 대한 보정 (Clipping)
    &lt;ul&gt;
      &lt;li&gt;Example
        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;output sentence&lt;/p&gt;

            &lt;p&gt;&lt;strong&gt;The more&lt;/strong&gt; decomposition &lt;strong&gt;the more&lt;/strong&gt; flavor &lt;strong&gt;the&lt;/strong&gt; food has&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;true sentence&lt;/p&gt;

            &lt;p&gt;&lt;strong&gt;The more the&lt;/strong&gt; merrier I always say&lt;/p&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;1-gram precision&lt;/p&gt;

\[\frac{\text{\# of 1-gram in output sentence}}{\text{all 1-gram pair in output sentence}}=\frac{5}{9}\]
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Clipping 1-gram precision&lt;/p&gt;

\[\frac{\text{\# of 1-gram in output sentence}}{\text{all 1-gram pair in output sentence}}=\frac{3}{9}\]
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;문장 길이에 대한 보정 (Brevity Penalty)
    &lt;ul&gt;
      &lt;li&gt;Example
        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;output sentence&lt;/p&gt;

            &lt;p&gt;&lt;strong&gt;빛이 쐬는&lt;/strong&gt; 노인은 &lt;strong&gt;완벽한&lt;/strong&gt; 어두운 곳에서 잠듬&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;true sentence&lt;/p&gt;

            &lt;p&gt;&lt;strong&gt;빛이 쐬는&lt;/strong&gt; 사람은 &lt;strong&gt;완벽한&lt;/strong&gt; 어둠에서 잠든 사람과 비교할 때 우울증이 심해질 가능성이 훨씬 높았다&lt;/p&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;brevity penalty&lt;/p&gt;

\[min\left(1,\frac{\text{\# of words in output sentence}}{\text{\# of words in true sentence}}\right)=min\left(1,\frac{6}{14}\right)=\frac{3}{7}\]
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;최종 BLEU Score
    &lt;ul&gt;
      &lt;li&gt;Example
        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;output sentence&lt;/p&gt;

            &lt;p&gt;&lt;strong&gt;빛이 쐬는&lt;/strong&gt; 노인은 완벽한 어두운 곳에서 &lt;strong&gt;잠든 사람과 비교할 때&lt;/strong&gt; 강박증이 &lt;strong&gt;심해질&lt;/strong&gt; 기회가 &lt;strong&gt;훨씬 높았다&lt;/strong&gt;&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;true sentence&lt;/p&gt;

            &lt;p&gt;&lt;strong&gt;빛이 쐬는&lt;/strong&gt; 사람은 &lt;strong&gt;완벽한&lt;/strong&gt; 어둠에서 &lt;strong&gt;잠든 사람과 비교할 때&lt;/strong&gt; 우울증이 &lt;strong&gt;심해질&lt;/strong&gt; 가능성이 &lt;strong&gt;훨씬 높았다&lt;/strong&gt;&lt;/p&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;BLEU Score&lt;/p&gt;

\[BLEU=min\left(1,\frac{\text{output length}}{\text{reference length}}\right)\left(\prod_{i=1}^4precision_i\right)^{\frac{1}{4}}\\=min\left(1,\frac{14}{14}\right)\times\left(\frac{10}{14}\times\frac{5}{13}\times\frac{2}{12}\times\frac{1}{11}\right)^{\frac{1}{4}}\]
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;출처: &lt;a href=&quot;https://donghwa-kim.github.io/BLEU.html&quot;&gt;https://donghwa-kim.github.io/BLEU.html&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;

&lt;p&gt;기존의 NMT (Neural machine translation)는 모두 고정된 개수의 vocabulary 안에서 작업했다. 하지만 translation은 vocabulary 개수의 제한이 없는 open-vocabulary problem이기 OOV(out of vocabulary) word가 많이 발생할 수밖에 없다. 본 논문에서는 이러한 OOV 문제를 subword unit 활용해 해결하고자 했다.&lt;/p&gt;

&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;기존의 NMT Model은 OOV words에 대해 back-off model 사용해왔다. back-off model 대신 본 논문에서 제시할 subword unit을 사용할 경우 OOV 문제를 더 확실히 해결해 open-vocabulary problem에서 성능 향상을 이끌어낼 수 있다.&lt;/p&gt;

&lt;h1 id=&quot;subword-translation&quot;&gt;Subword Translation&lt;/h1&gt;

&lt;p&gt;현재의 language model에서 translatable하지 않더라도, 다른 language의 translation의 sub word를 사용하면 translate이 가능하다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;이름 등의 고유 명사는 음절 별로 대응시킨다.
    &lt;ul&gt;
      &lt;li&gt;Barack Obama (English; German)&lt;/li&gt;
      &lt;li&gt;Барак Обама (Russian)&lt;/li&gt;
      &lt;li&gt;バラク・オバマ (ba-ra-ku o-ba-ma) (Japanese)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;동의어, 외래어 등 같은 origin을 갖는 단어들은 일정한 규칙을 갖고 변형되므로, character-level translation 사용한다.
    &lt;ul&gt;
      &lt;li&gt;claustrophobia (English)&lt;/li&gt;
      &lt;li&gt;Klaustrophobie (German)&lt;/li&gt;
      &lt;li&gt;Клаустрофобия (Klaustrofobiâ) (Russian)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;복합어는 각각의 sub-word를 번역한 후 결합한다.
    &lt;ul&gt;
      &lt;li&gt;solar system (English)&lt;/li&gt;
      &lt;li&gt;Sonnensystem (Sonne + System) (German)&lt;/li&gt;
      &lt;li&gt;Naprendszer (Nap + Rendszer) (Hungarian)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;위와 같은 규칙으로 german training data에서 가장 빈도 낮은 100개의 word를 분석하면 english data를 통해 56개의 복합어, 21개의 고유명사, 6개의 외래어 등을 찾아낼 수 있었다.&lt;/p&gt;

&lt;h2 id=&quot;related-work&quot;&gt;Related Work&lt;/h2&gt;

&lt;p&gt;OOV는 고유명사 (사람 이름, 지역명), 외래어 등에 대해서 자주 발생한다. 이를 해결하기 위해 character level로 word를 분리한 뒤, 각 character들이 일정한 기준을 충족할 경우 하나의 token으로 묶어 표현하는 방식을 채택했다. 이를 통해 text size는 줄어들게 된다. 이 때 단어를 subword로 구분하는 기존의 Segmentation algorithm을 사용하되,  좀 더 aggressive한 기준을 적용하고자 했다. vocabulary size와 text size는 서로 trade-off 관계이므로 vocabulary size가 감소한다면 시간/공간 복잡도는 낮아지겠지만  unknown word의 개수가 증가하게 된다.&lt;/p&gt;

&lt;h2 id=&quot;byte-pair-encoding-bpe&quot;&gt;Byte Pair Encoding (BPE)&lt;/h2&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;re&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;collections&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;get_stats&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vocab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;pairs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;collections&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;defaultdict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;word&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;freq&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vocab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;items&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;symbols&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;word&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;symbols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;pairs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;symbols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;symbols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;freq&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pairs&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;merge_vocab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pair&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;v_out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{}&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;bigram&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;re&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;escape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;' '&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pair&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;re&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;compile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'(?&amp;lt;!\S)'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bigram&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;sa&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'(?!\S)'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;word&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;w_out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sub&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;''&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pair&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;word&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;v_out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w_out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;word&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v_out&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;vocab&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'low&amp;lt;/w&amp;gt;'&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'lower&amp;lt;/w&amp;gt;'&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
				 &lt;span class=&quot;s&quot;&gt;'newest&amp;lt;/w&amp;gt;'&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'widest&amp;lt;/w&amp;gt;'&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;num_merges&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_merges&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;pairs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get_stats&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vocab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;best&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pairs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pairs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;vocab&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;merge_vocab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;best&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vocab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;best&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# r .  -&amp;gt;  r.
# l o  -&amp;gt;  lo
# lo w -&amp;gt;  low
# e r. -&amp;gt;  er.
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;BPE는 가장 빈도가 높은 pair of bytes부터 하나의 single byte로 치환해 저장하는 압축 algorithm이다.&lt;/p&gt;

&lt;p&gt;BPE는 다음과 같은 과정을 따른다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;word를 character의 sequence로 변환 후 end symbol  ·  추가&lt;/li&gt;
  &lt;li&gt;모든 character의 pair를 센 후 가장 빈도가 높은 pair of character (‘A’, ‘B’)를 새로운 symbol ‘AB’ (character n-gram)로 치환&lt;/li&gt;
  &lt;li&gt;2번 단계를 원하는 횟수만큼(vocabulary size만큼 token이 생성될 때까지) 반복&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;BPE의 반복 횟수는 vocabulary size라는 hyperparameter에 따라 결정된다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;예시
    &lt;ol&gt;
      &lt;li&gt;
        &lt;p&gt;train sentences&lt;/p&gt;

        &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; &lt;span class=&quot;n&quot;&gt;sentence&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt; 
 &lt;span class=&quot;s&quot;&gt;'black bug bit a black bear but is the black bear that the big black bug bit'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
 &lt;span class=&quot;s&quot;&gt;'a big bug bit the little beetle but the little beetle bit the big bug back'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;
 &lt;span class=&quot;s&quot;&gt;'the better with the butter is the batter that is better'&lt;/span&gt;
 &lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;        &lt;/div&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;count segments&lt;/p&gt;

        &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; &lt;span class=&quot;p&quot;&gt;[(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'t h e &amp;lt;/w&amp;gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'b l a c k &amp;lt;/w&amp;gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'b i t &amp;lt;/w&amp;gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'i s &amp;lt;/w&amp;gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'b i g &amp;lt;/w&amp;gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'b e a r &amp;lt;/w&amp;gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'b u t &amp;lt;/w&amp;gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'t h a t &amp;lt;/w&amp;gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'l i t t l e &amp;lt;/w&amp;gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'b e e t l e &amp;lt;/w&amp;gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'b e t t e r &amp;lt;/w&amp;gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'b a c k &amp;lt;/w&amp;gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'w i t h &amp;lt;/w&amp;gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'b u t t e r &amp;lt;/w&amp;gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'b a t t e r &amp;lt;/w&amp;gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;        &lt;/div&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;count bi-grams&lt;/p&gt;

        &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; &lt;span class=&quot;p&quot;&gt;[((&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'t'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'h'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;11&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'h'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'e'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'t'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'&amp;lt;/w&amp;gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'g'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'&amp;lt;/w&amp;gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;        &lt;/div&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;add merge-rules&lt;/p&gt;

        &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'t'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'h'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;th&lt;/span&gt;
 &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'h'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'e'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;he&lt;/span&gt;
 &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'t'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'&amp;lt;/w&amp;gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;
 &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'g'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'&amp;lt;/w&amp;gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;        &lt;/div&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;evaluation&quot;&gt;Evaluation&lt;/h1&gt;

&lt;h2 id=&quot;subword-statistics&quot;&gt;Subword statistics&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-05-03-Neural-Machine-Translation-of-Rare-Words-with-Subword-Units/01.jpg&quot; alt=&quot;01.jpg&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;h1 id=&quot;tokens-text-size&quot;&gt;tokens: text size&lt;/h1&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;h1 id=&quot;types-vocabulary-size-token-개수&quot;&gt;types: vocabulary size, token 개수&lt;/h1&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;h1 id=&quot;unk-unknown-word-oov-word의-개수&quot;&gt;UNK: unknown word (OOV word)의 개수&lt;/h1&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;translation-experiments&quot;&gt;Translation experiments&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-05-03-Neural-Machine-Translation-of-Rare-Words-with-Subword-Units/02.jpg&quot; alt=&quot;02.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-05-03-Neural-Machine-Translation-of-Rare-Words-with-Subword-Units/03.jpg&quot; alt=&quot;03.jpg&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;W Unk: back-off dictionary를 사용하지 않은 model이다.&lt;/li&gt;
  &lt;li&gt;W Dict: back-off dictionary를 사용한 model이다.&lt;/li&gt;
  &lt;li&gt;C2-50k: char-bigram을 사용한 model이다.&lt;/li&gt;
  &lt;li&gt;CHR F3: 인간의 판단과 일치율&lt;/li&gt;
  &lt;li&gt;unigram F1: BLEU unigram(brevity penalty 제외)와 Recall의 조합&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;source와 target 각각 따로 BPE를 수행하는 BPE보다 동시에 수행하는 BPE joint가 더 좋은 성능을 보였다.&lt;/p&gt;

&lt;h1 id=&quot;analysis&quot;&gt;Analysis&lt;/h1&gt;

&lt;h2 id=&quot;unigram-accuracy&quot;&gt;Unigram accuracy&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-05-03-Neural-Machine-Translation-of-Rare-Words-with-Subword-Units/04.jpg&quot; alt=&quot;04.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-05-03-Neural-Machine-Translation-of-Rare-Words-with-Subword-Units/05.jpg&quot; alt=&quot;05.jpg&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;manual-analysis&quot;&gt;Manual Analysis&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-05-03-Neural-Machine-Translation-of-Rare-Words-with-Subword-Units/06.jpg&quot; alt=&quot;06.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2020-05-03-Neural-Machine-Translation-of-Rare-Words-with-Subword-Units/07.jpg&quot; alt=&quot;07.jpg&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;OOV 문제를 해결해 NMT와 같은 open-vocabulary translation에서 좋은 성능을 보였다. 기존에 OOV를 해결하기 위해 사용되던 back-off translation model보다 더 좋은 성능을 보였다.&lt;/p&gt;
</description>
        <pubDate>Sat, 02 May 2020 19:00:00 -0500</pubDate>
        <link>http://0.0.0.0:4000/machine%20learning/paper%20review/Neural-Machine-Translation-of-Rare-Words-with-Subword-Units/</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/machine%20learning/paper%20review/Neural-Machine-Translation-of-Rare-Words-with-Subword-Units/</guid>
        
        <category>NLP</category>
        
        
        <category>Machine Learning</category>
        
        <category>Paper Review</category>
        
      </item>
    
  </channel>
</rss>

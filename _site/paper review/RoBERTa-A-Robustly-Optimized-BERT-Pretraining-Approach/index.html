<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.21.0 by Michael Rose
  Copyright 2013-2020 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="ko" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>[NLP 논문 리뷰] RoBERTa: A Robustly Optimized BERT Pretraining Approach | Hansu Kim’s Dev Blog</title>
<meta name="description" content="Paper Info">


  <meta name="author" content="Hansu Kim">
  
  <meta property="article:author" content="Hansu Kim">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="ko_KR">
<meta property="og:site_name" content="Hansu Kim's Dev Blog">
<meta property="og:title" content="[NLP 논문 리뷰] RoBERTa: A Robustly Optimized BERT Pretraining Approach">
<meta property="og:url" content="http://0.0.0.0:4000/paper%20review/RoBERTa-A-Robustly-Optimized-BERT-Pretraining-Approach/">


  <meta property="og:description" content="Paper Info">







  <meta property="article:published_time" content="2020-10-04T19:00:00-05:00">



  <meta property="article:modified_time" content="2020-10-04T19:00:00-05:00">



  

  


<link rel="canonical" href="http://0.0.0.0:4000/paper%20review/RoBERTa-A-Robustly-Optimized-BERT-Pretraining-Approach/">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": "Hansu Kim",
      "url": "http://0.0.0.0:4000/"
    
  }
</script>






<!-- end _includes/seo.html -->


<link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Hansu Kim's Dev Blog Feed">

<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css">

<!--[if IE]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->





    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single">
    <nav class="skip-links">
  <h2 class="screen-reader-text">Skip links</h2>
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
          Hansu Kim's Dev Blog
          <span class="site-subtitle">기술 블로그</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/categories/">Categories</a>
            </li><li class="masthead__menu-item">
              <a href="/years/">Years</a>
            </li><li class="masthead__menu-item">
              <a href="/tags/">Tags</a>
            </li><li class="masthead__menu-item">
              <a href="/about/">About</a>
            </li></ul>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">토글 메뉴</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      



<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person">

  
    <div class="author__avatar">
      
        <img src="/assets/images/profile.jpg" alt="Hansu Kim" itemprop="image">
      
    </div>
  

  <div class="author__content">
    
      <h3 class="author__name" itemprop="name">Hansu Kim</h3>
    
    
      <div class="author__bio" itemprop="description">
        <p>ML, NLP를 주로 공부하는 학부생입니다.</p>

      </div>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      
        <li itemprop="homeLocation" itemscope itemtype="https://schema.org/Place">
          <i class="fas fa-fw fa-map-marker-alt" aria-hidden="true"></i> <span itemprop="name">Republic of Korea</span>
        </li>
      

      
        
          
            <li><a href="mailto:cpm0722@gmail.com" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-envelope-square" aria-hidden="true"></i><span class="label">Email</span></a></li>
          
        
          
            <li><a href="https://www.facebook.com/profile.php?id=100003380546271" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-facebook-square" aria-hidden="true"></i><span class="label">Facebook</span></a></li>
          
        
          
            <li><a href="https://github.com/cpm0722" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i><span class="label">GitHub</span></a></li>
          
        
      

      

      
        <li>
          <a href="mailto:cpm0722@gmail.com">
            <meta itemprop="email" content="cpm0722@gmail.com" />
            <i class="fas fa-fw fa-envelope-square" aria-hidden="true"></i><span class="label">Email</span>
          </a>
        </li>
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="[NLP 논문 리뷰] RoBERTa: A Robustly Optimized BERT Pretraining Approach">
    <meta itemprop="description" content="Paper Info">
    <meta itemprop="datePublished" content="2020-10-04T19:00:00-05:00">
    <meta itemprop="dateModified" content="2020-10-04T19:00:00-05:00">

		<script type="text/javascript" async
		src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
		</script>

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">[NLP 논문 리뷰] RoBERTa: A Robustly Optimized BERT Pretraining Approach
</h1>
          

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          5 분 소요
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content" itemprop="text">
        
        <h2 id="paper-info">Paper Info</h2>

<p><a href="https://arxiv.org/abs/1907.11692">Archive Link</a></p>

<p><a href="https://arxiv.org/pdf/1907.11692.pdf">Paper Link</a></p>

<p>Submit Date: Jul 26, 2019</p>

<hr />

<h1 id="introduction">Introduction</h1>

<p>BERT 계열의 model들은 지금까지 매우 뛰어난 성능을 보여왔다. 본 논문에서는 BERT에 대한 추가적인 연구를 통해 기존의 BERT model들이 undertrained되었음을 보여주고, 다음의 개선 방안들을 제시한다.</p>

<ol>
  <li>더 긴 시간, 더 큰 batch size의 training</li>
  <li>NSP 제거</li>
  <li>long sequence에 대한 학습</li>
  <li>MLM에서의 동적인 masking 정책</li>
</ol>

<p>본 논문의 Contribution은 다음과 같다.</p>

<ol>
  <li>downstream task에서의 성능 향상을 위한 BERT model의 design choice, training 전략을 제시</li>
  <li>새로운 CC-NEWS dataset을 도입</li>
  <li>적절한 design choice에 기반한 MLM이 다른 여러 method 대비 좋은 성능을 보임을 입증</li>
</ol>

<h1 id="background">Background</h1>

<p>BERT에 대한 overview</p>

<p>BERT Review 참고 <a href="https://cpm0722.github.io/paper%20review/BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding/">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a></p>

<h1 id="experimental-setup">Experimental Setup</h1>

<h2 id="implementation">Implementation</h2>

<p>대부분의 hyperparameters의 값은 BERT와 동일한 값을 채택했다. 하지만 peak learning rate와 #warmup steps, Adam epsilon에 대해서는 tuning을 진행했다. 전체 sequence의 최대 길이 T는 512로 했다.</p>

<h2 id="data">Data</h2>

<p>BERT-style의 pretraining 방식은 data의 양에 따라 성능이 결정된다. BERT 이후 더 큰 dataset을 사용한 여러 연구가 진행되었으나, 그 dataset이 공개되지는 않았다. 본 논문에서는 총 160GB의 4개의 English dataset을 사용했다.</p>

<ol>
  <li>
    <p>Book Corpus + Wikipedia</p>

    <p>Original BERT에서 사용했던 dataset으로, 총 16GB이다.</p>
  </li>
  <li>
    <p>CC-News</p>

    <p>총 63million개의 2016/09~2019/02 사이의 뉴스 기사를 crawling한 dataset으로, 총 76GB이다.</p>
  </li>
  <li>
    <p>OpenWebText</p>

    <p>Reddit과 같은 web site에서 URL 기반으로 crawling을 한 dataset으로, 총 38GB이다.</p>
  </li>
  <li>
    <p>Stories</p>

    <p>story-like dataset으로, 총 31GB이다.</p>
  </li>
</ol>

<h2 id="evaluation">Evaluation</h2>

<ol>
  <li>
    <p>GLUE</p>

    <p>BERT Review 참고 <a href="https://cpm0722.github.io/paper%20review/BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding/">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a></p>
  </li>
  <li>
    <p>SQuAD</p>

    <p>BERT Review 참고 <a href="https://cpm0722.github.io/paper%20review/BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding/">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a></p>
  </li>
  <li>
    <p>RACE</p>

    <p>ReAding Comprehension from Examinations이다. 중국에서의 Enghlish Examination에서 추출한 28,000개의 단락과 100,000개의 질문이 존재한다. 각 단락은 여러 질문과 함께 등장하는데, 그 중 하나의 알맞은 질문을 고르는 task이다. 기존의 다른 comprehension dataset에 비해 passage의 길이가 길고, 추론 질문의 비율이 높다는 점에서 차이가 있다.</p>
  </li>
</ol>

<h2 id="training-procedure-analysis">Training Procedure Analysis</h2>

<p>BERT model L=12, H=768, A=12, #params: 110M으로 은 고정해둔 상태로 실험을 진행한다.</p>

<h2 id="static-vs-dynamic-masking">Static vs Dynamic Masking</h2>

<p><img src="/assets/images/2020-10-05-RoBERTa-A-Robustly-Optimized-BERT-Pretraining-Approach/01.jpg" alt="01.jpg" /></p>

<p>본 논문에서는 Dynamic Masking 기법을 도입했다. 기존의 BERT에서의 Masking Rule은 전처리 과정에서 한 번 수행한 masking이 계속 유지되는 Static Masking이다. 이는 매 epoch마다 동일한 masking으로 학습을 하게 됨을 의미한다. Dynamic Masking은 training data를 10배로 복제해 각각의 training data마다 다른 masking을 수행했다. 같은 비율의 masking 정책 하에서 (80%/10%/10% 등) 다른 word가 masking되는 것이다. 이를 40 epochs동안 수행하는데, 결국 같은 masking으로 총 4epochs의 학습이 이루어지게 되는 것이다. 이러한 Dynamic Masking 기법은 dataset이 클 수록 Static Masking 대비 더 큰 성능 향상을 보였다. 위 Table에서 볼 수 있듯이 static 대비 dynamic masking이 조금이나마 더 좋은 성능을 보였다.</p>

<h2 id="model-input-format-and-next-sentence-prediction">Model Input Format and Next Sentence Prediction</h2>

<p><img src="/assets/images/2020-10-05-RoBERTa-A-Robustly-Optimized-BERT-Pretraining-Approach/02.jpg" alt="02.jpg" /></p>

<p>NSP가 성능 향상에 얼마나 기여하는지에 대해서는 많은 연구가 있어왔고, 때로는 각 논문마다 다른 결과를 도출해내기도 했다. 이를 검증하기 위해 위와 같은 실험을 진행했다. 각 항목에 대해서 자세히 살펴보겠다.</p>

<ul>
  <li>
    <p>SEGMENT-PAIR</p>

    <p>여러 문장으로 이루어질 수 있는 segment의 pair이다. 한 pair는 최대 512 tokens까지 가질 수 있다. NSP를 포함한다.</p>
  </li>
  <li>
    <p>SENTENCE-PAIR</p>

    <p>문장의 pair이다. 한 pair는 최대 512 tokens까지 가질 수 있다. 당연하게도 평균적인 #tokens가 SEGMENT-PAIR보다 작기 때문에, batch size를 늘려 SEGMENT-PAIR의 한 batch와 total #tokens가 비슷해지도록 했다. NSP를 포함한다.</p>
  </li>
  <li>
    <p>FULL-SENTENCES</p>

    <p>여러 문장들의 sequences이다. 최대 512 tokens까지 가질 수 있다. 문장이 중간에 끊기는 일은 없도록 한다. 특수한 경우로, 한 document가 끝났음에도 새로운 문장이 삽입될 수 있을 경우, 다음 document의 첫 문장부터 이어서 삽입되게 된다. 이 경우에 있어서는 서로 다른 document에서 온 문장 사이에 특수한 seperator token이 삽입된다. NSP를 포함하지 않는다.</p>
  </li>
  <li>
    <p>DOC-SENTENCES</p>

    <p>FULL-SENTENCES와 유사하지만, 서로 다른 document에서 온 문장이 연속되는 일이 없도록 한다. 당연하게도 FULL-SENTENCES에 비해 평균 #tokens가 낮을 수 밖에 없기 때문에, 이 역시 batch size를 늘려 FULL-SENTENCES의 한 batch와 total #tokens가 비슷해지도록 했다. NSP를 포함하지 않는다.</p>
  </li>
</ul>

<p>SEGMENT-PAIR와 SENTENCE-PAIR를 비교해보자. SENTENCE-PAIR가 더 낮은 성능을 보였다. SENTENCE-PAIR는 model이 long-range dependencies를 학습할 능력이 없다고 가정한 것인데, 실제로는 BERT model이 long sequences에 대해서도 dependency를 학습할 수 있음을 알 수 있다.</p>

<p>한편, DOC-SENTENCES와 BERT_BASE를 비교해보면 DOC-SENTENCES가 original BERT보다 더 좋은 성능을 보인다는 것을 확인할 수 있다. 이는 NSP가 오히려 downstream task의 성능에 악영향을 미친다는 것을 보여준다.</p>

<p>마지막으로, FULL-SENTENCES와 DOC-SENTENCES를 비교하면 한 document 안의 문장만 묶는 DOC-SENTENCES가 미약하게나마 더 좋은 성능을 보인다는 것을 확인할 수 있다. 하지만 DOC-SENTENCES는 batch size가 각 batch마다 다르기 때문에, 본 논문의 이후 실험에서는 편리성을 위해 FULL-SENTENCES를 사용하기로 한다.</p>

<h2 id="training-with-large-batches">Training with large batches</h2>

<p><img src="/assets/images/2020-10-05-RoBERTa-A-Robustly-Optimized-BERT-Pretraining-Approach/03.jpg" alt="03.jpg" /></p>

<p>batch-size를 증가시킬수록 optimization speed와 성능 향상에 기여한다는 것은 알려져 있다. BERT 역시 larget batch size가 효과가 있는지 확인을 해보기로 한다. batch size=256, #steps=1M은 original BERT_BASE의 값이다. 이와 동일한 computational cost를 갖는 다른 batch size &amp; #steps를 실험해본다. 실제로 같은 computational cost를 갖는 경우에도 batch size가 커질수록 perplexity가 감소함을 확인할 수 있었다. end-task에서의 정확도도 상승했다.</p>

<h2 id="text-encoding">Text Encoding</h2>

<p>BPE는 등장 빈도를 기반으로 subword를 생성해내는 기법으로, OOV가 없다는 장점이 있다. 기존의 BPE는 모두 character 단위로 이루어졌는데, original BERT도 이를 채택했다. 본 논문에서는 unicode character 단위가 아닌 byte 단위로 하는 BPE를 도입하기로 한다. original BERT의 BPE는 vocabulary size가 30K였다면, 새로운 방식은 50K 정도의 큰 vocabulary size가 필요하다. 하지만 기존에는 필수적이던 전처리 과정이 필요없다는 장점을 갖는다. 사실 새로운 BPE는 약간의 성능 하락을 보여주지만, universal한 encoding 방식을 도입했다는 점에서 미미한 정도의 성능 하락을 감안하고서라도 채택해볼 만 하다.</p>

<h1 id="roberta">RoBERTa</h1>

<p>정리하자면, <strong>RoBERTa</strong>는 <strong>R</strong>obustly <strong>o</strong>ptimized <strong>BERT</strong> <strong>a</strong>pproach의 약자로, 다음의 4가지 특징을 갖는다.</p>

<ol>
  <li>dynamic masking 기법</li>
  <li>FULL-SENTENCES</li>
  <li>large mini batches</li>
  <li>byte-level BPE</li>
</ol>

<p><img src="/assets/images/2020-10-05-RoBERTa-A-Robustly-Optimized-BERT-Pretraining-Approach/04.jpg" alt="04.jpg" /></p>

<p>original BERT_LARGE model과 성능을 비교하기 위해서 RoBERTa의 model 크기를 BERT_LARGE와 동일하게 했다. 또한 original BERT에서 사용했던 dataset으로만 pretraining한 경우, 추가적인 dataset으로 pretraining한 경우, pretraining 횟수를 100K에서 300K, 500K로 증가시킨 경우를 비교했다. RoBERTa는 BERT_LARGE나 XLNet_LARGE와 동일한 조건에서도 더 높은 성능을 보였으며, 당연하게도 가장 많은 pretraining을 시킨 경우가 가장 좋은 성능을 보였다.</p>

<h2 id="glue-results">GLUE Results</h2>

<p><img src="/assets/images/2020-10-05-RoBERTa-A-Robustly-Optimized-BERT-Pretraining-Approach/05.jpg" alt="05.jpg" /></p>

<p>Single-task single models on dev는 GLUE의 각 subtask에 대해 별개로 fine-tuning을 진행한 model들이다.</p>

<p>Ensembles on test는 GLUE leaderboard에 있는 다른 score와 비교한 것이다. 특이하게 RoBERTa는 RTE, SST, MRPC subtask에 대해서 pretrained RoBERTa에서 fine-tuning을 시작하지 않고, MNLI single-task model에서 fine-tuning을 시작했다. 이 경우가 더 좋은 성능을 보였다고 한다.</p>

<p>Single-task, single models에서는 RoBERTa가 9개의 모든 GLUE subtask에서 SOTA를 달성했다. 주목할만한 점은, 여기서의 RoBERTa는 original BERT_LARGE와 동일한 model architecture, 동일한 masking rule(static masking)을 적용했다는 점이다. 이는 굳이 dataset size나 training time을 배제하더라도, training objective(NSP 제거)가 얼마나 큰 영향을 미치는지를 보여준다.</p>

<p>Ensembles on test에서 RoBERTa는 전체 9개 중 4개의 subtask에서 SOTA를 달성했다. 비교 대상인 다른 model들과 달리 RoBERTa는 multi-task fine-tuning을 수행하지 않았다는 점에서 큰 의미가 있다.</p>

<h2 id="squad-results">SQuAD Results</h2>

<p><img src="/assets/images/2020-10-05-RoBERTa-A-Robustly-Optimized-BERT-Pretraining-Approach/06.jpg" alt="06.jpg" /></p>

<p>SQuAD 2.0에 대해서는 추가적으로 answerable에 대한 binary classification을 수행하고, 기존의 loss와 더했다. 한편, RoBERTa는 original BERT나 XLNet과 달리 pretraining에서 추가적인 QA dataset을 사용하지 않고, 바로 SQuAD에 대해 fine-tuning을 진행했다. 그럼에도 불구하고 BERT나 XLNet에 비해 더 좋은 성능을 보였다.</p>

<h2 id="race-results">RACE Results</h2>

<p><img src="/assets/images/2020-10-05-RoBERTa-A-Robustly-Optimized-BERT-Pretraining-Approach/07.jpg" alt="07.jpg" /></p>

<p>RoBERTa 는 RACE task에서도 Middle school data와 High school data 모두에서 BERT_LARGE나 XLNET_LARGE 대비 더 좋은 성능을 보였다.</p>

<h1 id="conclusion">Conclusion</h1>

<p>BERT model의 성능을 향상시키는 여러 방법을 제시했다.</p>

<ol>
  <li>더 많은 횟수, 더 큰 batch size, 더 많은 data로 pretraining</li>
  <li>NSP 제거</li>
  <li>longer sequences로 pretraining</li>
  <li>dynamic masking</li>
  <li>byte 기반 BPE</li>
</ol>

<p>이를 종합한 RoBERTa는 GLUE, SQuAD, RACE에서 SOTA를 달성했다.</p>

        
      </section>

      <footer class="page__meta">
        
        
  


  
  
  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> 태그: </strong>
    <span itemprop="keywords">
    
      
      
      <a href="/tags/#nlp" class="page__taxonomy-item" rel="tag">NLP</a>
    
    </span>
  </p>




  


  
  
  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> 카테고리: </strong>
    <span itemprop="keywords">
    
      
      
      <a href="/categories/#paper-review" class="page__taxonomy-item" rel="tag">Paper Review</a>
    
    </span>
  </p>


        
  <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> 업데이트:</strong> <time datetime="2020-10-05">October 5, 2020</time></p>


      </footer>

      <section class="page__share">
  
    <h4 class="page__share-title">공유하기</h4>
  

  <a href="https://twitter.com/intent/tweet?text=%5BNLP+%EB%85%BC%EB%AC%B8+%EB%A6%AC%EB%B7%B0%5D+RoBERTa%3A+A+Robustly+Optimized+BERT+Pretraining+Approach%20http%3A%2F%2F0.0.0.0%3A4000%2Fpaper%2520review%2FRoBERTa-A-Robustly-Optimized-BERT-Pretraining-Approach%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="공유하기 Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2F0.0.0.0%3A4000%2Fpaper%2520review%2FRoBERTa-A-Robustly-Optimized-BERT-Pretraining-Approach%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="공유하기 Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=http%3A%2F%2F0.0.0.0%3A4000%2Fpaper%2520review%2FRoBERTa-A-Robustly-Optimized-BERT-Pretraining-Approach%2F" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="공유하기 LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/paper%20review/Subword-level-Word-Vector-Representation-for-Korean/" class="pagination--pager" title="[NLP 논문 리뷰] Subword-level Word Vector Representation for Korean
">이전</a>
    
    
      <a href="/paper%20review/An-Empirical-Study-of-Tokenization-Strategies-for-Various-Korean-NLP-Tasks/" class="pagination--pager" title="[NLP 논문 리뷰] An Empirical Study of Tokenization Strategies for Various Korean NLP Tasks
">다음</a>
    
  </nav>

    </div>

    
  </article>

  
  
    <div class="page__related">
      <h4 class="page__related-title">참고</h4>
      <div class="grid__wrapper">
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/paper%20review/Deep-contextualized-word-representations/" rel="permalink">[NLP 논문 리뷰] Deep Contextualized Word Representations (ELMo)
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          5 분 소요
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Paper Info
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/paper%20review/Efficient-Estimation-of-Word-Representations-in-Vector-Space/" rel="permalink">[NLP 논문 리뷰] Efficient Estimation Of Word Representations In Vector Space (Word2Vec)
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          5 분 소요
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Paper Info
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/paper%20review/KR-BERT-A-Small-Scale-Korean-Specific-Language-Model/" rel="permalink">[NLP 논문 리뷰] KR-BERT: A Small Scale Korean Specific Language Model
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          6 분 소요
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Paper Info
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/paper%20review/An-Empirical-Study-of-Tokenization-Strategies-for-Various-Korean-NLP-Tasks/" rel="permalink">[NLP 논문 리뷰] An Empirical Study of Tokenization Strategies for Various Korean NLP Tasks
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          7 분 소요
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Paper Info
</p>
  </article>
</div>

        
      </div>
    </div>
  
  
</div>

    </div>

    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    

    
      
        
          <li><a href="mailto:cpm0722@gmail.com" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-envelope-square" aria-hidden="true"></i> Email</a></li>
        
      
        
          <li><a href="https://www.facebook.com/profile.php?id=100003380546271" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-facebook-square" aria-hidden="true"></i> Facebook</a></li>
        
      
        
          <li><a href="https://github.com/cpm0722" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
    

    <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> RSS</a></li>
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2021 Hansu Kim. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>










  </body>
</html>

I"ê<h1 id="bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</h1>
<p>title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
subtitle: BERT
categories: Paper Review
tags: NLP
date: 2021-01-19 13:00:09 +0000
last_modified_at: 2021-01-19 13:00:09 +0000
‚Äî</p>
:ET